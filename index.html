<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Hao Tang</title>
    <meta name="author" content="Hao Tang">
    <meta name="description" content="&lt;p&gt; Computer Vision Lab, ETH ZÃ¼rich, Switzerland&lt;br&gt; Office: ETF C 108, Sternwartstrasse 7, 8092 ZÃ¼rich, Switzerland&lt;br&gt; Email: bjdxtanghao@gmail.com&lt;/p&gt;
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;">

    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              
              <!--  -->
              <!-- Blog -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li> -->
              
              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/services/">Services</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/others/">Others</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <div class="post">


  <article>
    <div class="row">
        <div class="col-sm-3">
            
            <img style="width: 200px;border-radius: 10px" src="/assets/img/th.jpg">
            
        </div>
    
        <div class="col-sm-9">
            <h1 class="post-title">
            Hao Tang
            </h1>
            <p class="desc"></p>
          <p><span style="color: gray;">åŒ—äº¬å¤§å­¦ åŠ©ç†æ•™æˆ/ç ”ç©¶å‘˜ åšå£«ç”Ÿå¯¼å¸ˆ åšé›…é’å¹´å­¦è€…/æœªåé’å¹´å­¦è€…/æ™ºæºé’å¹´å­¦è€…</span><br>
            <span style="color: gray;">å›½å®¶çº§æµ·å¤–é«˜æ°´å¹³äººæ‰è®¡åˆ’å…¥é€‰è€… å›½å®¶ä¼˜ç§€ç•™å­¦ç”Ÿå¥–ï¼ˆå½’å›½ç±»ï¼‰</span>
            <br>Office: 5 Yiheyuan Road, Haidian District, Beijing, 100871, ChinağŸ‡¨ğŸ‡³<br> Email: bjdxtanghao@gmail.com / haotang@pku.edu.cn</p>

            <div class="social">
                <div class="contact-icons">
            <a href="mailto:bjdxtanghao@gmail.com" title="Email" rel="noopener noreferrer">
            <i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=9zJkeEMAAAAJ&hl=en" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/Ha0Tang" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://github.com/TH-AI-Lab-PKU" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github-alt"></i></a>
            <a href="https://www.linkedin.com/in/hao-tang-887475138/" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://dblp.uni-trier.de/pid/07/5751-5.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a>
            <a href="https://twitter.com/HaoTang_ai" title="X" target="_blank" rel="noopener noreferrer">
            <i class="fab fa-x-twitter"></i></a>
            <a href="https://orcid.org/0000-0002-2077-1246" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="fab fa-orcid"></i></a>
            <a href="https://www.scopus.com/authid/detail.uri?authorId=57208238003" title="SCOPUS" target="_blank" rel="noopener noreferrer"><i class="ai ai-scopus"></i></a>
            <a href="https://arxiv.org/a/tang_h_1.html" title="Arxiv" target="_blank" rel="noopener noreferrer"><i class="ai ai-arxiv"></i></a>
            <a href="https://zhuanlan.zhihu.com/p/710908497" title="Zhihu" target="_blank" rel="noopener noreferrer"><i class="fab fa-zhihu"></i></a>
                </div>
            </div>
   
        </div>
    </div>

    <div class="clearfix">
      <p>Hey, thanks for stopping by! ğŸ‘‹</p>

      <p>
        I am a tenure-track Assistant Professor (Ph.D. supervisor) at the School of Computer Science, Peking University, ChinağŸ‡¨ğŸ‡³, where I lead the Embodied and Generative Intelligence Lab and conduct research in computer vision, machine learning, robotics, and artificial intelligence. Previously, I held postdoctoral positions at both CMU (Robotics Institute), USAğŸ‡ºğŸ‡¸, and ETH ZÃ¼rich (Computer Vision Lab), SwitzerlandğŸ‡¨ğŸ‡­. My academic journey includes earning a master's degree from Peking University, ChinağŸ‡¨ğŸ‡³, and completing my Ph.D. (cum laude) at University of Trento, ItalyğŸ‡®ğŸ‡¹. Additionally, 
        I had the privilege of working and visiting several institutions, including University of Oxford (UKğŸ‡¬ğŸ‡§), MIT (USAğŸ‡ºğŸ‡¸), Harvard University (USAğŸ‡ºğŸ‡¸), IIAI (UAEğŸ‡¦ğŸ‡ª), Northeastern University (USAğŸ‡ºğŸ‡¸), NUS (SingaporeğŸ‡¸ğŸ‡¬), NTU (SingaporeğŸ‡¸ğŸ‡¬), University of Michigan (USAğŸ‡ºğŸ‡¸), UCLA (USAğŸ‡ºğŸ‡¸), UPenn (USAğŸ‡ºğŸ‡¸), University of Lille (FranceğŸ‡«ğŸ‡·), HKU (Hong KongğŸ‡­ğŸ‡°), and so on.
      </p>
      
      <p>
        Beyond academia, I have also had the honor of serving as senior technical consultants for numerous AI startups, including those in USAğŸ‡ºğŸ‡¸, UKğŸ‡¬ğŸ‡§, RomaniağŸ‡·ğŸ‡´, and ChinağŸ‡¨ğŸ‡³, with technologies ranging from efficient AI to 3D vision to AIGC to AI4Blockchain, etc.
      </p>

    </div>

          <div class="news">
            <h2>News & Events</h2>
            <div class="table-responsive" style="max-height: 30vw">
              <table class="table table-sm table-borderless">

                  <th scope="row"><strong style="color: red;">Hiring!</strong></th>
                  <td>
                    <strong style="color: red;">
                    We're hiring Postdoc/Ph.D./Master/Intern researchers on Generative AI, World Model, Spatial Intelligence, and Embodied AI for our PKU lab, feel free to reach out to me directly.</strong>
                  </td>
                </tr> 

                <th scope="row">2026-01</th>
                  <td>
                    We have 1 paper (All-in-One Image Restoration) accepted to <strong>TPAMI 2026</strong>, 4 papers (3D VLM on Ancient Greek Pottery + LVLM Hallucination + Brain-Inspired Stereo Depth Estimation + Human Video Generation) accepted to <strong>ICLR 2026</strong>, and 1 paper (Stereo Depth Estimation in Underwater Scenes) accepted to <strong>ICRA 2026</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-12</th>
                  <td>
                    We have 1 paper (Task-Aware Mixture-of-Experts) accepted to <strong>AAMAS 2026</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-11</th>
                  <td>
                    We have 2 papers (Multi-Task Adaptation + Diffusion Quantization) accepted to <strong>AAAI 2026</strong>, 2 papers (3D Captioning + Robot Modeling) accepted to <strong>3DV 2026</strong>, 1 paper (Cellular Phenotype Transition) accepted to <strong>AAAI 2026 Bridge AIMedHealth</strong> and 1 paper (Molecular Representation) accepted to <strong>AAAI 2026 Workshop AIDD</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-10</th>
                  <td>
                    ğŸ¤Honored to be invited by NVIDIA to share our latest research and vision on Shaping the Future with Generative and Embodied Intelligence, and we have 1 paper (Survey about Multimodal Alignment and Fusion) accepted to <strong>IJCV 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-09</th>
                  <td>
                    ğŸ‰I was elected as one of the <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/8" target="_blank">World's Top 2% Scientists in 2025 by Stanford University</a>, and we have 1 paper (Articulation and Diffusion for Robot Modeling) accepted to <strong>CoRL 2025 LSRW Workshop</strong>, 4 papers including 1 spotlight (Parameter Efficient Merging for MLLM + Diffusion-based Adversarial Attacks + Spatial Adversarial Alignment + Dental AI) accepted to <strong>NeurIPS 2025</strong>, and 1 paper (VLM for Open-Vocabulary Segmentation) accepted to <strong>CVIU 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-08</th>
                  <td>
                    ğŸ‰I was invited to serve a Area Chair (AC) at <strong>ICLR 2026</strong>, and we have 1 paper (VLA for Multi-Task Manipulation) accepted to <strong>CoRL 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-07</th>
                  <td>
                    ğŸ‰I was invited to serve a Senior Program Committee (SPC) at <strong>AAAI 2026</strong>, and we have 1 paper (Music-Guided Dance Video Synthesis) accepted to <strong>TPAMI 2025</strong>, 1 paper (Video Anomaly Detection) accepted to <strong>ACM MM 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-06</th>
                  <td>
                    We have 1 paper (Cellular Phenotypic Transdifferentiation) accepted to <strong>ICML 2025 Workshop</strong>, 1 paper (Monocular Depth Estimation) accepted to <strong>TCSVT 2025</strong>, 2 oral papers (Surgical Robot + Reconstruction and Editing in V2X Scenarios) accepted to <strong>IROS 2025</strong>, and 1 paper (Medical Image Segmentation) accepted to <strong>ICCV 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-05</th>
                  <td>
                    ğŸ‰I was invited to serve on the Editorial Board of <strong>Discover Artificial Intelligence</strong> (a journal by Springer Nature), I was also invited to serve as an Area Chair (AC) at <strong>EMNLP 2025</strong>, and we have 1 paper (Real-Time ViT on Mobile) accepted to <strong>IJCV 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-04</th>
                  <td>
                    We have 3 papers (Single-Step Image SR + In-Context Meta LoRA + Sparse MoE) accepted to <strong>IJCAI 2025</strong>, 1 paper (Synergistic Immunotherapy in Glioma) accepted to <strong>Advanced Science 2025</strong>, 1 paper (Continual Gesture Learning) accepted to <strong>IJCNN 2025</strong>, and 1 paper (Fake News Video Detection) accepted to <strong>TMM 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-03</th>
                  <td>
                    ğŸ‰I was invited to serve as an Area Chair (AC) at <strong>ACM MM 2025</strong>, and we have 1 paper (Accident Warning Agent) accepted to <strong>IV 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-02</th>
                  <td>
                    ğŸ‰ğŸ¤Honored to be invited by NVIDIA to share our latest research and vision on Bridging the Gap to Fault-Tolerant Quantum Computing, and I was invited to serve as an Area Chair (AC) at <strong>ACL 2025</strong>, a Senior Program Committee (SPC) at <strong>IJCAI 2025</strong>, and we have 3 papers including 1 oral (Mamba for Image Compression + 4D Reconstruction + Diffusion Fourier Neural Operator) accepted to <strong>CVPR 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-01</th>
                  <td>
                    We have 1 paper (SAR Automatic Target Recognition) accepted to <strong>TAES 2025</strong>, 1 paper (Person Image Generation) accepted to <strong>TPAMI 2025</strong>, 1 paper (Explainability in MLLMs) accepted to <strong>NAACL 2025 Main Conference</strong>, and 1 paper (Urological Surgical Robots) accepted to <strong>ICRA 2025</strong>. 
                  </td>
                </tr> 

                <th scope="row">2024-12</th>
                  <td>
                    We have 3 papers (Structured Pruning for LLM + FG-SBIR + Hair Transfer via Diffusion Model) accepted to <strong>AAAI 2025</strong> and 1 paper (Efficient Fine-Tuning of LLM) accepted to <strong>ICASSP 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-11</th>
                  <td>
                    We have 1 paper (Virtual Try-On) accepted to <strong>TMM 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-10</th>
                  <td>
                    We have 1 paper (Quantization on Bird's-Eye View Representation) accepted to <strong>WACV 2025</strong> and 1 paper (Semantic Segmentation on Autonomous Vehicles Platform) accepted to <strong>TCAD 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-09</th>
                  <td>
                    ğŸ‰I was elected as one of the <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/7" target="_blank">World's Top 2% Scientists in 2024 by Stanford University</a>,
                    and we have 1 paper (Camera-Agnostic Attack) accepted to <strong>NeurIPS 2024</strong> and 1 paper (Medical Image Segmentation) accepted to <strong>ACCV 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-08</th>
                  <td>
                    ğŸ¤I was invited as a speaker at <a href="https://cv-ac.github.io/MiGA2/" target="_blank">the 2nd Workshop & Challenge on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA)</a> at IJCAI 2024, and we have 2 papers (Guided Image Translation + 3D Human Pose Estimation) accepted to <strong>PR 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-07</th>
                  <td>
                    We have 6 papers (Motion Mamba + Dataset Growth + Story Visualization and Completion + Diffusion Model for Semantic Image Synthesis + Generalizable Image Editing + 3D Semantic Segmentation) accepted to <strong>ECCV 2024</strong>, 1 paper (Survey about Physical Adversarial Attack) accepted to <strong>TPAMI 2024</strong>, and 2 papers (Talking Head Avatar + Story Visualization and Continuation) accepted to <strong>ACM MM 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-06</th>
                  <td>
                    ğŸ‰I joined <strong>Peking University</strong> as an Assistant Professor.
                  </td>
                </tr> 

                <th scope="row">2024-04</th>
                  <td>
                    ğŸ‰I received offers from <strong>MIT</strong> and <strong>Harvard University</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-02</th>
                  <td>
                    We have 7 papers (Explanation for ViT + Faithfulness of ViT + Diffusion Policy for Versatile Navigation + Subject-Driven Generation [Final rating: 455] + Diffusion Model for 3D Hand Pose Estimation + Adversarial Learning for 3D Pose Transfer + Efficient Diffusion Distillation [224->235]) accepted to <strong>CVPR 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-01</th>
                  <td>
                    We have 1 paper (Architectural Layout Generation) accepted to <strong>TPAMI 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-12</th>
                  <td>
                    We have 1 paper (Sign Pose Sequence Generation) accepted to <strong>AAAI 2024</strong>.
                  </td>
                </tr> 


                <th scope="row">2023-10</th>
                  <td>
                    ğŸ‰I was elected as one of the <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/6" target="_blank">World's Top 2% Scientists in 2023 by Stanford University</a>
                    and we have 4 papers (BEV Perception + Efficient ViT + 3D Motion Transfer + Graph Distillation) accepted to <strong>NeurIPS 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-09</th>
                  <td>
                    We have 1 paper (Practical Blind Image Denoising) accepted to <strong>MIR 2023</strong> and 1 paper (Diffusion Model for HDR Deghosting) accepted to <strong>TCSVT 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-08</th>
                  <td>
                    ğŸ‰I received an offer from <strong>CMU</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-07</th>
                  <td>
                    We have 1 paper (Semantic Image Synthesis) accepted to <strong>TPAMI 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-06</th>
                  <td>
                    We have 1 paper (Visible-Infrared Person Re-ID) accepted to <strong>ICCV 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-05</th>
                  <td>
                    We have 2 papers (Image Restoration Dataset + 3D-Aware Video Generation) accepted to <strong>CVPRW 2023</strong> and 1 paper (3D Face Generation) accepted to <strong>JSTSP 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-04</th>
                  <td>
                    We have 1 paper (Speed-Aware Object Detection) accepted to <strong>ICML 2023</strong>, 2 papers (Lottery Ticket Hypothesis for ViT + Zero-shot Character Recognition) accepted to <strong>IJCAI 2023</strong>, 1 paper (3D Human Pose Estimation) accepted to <strong>PR 2023</strong>, and 1 paper (SAR Target Recognition) accepted to <strong>TGRS 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-03</th>
                  <td>
                    We have 6 papers (HDR Deghosting + Point Cloud Registration + Graph-Constrained House Generation + Mathematical Architecture Design + Text-to-Image Synthesis + Efficient Semantic Segmentation) accepted to <strong>CVPR 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-02</th>
                  <td>
                    We have 3 papers (Camouflaged Object Detection + Brain Vessel Image Segmentation + Cross-View Image Translation) accepted to <strong>ICASSP 2023</strong> and 1 paper (Camouflaged Object Detection) accepted to <strong>TCSVT 2023</strong>.
                  </td>
                </tr> 
               
                <tr>
                  <th scope="row">2023-01</th>
                  <td>
                    We have 1 paper (Semantic Image Synthesis) accepted to <strong>ICLR 2023</strong> and 1 paper (Human Reaction Generation) accepted to <strong>TMM 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-11</th>
                  <td>
                    We have 4 papers (Real-Time Segmentation + Wearable Design + Efficient ViT Training + Text-Guided Image Editing) accepted to <strong>AAAI 2023</strong>, 1 paper accepted (Person Pose and Facial Image Synthesis) to <strong>IJCV 2022</strong>, 1 paper (Salient Object Detection) accepted to <strong>TIP 2022</strong>, and 1 paper (Object Detection Transformer) accepted to <strong>TCSVT 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2022-10</th>
                  <td>
                    We have 1 paper (Sinusoidal Neural Radiance Fields) accepted to <strong>BMVC 2022</strong> and 1 paper (Guided Image-to-Image Translation) accepted to <strong>TPAMI 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-09</th>
                  <td>
                    We have 1 paper (Facial Expression Translation) accepted to <strong>TAFFC 2022</strong> and 1 paper (Ship Detection) accepted to <strong>TGRS 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-07</th>
                  <td>
                     We have 5 papers (Real-Time SR + Video SR + Soft Token Pruning for ViT + 3D-Aware Human Synthesis + Video Semantic Segmentation) accepted to <strong>ECCV 2022</strong>, 1 paper (Gaze Correction and Animation) accepted to <strong>TIP 2022</strong>, and 1 paper (Cross-view Panorama Image Synthesis) accepted to <strong>PR 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-06</th>
                  <td>
                     We have 2 papers (Character Image Restoration + Character Image Denoising) accepted to <strong>ACM MM 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-04</th>
                  <td>
                    We have 1 paper (Real-Time Portrait Stylization) accepted to <strong>IJCAI 2022</strong>, 1 paper (Wide-Context Transformer for Semantic Segmentation) accepted to <strong>TGRS 2022</strong>, and 1 paper (Incremental Learning for Semantic Segmentation) accepted to <strong>TMM 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2022-03</th>
                  <td>
                    We have 5 papers including 1 oral (Text-to-Image Synthesis + 3D Human Pose Estimation + Text-Driven Image Manipulation + 3D Face Modeling + 3D Face Restoration) accepted to <strong>CVPR 2022</strong>, 1 paper (Image Generation) accepted to <strong>TPAMI 2022</strong>, and 1 paper (Cross-View Panorama Image Synthesis) accepted to <strong>TMM 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-12</th>
                  <td>
                    We have 2 papers (Generalized 3D Pose Transfer + Audio-Visual Speaker Tracking) accepted to <strong>AAAI 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-11</th>
                  <td>
                    We have 1 paper (Building Extraction in VHR Remote Sensing Images) accepted to <strong>TIP 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-10</th>
                  <td>
                    We have 3 papers (Cross-View Image Translation + Data-driven 3D Animation + Natural Image Matting) accepted to <strong>BMVC 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-08</th>
                  <td>
                    We have 1 paper (Layout-to-Image Translation) accepted to <strong>TIP 2021</strong> and 1 paper (Unpaired Image-to-Image Translation) accepted to <strong>TNNLS 2021</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2021-07</th>
                  <td>
                    We have 2 papers (Continuous Pixel-Wise Prediction + Unsupervised 3D Pose Transfer) accepted to <strong>ICCV 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-06</th>
                  <td>
                    We have 1 paper (Cross-View Exocentric to Egocentric Video Synthesis) accepted to <strong>ACM MM 2021</strong> and 1 paper (Total Generate) accepted to <strong>TMM 2021</strong>.
                  </td>
                </tr> 

                <th scope="row">2021-05</th>
                  <td>
                    ğŸ‰I received an offer from <strong>ETH Zurich</strong>.
                  </td>
                </tr> 

                <th scope="row">2020-09</th>
                  <td>
                    ğŸ‰I received an offer from <strong>IIAI</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2020-08</th>
                  <td>
                    We have 1 paper (Person Image Generation) accepted to <strong>BMVC 2020</strong>, 2 papers (Semantic Image Synthesis + Unsupervised Gaze Correction and Animation) accepted to <strong>ACM MM 2020</strong>, and 1 paper (Controllable Image-to-Image Translation) accepted to <strong>TIP 2020</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2020-07</th>
                  <td>
                    We have 1 paper (Person Image Generation) accepted to <strong>ECCV 2020</strong>.
                  </td>
                </tr>

                <tr>
                  <th scope="row">2020-05</th>
                  <td>
                    We have 1 paper (Deep Dictionary Learning and Coding) accepted to <strong>TNNLS 2020</strong> and 1 paper (Semantic Segmentation of Remote Sensing Images) accepted to <strong>TGRS 2020</strong>.
                  </td>
                </tr>

                <tr>
                  <th scope="row">2020-02</th>
                  <td>
                    We have 1 paper (Semantic-Guided Scene Generation) accepted to <strong>CVPR 2020</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2019-07</th>
                  <td>
                    We have 1 paper (Keypoint-Guided Image Generation) accepted to <strong>ACM MM 2019</strong>.
                  </td>
                </tr>

                <th scope="row">2019-05</th>
                  <td>
                    ğŸ‰I received an offer from <strong>University of Oxford</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2019-02</th>
                  <td>
                    We have 1 paper (Cross-View Image Translation) accepted to <strong>CVPR 2019</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2018-06</th>
                  <td>
                    We have 1 paper (Hand Gesture-to-Gesture Translation) accepted to <strong>ACM MM 2018</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2018-02</th>
                  <td>
                    We have 1 paper (Monocular Depth Estimation) accepted to <strong>CVPR 2018</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2016-07</th>
                  <td>
                    We have 1 paper (Large Scale Image Retrieval) accepted to <strong>IJCAI 2016</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2015-08</th>
                  <td>
                    We have 1 paper (Gender Classification) accepted to <strong>ACM MM 2015</strong>.
                  </td>
                </tr>
                
              </table>
            </div> 
          </div>


<div class="lab" style="margin-top: 40px;">
      <h2>Position Openings</h2>
      <div class="box">
            <p>For prospective collaborators interested in Generative AI, World Model, Spatial Intelligence, and Embodied AI, we are offering multiple positions for highly motivated Postdoc/Ph.D./Master/RA/externship/internship/visiting students. If you are interested in joining our group, please email me with your self-introduction, the project of interest (including the problem you are trying to solve and how you plan to solve it, being as specific as possible), your transcript, and CV to haotang@pku.edu.cn/bjdxtanghao@gmail.com. I'm sorry that I may not be able to respond to every email, but I assure you that your message will stand out if you have a strong research background.</p>

            <p>For Ph.D./Postdoc/Master applicants, we have several openings for domestic students each year. Please reach out at least 6 months prior to the application deadline.

            For international students, PKU CS offers a variety of programs in English, including <a href="https://twitter.com/HaoTang_ai/status/1847175500558750055/photo/1" target="_blank">Master's</a>, <a href="http://www.studyatpku.com" target="_blank">Ph.D. programs</a>, <a href="https://cs.pku.edu.cn/English/Internationalization/Summer_Winter_Camp1.htm" target="_blank">Summer/Winter Schools</a>, and <a href="https://cs.pku.edu.cn/English/Internationalization/Internship_Program.htm" target="_blank">various other options</a>. Feel free to reach out if you are interested or have any questions.

            For RA/externship/internship/visiting students, we welcome undergraduate and graduate students from all over the world to apply for >6 months research internship. Our RAs/interns/visitors have published many top-tier conference/journal papers (e.g., TPAMIã€CVPRã€NeurIPS) and have been admitted to Postdoc/Ph.D./Master programs in prestigious institutions such as MIT, Harvard, Google, Brown University, UMich, University of Toronto, Caltech, UCSC, ETH ZÃ¼rich, NTU, NUS, and TUM.</p>
      </div>
  
      <h2>Research Lab</h2>
      <div class="box">

            <p>The mission of our research lab is to harness AI to address real-world challenges by bring the gap between digital generation and physical interaction. Our research priorities include Embodied AI, Generative AI (including LLM), and AI4Quantum.</p>

        <ul>
            <li>Qilin Wang (PhD, previously from Fudan University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Siyuan Qian (PhD, <a href="https://scholar.google.com/citations?user=voqw10cAAAAJ&hl=en" target="_blank">w/ Shanghang Zhang</a>, previously from BUAA, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Haoyu Wang (PhD, <a href="https://scholar.google.com/citations?user=7phvKK4AAAAJ&hl=en" target="_blank">w/ Shiliang Zhang</a>, previously from HIT, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Zhen Chen (PhD, <a href="https://scholar.google.com/citations?user=7phvKK4AAAAJ&hl=en" target="_blank">w/ Shiliang Zhang</a>, previously from Tongji University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Derek Zeng (Master, previously from University of Waterloo, CanadağŸ‡¨ğŸ‡¦)</li>
            <li>Jiarui Ye (Master, previously from NUAA, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Xiaoyuan Wang (Visiting from CMU, USAğŸ‡ºğŸ‡¸)</li>
            <li>Haozhan Tang (Visiting from CMU, USAğŸ‡ºğŸ‡¸)</li>
            <li>Wenbo Gou (Visiting from CMU, USAğŸ‡ºğŸ‡¸)</li>
            <li>Zhenyu Lu (Visiting from CMU, USAğŸ‡ºğŸ‡¸)</li>
            <li>Rohan Siva (Visiting from UT Austin, USAğŸ‡ºğŸ‡¸)</li>
            <li>Jun Liu (Visiting from NEU, USAğŸ‡ºğŸ‡¸)</li>
            <li>Changdi Yang (Visiting from NEU, USAğŸ‡ºğŸ‡¸)</li>
            <li>Na Li (Visiting from Goldman Sachs & UPenn, USAğŸ‡ºğŸ‡¸)</li>
            <li>Zihao Wang (Visiting from UPenn, USAğŸ‡ºğŸ‡¸)</li>
            <li>Yao Gong (Visiting from UPenn, USAğŸ‡ºğŸ‡¸)</li>
            <li>Junjie Zeng (Visiting from UMich, USAğŸ‡ºğŸ‡¸)</li>
            <li>Yuling Feng (Visiting from UMich, USAğŸ‡ºğŸ‡¸)</li>
            <li>Peng Huang (Visiting from Boston University, USAğŸ‡ºğŸ‡¸)</li>
            <li>Xiaoyi Liu (Visiting from Washington University in St. Louis, USAğŸ‡ºğŸ‡¸ -> now Ph.D. at Brown University, USAğŸ‡ºğŸ‡¸)</li>
            <li>Kang Chen (Visiting from Rensselaer Polytechnic Institute, USAğŸ‡ºğŸ‡¸)</li>
            <li>Linxi Wu (Visiting from University of North Carolina at Chapel Hill, USAğŸ‡ºğŸ‡¸)</li>
            <li>Bin Xie (Visiting from IIT, USAğŸ‡ºğŸ‡¸)</li>
            <li>Huixiu Jiang (Visiting from IIT, USAğŸ‡ºğŸ‡¸)</li>
            <li>Zitong Zhang (Visiting from University of Louisville, USAğŸ‡ºğŸ‡¸)</li>
            <li>Wanru Cheng (Visiting from University of Toronto, CanadağŸ‡¨ğŸ‡¦)</li>
            <li>Peize Li (Visiting from KCL, UKğŸ‡¬ğŸ‡§)</li>
            <li>Jingyi Wan (Visiting from University of Cambridge, UKğŸ‡¬ğŸ‡§)</li>
            <li>Xuanyu Lai (Visiting from ICL, UKğŸ‡¬ğŸ‡§)</li>
            <li>Yitong Luo (Visiting from ICL, UKğŸ‡¬ğŸ‡§)</li>
            <li>Haitao He (Visiting from QMUL, UKğŸ‡¬ğŸ‡§)</li>
            <li>Baohua Yin (Visiting from University of Sussex, UKğŸ‡¬ğŸ‡§)</li>
            <li>Enze Wang (Visiting from Technical University of Munich, GermanyğŸ‡©ğŸ‡ª)</li>
            <li>Yitao Song (Visiting from Moscow State University, RussiağŸ‡·ğŸ‡º)</li>
            <li>Zhiguang Han (Visiting from NTU, SingaporeğŸ‡¸ğŸ‡¬)</li>
            <li>Zhen Long (Visiting from NUS, SingaporeğŸ‡¸ğŸ‡¬)</li>
            <li>Ali Haider (Visiting from Kyung Hee University, South KoreağŸ‡°ğŸ‡·)</li>
            <li>Amitoj Singh Miglani (Visiting from IIT Roorkee, IndiağŸ‡®ğŸ‡³)</li>
            <li>Pirzada Suhail (Visiting from IIT Bombay, IndiağŸ‡®ğŸ‡³)</li>
            <li>Siddhant Pathak (Visiting from IIT BHU, IndiağŸ‡®ğŸ‡³)</li>
            <li>Vrushank Ajay Ahire (Visiting from IIT Ropar, IndiağŸ‡®ğŸ‡³)</li>
            <li>Mohammed Shehin (Visiting from NIT Calicut, IndiağŸ‡®ğŸ‡³)</li>
            <li>Ziwei Li (Visiting from KAUST, Saudi ArabiağŸ‡¸ğŸ‡¦)</li>
            <li>Ahmad Imran (Visiting from NUST, PakistanğŸ‡µğŸ‡°)</li>
            <li>Zeyu Zhang (Visiting from Australian National University, AustraliağŸ‡¦ğŸ‡º)</li>
            <li>Hongpeng Wang (Visiting from University of Sydney, AustraliağŸ‡¦ğŸ‡º)</li>
            <li>Jinqi Liao (Visiting from University of Sydney, AustraliağŸ‡¦ğŸ‡º)</li>
            <li>Zeyu Ren (Visiting from University of Melbourne, AustraliağŸ‡¦ğŸ‡º)</li>
            <li>Haihang Wu (Visiting from University of Melbourne, AustraliağŸ‡¦ğŸ‡º)</li>
            <li>Ziang Li (Visiting from UTS, AustraliağŸ‡¦ğŸ‡º)</li>
            <li>Zhixing Wang (Visiting from University of Malaya, MalaysiağŸ‡²ğŸ‡¾)</li>
            <li>Pakawat Phasook (Visiting from King Mongkutâ€™s University of Technology Thonburi, ThailandğŸ‡¹ğŸ‡­)</li>
            <li>Ahmed Eldaw Mohamed (Visiting from University of Cape Town, South AfricağŸ‡¿ğŸ‡¦)</li>
            <li>Hongfeng Lai (Visiting from University of Hong Kong, Hong KongğŸ‡­ğŸ‡°)</li>
            <li>Yuxin Cheng (Visiting from University of Hong Kong, Hong KongğŸ‡­ğŸ‡°)</li>
            <li>Zicheng Liu (Visiting from University of Hong Kong, Hong KongğŸ‡­ğŸ‡°)</li>
            <li>Yihua Shao (Visiting from City University of Hong Kong, Hong KongğŸ‡­ğŸ‡°)</li>
            <li>Yuxuan Fan (Visiting from HKUST (Guangzhou), ChinağŸ‡¨ğŸ‡³)</li>
            <li>Nonghai Zhang (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Dongjian Li (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Rui Yang (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Mingyu Li (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Zhaohui Wang (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Ziyan Mao (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Xinran Kuang (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Keyu Chen (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Di Yu (Visiting from Tsinghua University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Yuxuan Zhang (Visiting from Shanghai Jiao Tong University, ChinağŸ‡¨ğŸ‡³ -> now Ph.D. at CUHK, Hong KongğŸ‡­ğŸ‡°)</li>
            <li>Renkai Wu (Visiting from Shanghai Jiao Tong University -> now Ph.D. at Tsinghua University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Junxian Li (Visiting from Shanghai Jiao Tong University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Jiaxing Zhang (Visiting from Sichuan University -> now Ph.D. at Shanghai Jiao Tong University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Hui Wei (Visiting from Wuhan University, ChinağŸ‡¨ğŸ‡³ -> now Postdoc at University of Oulu, FinlandğŸ‡«ğŸ‡®)</li>
            <li>Xiaofeng Zhang (Visiting from Shanghai Jiao Tong University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Ting Huang (Visiting from Shanghai University of Engineering Science, ChinağŸ‡¨ğŸ‡³)</li>
            <li>I-Tak Ieong (Visiting from Tongji University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Kunze Jiang (Visiting from USTC, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Lei Xin (Visiting from Wuhan University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Fanhu Zeng (Visiting from CAISA, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Mingju Gao (Visiting from ICT, CAS, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Songtao Li (Visiting from Northeastern Universiyty, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Jiawei Mao (Visiting from Hangzhou Dianzi University, ChinağŸ‡¨ğŸ‡³ -> now Ph.D. at UCSC, USAğŸ‡ºğŸ‡¸)</li>
            <li>Qinhua Xie (Visiting from East China Normal University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Zihang Liu (Visiting from Beijing Institute of Technology, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Aoming Liang (Visiting from Westlake University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Sifan Li (Visiting from Liaoning University, ChinağŸ‡¨ğŸ‡³)</li>
        </ul>   

        <p><strong>Former members and visitors:</strong> 

        <ul>
            <li>Youran Qu (RA from Peking University, ChinağŸ‡¨ğŸ‡³ -> now Master at Dartmouth College, USAğŸ‡ºğŸ‡¸)</li>
            <li>Kaiwen Shi (RA from Peking University, ChinağŸ‡¨ğŸ‡³ -> now PhD at University of Notre Dame, USAğŸ‡ºğŸ‡¸)</li>
            <li>Lujing Xie (RA from Peking University, ChinağŸ‡¨ğŸ‡³ -> now PhD at University of Texas at Dallas, USAğŸ‡ºğŸ‡¸)</li>
            <li>Yaowu Zhang (RA from Peking University, ChinağŸ‡¨ğŸ‡³ -> now Master at Institute of Computing Technology, Chinese Academy of Sciences, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Haoran Li (RA from Peking University, ChinağŸ‡¨ğŸ‡³ -> now Master at Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Yaoxiang Xiong (RA from Peking University, ChinağŸ‡¨ğŸ‡³ -> now Master at Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Jinxian Ren (RA from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Guillaume Thiry (RA from ETH ZÃ¼rich, SwitzerlandğŸ‡¨ğŸ‡­ -> now Software Engineer at Google, SwitzerlandğŸ‡¨ğŸ‡­)</li>
            <li>Sherwin Bahmani (RA from ETH ZÃ¼rich, SwitzerlandğŸ‡¨ğŸ‡­ -> now Ph.D. at University of Toronto, CanadağŸ‡¨ğŸ‡¦)</li>
            <li>Sanghwan Kim (RA from ETH ZÃ¼rich, SwitzerlandğŸ‡¨ğŸ‡­ -> now Ph.D. at TUM, GermanyğŸ‡©ğŸ‡ª)</li>
            <li>Alexandros Delitzas (RA from ETH ZÃ¼rich, SwitzerlandğŸ‡¨ğŸ‡­ -> now Ph.D. at ETH ZÃ¼rich and Max Planck Institute for Informatics, SwitzerlandğŸ‡¨ğŸ‡­ and GermanyğŸ‡©ğŸ‡ª)</li>
            <li>Jingfeng Rong (RA from ETH ZÃ¼rich, SwitzerlandğŸ‡¨ğŸ‡­ -> now Ph.D. at Swiss Finance Institute, SwitzerlandğŸ‡¨ğŸ‡­)</li>
            <li>Yitong Xia (RA from ETH ZÃ¼rich, SwitzerlandğŸ‡¨ğŸ‡­ -> now Ph.D. at NTU, SingaporeğŸ‡¸ğŸ‡¬)</li>
            <li>Boyan Duan (RA, now Master at ETH ZÃ¼rich, SwitzerlandğŸ‡¨ğŸ‡­)</li>
            <li>Baptiste Chopin (RA, now Postdoc at INRIA, FranceğŸ‡«ğŸ‡·)</li>
            <li>Chenyang Gu (RA from Peking University, ChinağŸ‡¨ğŸ‡³ -> now Ph.D. at Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Kosta Gjorgjievski (RA from UCLA, USAğŸ‡ºğŸ‡¸ -> now Master at Tsinghua University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Xiaoyu Yi (RA from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
        </ul>   
   
      </div>

      <h2>Teaching</h2>
      <div class="box">   
            <ul>
              <li>2025 Fall, <a href="https://ha0tang.github.io/DLLM_PKU" target="_blank">PKU: Deep Learning and Large Models</a></li>  
              <li>2025 Spring, <a href="https://ha0tang.github.io/DGM_PKU" target="_blank">PKU: Deep Generative Models</a></li>   
              <li>2025 Spring, PKU: Video Encoding and Understanding</li>   
            </ul>   

      </div>

      <h2>International Collaborations</h2>
      <div class="box">

            <p>
            Our lab maintains strong collaborative relationships with several leading international research institutions, including 
              <ul>
                  <li>USAğŸ‡ºğŸ‡¸: MIT, Harvard, Stanford University, CMU, Princeton University, UIUC, UMich, Northeastern University, University of Maryland, University of Texas at Austin, UC Irvine, University of Illinois at Chicago, Illinois Institute of Technology, University of Connecticut, Texas State University, University of Georgia, Clemson University, University of Oregon, College of William & Mary</li>
                  <li>CanadağŸ‡¨ğŸ‡¦: University of Toronto, Simon Fraser University</li>
                  <li>SwitzerlandğŸ‡¨ğŸ‡­: ETH ZÃ¼rich, EPFL</li>
                  <li>UKğŸ‡¬ğŸ‡§: University of Oxford, University of Cambridge, University of Leicester, University of Warwick</li>
                  <li>ItalyğŸ‡®ğŸ‡¹: University of Trento, FBK, Politecnico di Milano, University of Modena e Reggio Emilia</li>
                  <li>GermanyğŸ‡©ğŸ‡ª: TUM, University of WÃ¼rzburg</li>
                  <li>FranceğŸ‡«ğŸ‡·: INRIA, University of Lille</li>
                  <li>FinlandğŸ‡«ğŸ‡®: University of Oulu</li>
                  <li>NetherlandsğŸ‡³ğŸ‡±: TU Delft</li>
                  <li>BelgiumğŸ‡§ğŸ‡ª: KU Leuven</li>
                  <li>BulgariağŸ‡§ğŸ‡¬: INSAIT</li>
                  <li>SingaporeğŸ‡¸ğŸ‡¬: NUS, NTU</li>
                  <li>JapanğŸ‡¯ğŸ‡µ: University of Tokyo, National Institute of Informatics</li>
                  <li>South KoreağŸ‡°ğŸ‡·: Sungkyunkwan University</li>
                  <li>AustraliağŸ‡¦ğŸ‡º: University of Adelaide, ANU, Monash University, University of Technology Sydney</li>
                  <li>UAEğŸ‡¦ğŸ‡ª: IIAI, MBZUAI</li>  
                  <li>HongKongğŸ‡­ğŸ‡°: University of Hong Kong, Hong Kong University of Science and Technology</li>  
              </ul>   
            I am deeply grateful for the opportunities to collaborate with such esteemed institutions and for the valuable contributions they have made to our joint research efforts.

            Additionally, we maintain long-term collaborations with industry, including Google, Meta, Amazon, Cisco, Western Digital, Mercedes-Benz, Xiaohongshu, Alibaba, Tencent, etc, aiming to translate cutting-edge research into practical applications and drive technological advancement.
      </div>

    </div>
      
          <div class="publications">
            <h2>Featured Publications</h2>
            <h5>(Including CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, AAAI, IJCAI, ACM MM, ICRA, IROS, CoRL, NAACL, 3DV, TPAMI, IJCV)</h5>
            <p class="post-description"><sup>â€ </sup>My Students or Interns, <sup>*</sup>Corresponding Author(s)</p>
            <ol class="bibliography">

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Fidelity-Aware Data Composition for Robust Robot Generalization</div>
          <!-- Author -->
          <div class="author">

            Â Zizhao Tong, Â Di Chen, Â Sicheng Hu, Â Hongwei Fan, Â Liliang Chen, Â Guanghui Ren, Â <em>Hao Tang</em>, Â Hao Dong, Â Ling Shao</div>
          <div class="periodical">
            
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2509.24797" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">UniVid: The Open-Source Unified Video Model</div>
          <!-- Author -->
          <div class="author">

            Â Jiabin Luo, Â Junhui Lin, Â Zeyu Zhang, Â Biao Wu, Â Meng Fang, Â Ling Chen, Â <em>Hao Tang*</em></div>
          <div class="periodical">
            
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2509.24200" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/AIGeeksGroup/UniVid" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Nav-R1: Reasoning and Navigation in Embodied Scenes</div>
          <!-- Author -->
          <div class="author">

          Â Qingxiang Liu, Â Ting Huang, Â Zeyu Zhang, Â <em>Hao Tang*</em></div>
          <div class="periodical">

            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2509.10884" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/AIGeeksGroup/Nav-R1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">ReMoMask: Retrieval-Augmented Masked Motion Generation</div>
          <!-- Author -->
          <div class="author">

          Â Zhengdao Li, Â Siheng Wang, Â Zeyu Zhang, Â <em>Hao Tang*</em></div>
          <div class="periodical">

            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2508.02605" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/AIGeeksGroup/ReMoMask" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding</div>
          <!-- Author -->
          <div class="author">

          Â Ting Huang, Â Zeyu Zhang, Â <em>Hao Tang*</em></div>
          <div class="periodical">

            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2507.23478" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/AIGeeksGroup/3D-R1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance</div>
          <!-- Author -->
          <div class="author">

          Â Haijie Yang, Â Zhenyu Zhang, Â <em>Hao Tang</em>, Â Jianjun Qian, Â Jian Yang</div>
          <div class="periodical">
          
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2503.22225" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">FE-UNet: Frequency Domain Enhanced U-Net with Segment Anything Capability for Versatile Image Segmentation</div>
          <!-- Author -->
          <div class="author">

        Â Guohao Huo, Â Ruiting Dai, Â Ling Shao, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2502.03829" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">RFMedSAM 2: Automatic Prompt Refinement for Enhanced Volumetric Medical Image Segmentation with SAM 2</div>
          <!-- Author -->
          <div class="author">

        Â Bin Xie, Â <em>Hao Tang</em>, Â Yan Yan, Â Gady Agam</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2502.02741" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Self-Prompt SAM: Medical Image Segmentation via Automatic Prompt SAM Adaptation</div>
          <!-- Author -->
          <div class="author">

        Â Bin Xie, Â <em>Hao Tang</em>, Â Dawen Cai, Â Yan Yan, Â Gady Agam</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2502.00630" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">UDiTQC: U-Net-Style Diffusion Transformer for Quantum Circuit Synthesis</div>
          <!-- Author -->
          <div class="author">

        Â Zhiwei Chen, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2501.16380" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Artificial Intelligence for Quantum Error Correction: A Comprehensive Review</div>
          <!-- Author -->
          <div class="author">
          
          Â Zihao Wang, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2412.20380" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="https://thequantuminsider.com/2025/01/06/ai-for-quantum-error-correction-a-comprehensive-guide-to-using-artificial-intelligence-to-improve-quantum-error-correction/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">PolSAM: Polarimetric Scattering Mechanism Informed Segment Anything Model</div>
          <!-- Author -->
          <div class="author">
          
          Â Yuqing Wang, Â Zhongling Huang, Â Shuxin Yang, Â <em>Hao Tang</em>, Â Xiaolan Qiu, Â Junwei Han, Â Dingwen Zhang</div>
          
          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2412.12737" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/XAI4SAR/PolSAM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Artificial Intelligence for Central Dogma-Centric Multi-Omics: Challenges and Breakthroughs</div>
          <!-- Author -->
          <div class="author">
          
          Â Lei Xin, Â Caiyun Huang, Â Hao Li, Â Shihong Huang, Â Yuling Feng, Â Zhenglun Kong, Â Zicheng Liu, Â Siyuan Li, Â Chang Yu, Â Fei Shen, Â <em>Hao Tang*</em></div>
          
          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2412.12668" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Text-to-Image Synthesis: A Decade Survey</div>
          <!-- Author -->
          <div class="author">
          
          Â Nonghai Zhang, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2411.16164" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">KMM: Key Frame Mask Mamba for Extended Motion Generation</div>
          <!-- Author -->
          <div class="author">
          
          Â Zeyu Zhang, Â Hang Gao, Â Akide Liu, Â Qi Chen, Â Feng Chen, Â Yiran Wang, Â Danning Li, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2411.06481" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/steve-zeyu-zhang/KMM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">GWQ: Gradient-Aware Weight Quantization for Large Language Models</div>
          <!-- Author -->
          <div class="author">
          
          Â Yihua Shao, Â Siyu Liang, Â Xiaolin Lin, Â Zijian Ling, Â Zixian Zhu, Â Minxi Yan, Â Haiyang Liu, Â Siyu Chen, Â Ziyang Yan, Â Yilan Meng, Â Chenyu Zhang, Â Haotong Qin*, Â Michele Magno, Â Yang Yang, Â Zhen Lei, Â Yan Wang, Â Jingcai Guo, Â Ling Shao, Â <em>Hao Tang*</em></div>


          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2411.00850" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">M<sup>2</sup>M: Learning Controllable Multi of Experts and Multi-Scale Operators Are the Partial Differential Equations Need</div>
          <!-- Author -->
          <div class="author">
          
          Â Aoming Liang, Â Zhaoyang Mu, Â Pengxiao Lin, Â Cong Wang,  Â Mingming Ge, Â Ling Shao, Â Dixia Fan*, Â <em>Hao Tang*</em></div>


          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2410.11617" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Barbie: Text to Barbie-Style 3D Avatars</div>
          <!-- Author -->
          <div class="author">
          
          Â Xiaokun Sun, Â Zhenyu Zhang, Â Ying Tai, Â Qian Wang, Â <em>Hao Tang</em>, Â Zili Yi, Â Jian Yang</em></div>


          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2408.09126" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/2017211801/Barbie" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">InfiniMotion: Mamba Boosts Memory in Transformer for Arbitrary Long Motion Generation</div>
          <!-- Author -->
          <div class="author">
          
          Â Zeyu Zhang, Â Akide Liu, Â Qi Chen, Â Feng Chen, Â Ian Reid, Â Richard Hartley, Â Bohan Zhuang, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2407.10061" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://steve-zeyu-zhang.github.io/InfiniMotion/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">A Survey on Multimodal Wearable Sensor-based Human Action Recognition</div>
          <!-- Author -->
          <div class="author">
          
          Â Jianyuan Ni, Â <em>Hao Tang</em>, Â Syed Tousiful Haque, Â Yan Yan, Â Anne HH Ngu</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.15349" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">StableGarment: Garment-Centric Generation via Stable Diffusion</div>
          <!-- Author -->
          <div class="author">
          
          Â Rui Wang, Â Hailong Guo, Â Jiaming Liu, Â Huaxia Li, Â Haibo Zhao, Â Xu Tang, Â Yao Hu, Â <em>Hao Tang</em>, Â Peipei Li</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.10783" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/logn-2024/StableGarment" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Robotics_and_Automation" rel="external nofollow noopener" target="_blank">ICRA</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes</div>
          <!-- Author -->
          <div class="author">

          Â Zhengri Wu, Â Yiran Wang, Â Yu Wen, Â Zeyu Zhang, Â Biao Wu, Â <em>Hao Tang*</em></div>
          <div class="periodical">
            
            In <em>ICRA</em>, 2026, Vienna, Austria
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2509.16415" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/AIGeeksGroup/StereoAdapter" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations" rel="external nofollow noopener" target="_blank">ICLR</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery</div>
          <!-- Author -->
          <div class="author">

            Â Nonghai Zhang, Â Zeyu Zhang, Â Jiazi Wang, Â Yang Zhao, Â <em>Hao Tang*</em></div>
          <div class="periodical">
            
            In <em>ICLR</em>, 2026, Rio de Janeiro, Brazil
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2510.04479" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/AIGeeksGroup/VaseVQA-3D" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations" rel="external nofollow noopener" target="_blank">ICLR</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Hallucination Begins Where Saliency Drops</div>
          <!-- Author -->
          <div class="author">

            Â Xiaofeng Zhang, Â Yuanchao Zhu, Â Chaochen Gu, Â Xiaosong Yuan, Â Qiyan Zhao, Â Jiawei Cao, Â Feilong Tang, Â Sinan Fan, Â Yaomin Shen, Â Chen Shen, Â <em>Hao Tang</em></div>
          <div class="periodical">
            
            In <em>ICLR</em>, 2026, Rio de Janeiro, Brazil
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2601.20279" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/zhangbaijin/LVLMs-Saliency" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations" rel="external nofollow noopener" target="_blank">ICLR</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">MoSA: Motion-Coherent Human Video Generation via Structure-Appearance Decoupling</div>
          <!-- Author -->
          <div class="author">

          Â Haoyu Wang, Â <em>Hao Tang</em>, Â Donglin Di, Â Zhilu Zhang, Â Wangmeng Zuo, Â Feng Gao, Â Siwei Ma, Â Shiliang Zhang</div>
          <div class="periodical">
            
            In <em>ICLR</em>, 2026, Rio de Janeiro, Brazil
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2508.17404" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://hywang2002.github.io/MoSA/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations" rel="external nofollow noopener" target="_blank">ICLR</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">SpikeStereoNet: A Brain-Inspired Framework for Stereo Depth Estimation from Spike Streams</div>
          <!-- Author -->
          <div class="author">

          Â Zhuoheng Gao, Â Yihao Li, Â Jiyao Zhang, Â Rui Zhao, Â Tong Wu, Â <em>Hao Tang</em>, Â Zhaofei Yu, Â Hao Dong, Â Guozhang Chen, Â Tiejun Huang</div>
          <div class="periodical">
            
            In <em>ICLR</em>, 2026, Rio de Janeiro, Brazil
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2505.19487" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" rel="external nofollow noopener" target="_blank">TPAMI</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">AllRestorer: All-in-One Transformer for Image Restoration under Composite Degradations</div>
          <!-- Author -->
          <div class="author">
          
          Â Jiawei Mao, Â Yu Yang, Â Xuesong Yin, Â Ling Shao, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2026
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2411.10708" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Autonomous_Agents_and_Multiagent_Systems" target="_blank">AAMAS</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Resolving Task Objective Conflicts in Unified Multimodal Understanding and Generation via Task-Aware Mixture-of-Experts</div>
          <!-- Author -->
          <div class="author">

          Â Jiaxing Zhang, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>AAMAS</em> 2026, Paphos, Cyprus
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2506.03591" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">ICM-Fusion: In-Context Meta-Optimized LoRA Fusion for Multi-Task Adaptation</div>
          <!-- Author -->
          <div class="author">

          Â Yihua Shao, Â Xiaofeng Lin, Â Xinwei Long, Â Siyu Chen, Â Minxi Yan, Â Yang Liu, Â Ziyang Yan, Â Ao Ma, Â <em>Hao Tang</em>, Â Jingcai Guo</div>

          <div class="periodical">
          
            In <em>AAAI</em> 2026, Singapore City, Singapore
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2508.04153" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">TR-DQ: Time-Rotation Diffusion Quantization</div>
          <!-- Author -->
          <div class="author">
            
          Â Yihua Shao, Â Deyang Lin, Â Minxi Yan, Â Siyu Chen, Â Fanhu Zeng, Â Minwen Liao, Â Ao Ma, Â Ziyang Yan, Â Haozhe Wang, Â Yan Wang, Â Zhi Chen, Â Xiaofeng Cao, Â Haotong Qin*, Â <em>Hao Tang*</em>, Â Jingcai Guo*</div>

          <div class="periodical">
          
            In <em>AAAI</em> 2026, Singapore City, Singapore
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2503.06564" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://3dvconf.github.io/2026/" target="_blank">3DV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">3D Coca: Contrastive Learners Are 3D Captioners</div>
          <!-- Author -->
          <div class="author">

          Â Ting Huang, Â Zeyu Zhang, Â Yemin Wang, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>3DV</em> 2026, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2504.09518" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/AIGeeksGroup/3DCoCa" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://3dvconf.github.io/2026/" target="_blank">3DV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">GRADRobot: Geometry-Aware Rendering with Articulation and Diffusion for Robot Modeling</div>
          <!-- Author -->
          <div class="author">
          
          Â Yunlong Li, Â Boyuan Chen, Â Chongjie Ye, Â Bohan Li, Â Zhaoxi Chen, Â Shaocong Xu, Â <em>Hao Tang</em>, Â Hao Zhao</div>

          <div class="periodical">
          
            In <em>3DV</em> 2026, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/forum?id=7TmImaRwLQ&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3D3DV%2F2026%2FConference%2FAuthors%23your-submissions)" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://www.springer.com/journal/11263" rel="external nofollow noopener" target="_blank">IJCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022pfdn" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Multimodal Alignment and Fusion: A Survey</div>
          <!-- Author -->
          <div class="author">
          
          Â Songtao Li, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            <em>Springer International Journal of Computer Vision (IJCV)</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2411.17040" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS <br> Spotlight</a></abbr></div>


        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">RobustMerge: Parameter-Efficient Model Merging for MLLMs with Direction Robustness</div>
          <!-- Author -->
          <div class="author">

          Â Fanhu Zeng, Â Haiyang Guo, Â Fei Zhu*, Â Li Shen, Â <em>Hao Tang*</em></div>
          
          <div class="periodical">
          
            In <em>NeurIPS</em> 2025, San Diego, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2502.17159" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis</div>
          <!-- Author -->
          <div class="author">

          Â Jing Hao, Â Yuxuan Fan, Â Yanpeng Sun, Â Kaixin Guo, Â Lizhuo Lin, Â Jinrong Yang, Â Qi Yong H. Ai, Â Lun M. Wong, Â <em>Hao Tang*</em>, Â Kuo Feng Hung*</div>

          <div class="periodical">
          
            In <em>NeurIPS</em> 2025, San Diego, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2509.09254" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/isbrycee/OralGPT/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Enhancing Diffusion-based Unrestricted Adversarial Attacks via Adversary Preferences Alignment</div>
          <!-- Author -->
          <div class="author">

          Â Kaixun Jiang, Â Zhaoyu Chen, Â HaiJing Guo, Â Jinglun Li, Â Jiyuan Fu, Â Pinxue Guo, Â <em>Hao Tang</em>, Â Bo Li, Â Wenqiang Zhang</div>

          <div class="periodical">
          
            In <em>NeurIPS</em> 2025, San Diego, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2506.01511" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/deep-kaixun/APA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Boosting Adversarial Transferability with Spatial Adversarial Alignment</div>
          <!-- Author -->
          <div class="author">

          Â Zhaoyu Chen, Â Haijing Guo, Â Kaixun Jiang, Â Jiyuan Fu, Â Xinyu Zhou, Â Dingkang Yang, Â <em>Hao Tang</em>, Â Bo Li, Â Wenqiang Zhang</div>

          <div class="periodical">
          
            In <em>NeurIPS</em> 2025, San Diego, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2501.01015" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/deep-kaixun/APA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Robot_Learning" rel="external nofollow noopener" target="_blank">CoRL</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">3DS-VLA: A 3D Spatial-Aware Vision Language Action Model for Robust Multi-Task Manipulation</div>
          <!-- Author -->
          <div class="author">

          Â Xiaoqi Li, Â Liang Heng, Â Jiaming Liu, Â Yan Shen, Â Chenyang Gu, Â Zhuoyang Liu, Â Hao Chen, Â Nuowei Han, Â Renrui Zhang, Â <em>Hao Tang</em>, Â Shanghang Zhang, Â Hao Dong</div>
          
          <div class="periodical">
          
            In <em>CoRL</em> 2025, Seoul, Korea
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf/2db27aa32a273716029a3a1f929f4ac3ab7d34c5.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Robot_Learning" rel="external nofollow noopener" target="_blank">CoRL <br> Workshop</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">GRADRobot: Geometry-Aware Rendering with Articulation and Diffusion for Robot Modeling</div>
          <!-- Author -->
          <div class="author">
          
          Â Yunlong Li, Â Boyuan Chen, Â Chongjie Ye, Â Bohan Li, Â Zhaoxi Chen, Â Shaocong Xu, Â <em>Hao Tang</em>, Â Hao Zhao</div>

          <div class="periodical">
          
            In <em>CoRL</em> 2025, Seoul, Korea
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/forum?id=zTqeqm1hkZ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/ACM_Multimedia" rel="external nofollow noopener" target="_blank">ACM MM</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">EventVAD: Training-free Event-aware Video Anomaly Detection</div>
          <!-- Author -->
          <div class="author">

          Â Yihua Shao, Â Haojin He, Â Sijie Li, Â Siyu Chen, Â Xinwei Long, Â Fanhu Zeng, Â Yuxuan Fan, Â Muyang Zhang, Â Ziyang Yan, Â Ao Ma, Â Xiaochen Wang, Â <em>Hao Tang</em>, Â Yan Wang, Â Shuyan Li</div>
          
          <div class="periodical">
          
            In <em>ACM MM</em> 2025, Dublin, Ireland
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2504.13092" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ICCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation</div>
          <!-- Author -->
          <div class="author">
          
          Â Bin Xie, Â <em>Hao Tang</em>, Â Bin Duan, Â Dawen Cai, Â Yan Yan, Â Gady Agam</div>

          <div class="periodical">
          
            In <em>ICCV</em> 2025, Honolulu, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.14103" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Intelligent_Robots_and_Systems" rel="external nofollow noopener" target="_blank">IROS <br> Oral</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">TTTFusion: A Test-Time Training-Based Strategy for Multimodal Medical Image Fusion in Surgical Robots</div>
          <!-- Author -->
          <div class="author">

          Â Qinhua Xie, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>IROS</em> 2025, Hangzhou, China
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2504.20362" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Intelligent_Robots_and_Systems" rel="external nofollow noopener" target="_blank">IROS <br>Oral</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using Gaussian Splatting</div>
          <!-- Author -->
          <div class="author">

          Â Haoran Xu, Â Saining Zhang, Â Peishuo Li, Â Baijun Ye, Â Xiaoxue Chen, Â Huan-ang Gao, Â Jv Zheng, Â Xiaowei Song, Â Ziqiao Peng, Â Run Miao, Â Jinrang Jia, Â Yifeng Shi, Â Guangqi Yi, Â Hang Zhao, Â <em>Hao Tang</em>, Â Hongyang Li, Â Kaicheng Yu, Â Hao Zhao</div>

          <div class="periodical">
          
            In <em>IROS</em> 2025, Hangzhou, China
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2507.18473" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/SainingZhang/CRUISE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://www.springer.com/journal/11263" rel="external nofollow noopener" target="_blank">IJCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022pfdn" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">AutoViT: Achieving Real-Time Vision Transformers on Mobile via Latency-aware Coarse-to-Fine Search</div>
          <!-- Author -->
          <div class="author">
          
           Â Zhenglun Kong, Â Dongkuan Xu, Â Zhengang Li, Â Peiyan Dong, Â <em>Hao Tang</em>, Â Yanzhi Wang, Subhabrata Mukherjee</div>

          <div class="periodical">
          
            <em>Springer  International Journal of Computer Vision (IJCV)</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://link.springer.com/article/10.1007/s11263-025-02480-w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Joint_Conference_on_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">IJCAI</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Semantic-Guided Diffusion Model for Single-Step Image Super-Resolution</div>
          <!-- Author -->
          <div class="author">

          Â Zihang Liu, Â Zhenyu Zhang, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>IJCAI</em> 2025, Montreal, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2505.07071" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Liu-Zihang/SAMSR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Joint_Conference_on_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">IJCAI</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">In-Context Meta LoRA Generation</div>
          <!-- Author -->
          <div class="author">

          Â Yihua Shao, Â Minxi Yan, Â Yang Liu, Â Siyu Chen, Â Wenjie Chen, Â Xinwei Long, Â Ziyang Yan, Â Lei Li, Â Chenyu Zhang, Â Nicu Sebe, Â <em>Hao Tang*</em>, Â Yan Wang, Â Hao Zhao, Â Mengzhu Wang, Â Jingcai Guo*</div>
          <div class="periodical">
          
            In <em>IJCAI</em> 2025, Montreal, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2501.17635" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Joint_Conference_on_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">IJCAI</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">FairSMOE: Mitigating Multi-Attribute Fairness Problem with Sparse Mixture-of-Experts</div>
          <!-- Author -->
          <div class="author">

          Â Changdi Yang, Â Zheng Zhan, Â Ci Zhang, Â Yifan Gong, Â Yize Li, Â Zichong Meng, Â Jun Liu, Â Xuan Shen, Â <em>Hao Tang</em>, Â Geng Yuan, Â Pu Zhao, Â Xue Lin, Â Yanzhi Wang</div>
          <div class="periodical">
          
            In <em>IJCAI</em> 2025, Montreal, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://ijcai-preprints.s3.us-west-1.amazonaws.com/2025/7307.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://advanced.onlinelibrary.wiley.com/journal/21983844" rel="external nofollow noopener" target="_blank">Advanced <br>Science</a></abbr></div>

        <!-- Entry bib key -->
        <div id="xia2022gan" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Smart Organicâ€“Inorganic Copolymer Nanoparticles Distinguish Between Microglia and Cancer Cells for Synergistic Immunotherapy in Glioma</div>
          <!-- Author -->
          <div class="author">
          

          Â Shiming Zhang, Â Kun Shang, Â Lidong Gong, Â Qian Xie, Â Jianfei Sun, Â Meng Xu, Â Xunbin Wei, Â Zhaoheng Xie, Â Xinyu Liu, Â <em>Hao Tang</em>, Â Zhengren Xu, Â Wei Wang, Â Haihua Xiao, Â Zhiqiang Lin, Â Hongbin Han</div>

          <div class="periodical">
          
            <em>Advanced Science</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://advanced.onlinelibrary.wiley.com/doi/10.1002/advs.202500882?af=R" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR <br> Oral</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">DiffFNO: Diffusion Fourier Neural Operator</div>
          <!-- Author -->
          <div class="author">
          
          Â Xiaoyi Liu, Â <em>Hao Tang*</em></div>
          
          <div class="periodical">
          
            In <em>CVPR</em> 2025, Nashville, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2411.09911" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">MambaIC: State Space Models for High-Performance Learned Image Compression</div>
          <!-- Author -->
          <div class="author">
          
          Â Fanhu Zeng, Â <em>Hao Tang</em>, Â Yihua Shao, Â Siyu Chen, Â Ling Shao, Â Yan Wang</div>

          <div class="periodical">
          
            In <em>CVPR</em> 2025, Nashville, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2503.12461" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/AuroraZengfh/MambaIC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model</div>
          <!-- Author -->
          <div class="author">
          
          Â Mingju Gao, Â Yike Pan, Â Huan-ang Gao, Â Zongzheng Zhang, Â Wenyi Li, Â Hao Dong, Â <em>Hao Tang</em>, Â Li Yi, Â Hao Zhao</div>
          
          <div class="periodical">
          
            In <em>CVPR</em> 2025, Nashville, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2503.19913" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/GasaiYU/PartRM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Robotics_and_Automation" rel="external nofollow noopener" target="_blank">ICRA</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Toward Zero-Shot Learning for Visual Dehazing of Urological Surgical Robots</div>
          <!-- Author -->
          <div class="author">

          Â Renkai Wu, Â Xianjin Wang, Â Pengchen Liang, Â Zhenyu Zhang, Â Qing Chang*, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>ICRA</em> 2025, Atlanta, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2410.01395" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/wurenkai/RSF-Dehaze" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/North_American_Chapter_of_the_Association_for_Computational_Linguistics" target="_blank">NAACL</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks</div>
          <!-- Author -->
          <div class="author">

          Â Xiaofeng Zhang, Â Yihao Quan, Â Chen Shen, Â Xiaosong Yuan, Â Shaotian Yan, Â Liang Xie, Â Wenxiao Wang, Â Chaochen Gu, Â <em>Hao Tang</em>, Jieping Ye</div>

          <div class="periodical">
          
            In <em>NAACL</em> 2025, Albuquerque, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2406.06579" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/zhangbaijin/From-Redundancy-to-Relevance" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Toward Adaptive Large Language Models Structured Pruning via Hybrid-grained Weight Importance Assessment</div>
          <!-- Author -->
          <div class="author">

          Â Jun Liu, Â Zhenglun Kong, Â Pu Zhao, Â Changdi Yang, Â <em>Hao Tang*</em>, Â Xuan Shen, Â Geng Yuan, Â Wei Niu, Â Wenbin Zhang, Â Xue Lin, Â Dong Huang*, Â Yanzhi Wang*</div>

          <div class="periodical">
          
            In <em>AAAI</em> 2025, Philadelphia, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.10799" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div>

        <!-- Entry bib key -->
        <div id="chen2022cross" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Stable-Hair: Real-World Hair Transfer via Diffusion Model</div>
          <!-- Author -->
          <div class="author">
          
          Â Yuxuan Zhang, Â Qing Zhang, Â Yiren Song, Â Jichao Zhang, Â <em>Hao Tang</em>, Â Jiaming Liu</div>
          
          <!-- <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>AAAI</em> 2025, Philadelphia, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2407.14078" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://xiaojiu-z.github.io/Stable-Hair.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
     
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Towards Self-Supervised FG-SBIR with Unified Sample Feature Alignment and Multi-Scale Token Recycling</div>
          <!-- Author -->
          <div class="author">
          
          Â Jianan Jiang, Â <em>Hao Tang*</em>, Â Zhilin Jiang, Â Weiren Yu, Â Di Wu*</div>

          <div class="periodical">
          
            In <em>AAAI</em> 2025, Philadelphia, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2406.11551" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/ExponentiAI/ARNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Revisiting Adversarial Patches for Designing Camera-Agnostic Attacks against Person Detection</div>
          <!-- Author -->
          <div class="author">

          Â Hui Wei, Â Zhixiang Wang, Â Kewei Zhang, Â Jiaqi Hou, Â Yuanwei Liu, Â <em>Hao Tang</em>, Â Zheng Wang</div>

          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2024, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/forum?id=2Inwtjvyx8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/weihui1308/CAP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/ACM_Multimedia" rel="external nofollow noopener" target="_blank">ACM MM <br> Oral</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">ConsistentAvatar: Learning to Diffuse Fully Consistent Talking Head Avatar with Temporal Guidance</div>
          <!-- Author -->
          <div class="author">
          
          Â Haijie Yang, Â Zhenyu Zhang, Â <em>Hao Tang</em>,  Â Jianjun Qian, Â Jian Yang</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
          
            In <em>ACM MM</em> 2024, Melbourne, Australia
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://dl.acm.org/doi/10.1145/3664647.3680619" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/ACM_Multimedia" rel="external nofollow noopener" target="_blank">ACM MM</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">CoIn: A Lightweight and Effective Framework for Story Visualization and Continuation</div>
          <!-- Author -->
          <div class="author">
          
          Â Ming Tao, Â Bao Bingkun, Â <em>Hao Tang</em>,  Â Yaowei Wang, Â Changsheng Xu</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
          
            In <em>ACM MM</em> 2024, Melbourne, Australia
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://dl.acm.org/doi/10.1145/3664647.3680873" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/NJUPT-MCC/CoIn" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" rel="external nofollow noopener" target="_blank">TPAMI</a></abbr></div>

        <!-- Entry bib key -->
        <div id="xia2022gan" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Physical Adversarial Attack Meets Computer Vision: A Decade Survey</div>
          <!-- Author -->
          <div class="author">
          

          Â Hui Wei, Â <em>Hao Tang</em>, Â Xuemei Jia, Â Zhixiang Wang, Â Hanxun Yu, Â Zhubo Li, Â Shin'ichi Satoh, Â Luc Van Gool, Â Zheng Wang</div>

          <div class="periodical">
          
            <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2209.15179" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/weihui1308/PAA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Motion Mamba: Efficient and Long Sequence Motion Generation with Hierarchical and Bidirectional Selective SSM</div>
          <!-- Author -->
          <div class="author">
          
          Â Zeyu Zhang, Â Akide Liu, Â Ian Reid, Â Richard Hartley, Â Bohan Zhuang, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.07487" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/steve-zeyu-zhang/MotionMamba/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">3D Weakly Supervised Semantic Segmentation with 2D Vision-Language Guidance</div>
          <!-- Author -->
          <div class="author">
          
          Â Xiaoxu Xu, Â Yitian Yuan, Â Jinlong Li, Â Qiudan Zhang, Â Zequn Jie, Â Lin Ma, Â <em>Hao Tang</em>, Â Nicu Sebe, Â Xu Wang</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2407.09826v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/xuxiaoxxxx/3DSS-VLG/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">StoryImager: A Unified and Efficient Framework for Coherent Story Visualization and Completion</div>
          <!-- Author -->
          <div class="author">
          
          Â Ming Tao, Â Bingkun Bao, Â <em>Hao Tang</em>, Â Yaowei Wang, Â Changsheng Xu</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.05979" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/tobran/StoryImager" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">InstructGIE: Towards Generalizable Image Editing</div>
          <!-- Author -->
          <div class="author">
        
          Â Zichong Meng, Â Changdi Yang, Â Jun Liu, Â <em>Hao Tang*</em>, Â Pu Zhao*, Â Yanzhi Wang*</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.05018" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://cr8br0ze.github.io/InstructGIE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior</div>
          <!-- Author -->
          <div class="author">
          
          Â Huan-ang Gao, Â Mingju Gao, Â Jiaju Li, Â Wenyi Li, Â Rong Zhi, Â <em>Hao Tang</em>, Â Hao Zhao</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.09638" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://air-discover.github.io/SCP-Diff/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Dataset Growth</div>
          <!-- Author -->
          <div class="author">
          
          Â Ziheng Qin, Â Zhaopan Xu, Â Yukun Zhou, Â Zangwei Zheng, Â Zebang Cheng, Â <em>Hao Tang</em>, Â Lei Shang, Â Baigui Sun, Â Xiaojiang Peng, Â Radu Timofte, Â Hongxun Yao, Â Kai Wang, Â Yang You</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2405.18347" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/NUS-HPC-AI-Lab/InfoGrowth" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR <br> Highlight</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud</div>
          <!-- Author -->
          <div class="author">
          
          Â Wencan Cheng, Â <em>Hao Tang</em>, Â Luc Van Gool, Â Jong Hwan Ko </div>
          
          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.03159" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/cwc1260/HandDiff" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Versatile Navigation under Partial Observability via Value-guided Diffusion Policy</div>
          <!-- Author -->
          <div class="author">
          
          Â Gengyu Zhang, Â <em>Hao Tang</em>, Â Yan Yan </div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.02176" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Towards Robust 3D Pose Transfer with Adversarial Learning</div>
          <!-- Author -->
          <div class="author">
          
          Â Haoyu Chen, Â <em>Hao Tang</em>, Â Ehsan Adeli, Â Guoying Zhao </div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.02242" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">On the Faithfulness of Vision Transformer Explanations</div>
          <!-- Author -->
          <div class="author">
          
          Â Junyi Wu, Â Weitai Kang, Â <em>Hao Tang</em>, Â Yuan Hong, Â Yan Yan </div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.01415" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer</div>
          <!-- Author -->
          <div class="author">
          
          Â Junyi Wu, Â Bin Duan, Â Weitai Kang, Â <em>Hao Tang</em>, Â Yan Yan </div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.14552" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation</div>
          <!-- Author -->
          <div class="author">
          
          Â Yuxuan Zhang, Â Jiaming Liu, Â Yiren Song, Â Rui Wang, Â <em>Hao Tang</em>, Â Jinpeng Yu, Â Huaxia Li, Â Xu Tang, Â Yao Hu, Â Han Pan, Â Zhongliang Jing</div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2312.16272" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Xiaojiu-z/SSR_Encoder" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR <br> Workshop</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Towards Online Real-Time Memory-based Video Inpainting Transformers</div>
          <!-- Author -->
          <div class="author">
          
          Â Guillaume Thiry, Â <em>Hao Tang*</em>, Â Radu Timofte, Â Luc Van Gool</div>
          
          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.16161" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div>

        <!-- Entry bib key -->
        <div id="chen2022cross" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">G2P-DDM: Generating Sign Pose Sequence from Gloss Sequence with Discrete Diffusion Model</div>
          <!-- Author -->
          <div class="author">
          
          Â Pan Xie, Â Qipeng Zhang, Â Peng Taiying, Â <em>Hao Tang*</em>, Â Yao Du, Â Zexian Li</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>AAAI</em> 2024, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2208.09141.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://slpdiffusier.github.io/g2p-ddm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">HotBEV: Hardware-oriented Transformer-based Multi-View 3D Detector for BEV Perception</div>
          <!-- Author -->
          <div class="author">

          Â Peiyan Dong, Â Zhenglun Kong, Â Xin Meng, Â Pinrui Yu, Â Yifan Gong, Â Geng Yuan, Â <em>Hao Tang*</em>, Yanzhi Wang</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=3Cj67k38st" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/PeiyanFlying/HotBEV" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">PackQViT: Faster Sub-8-bit Vision Transformers via Full and Packed Quantization on the Mobile</div>
          <!-- Author -->
          <div class="author">

        Â Peiyan Dong, Â Lei Lu, Â Chao Wu, Â Cheng Lyu, Â Geng Yuan, Â <em>Hao Tang*</em>, Yanzhi Wang</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=N56hAiQvot" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/PeiyanFlying/PackQViT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">LART: Neural Correspondence Learning with Latent Regularization Transformer for 3D Motion Transfer</div>
          <!-- Author -->
          <div class="author">

        Â Haoyu Chen, Â <em>Hao Tang</em>, Â Radu Timofte, Â Luc Van Gool, Â Guoying Zhao</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=g27BggUT3L" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/mikecheninoulu/LART" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Does Graph Distillation See Like Vision Dataset Counterpart?</div>
          <!-- Author -->
          <div class="author">

        Â Beining Yang, Â Kai Wang, Â Qingyun Sun, Â Cheng Ji, Â Xingcheng Fu, Â <em>Hao Tang</em>, Â Yang You, Â Jianxin Li</div>

          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2310.09192.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/RingBDStack/SGDD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://link.springer.com/journal/11633" rel="external nofollow noopener" target="_blank">MIR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Practical Blind Image Denoising via Swin-Conv-UNet and Data Synthesis</div>
          <!-- Author -->
          <div class="author">

        Â Kai Zhang, Â Yawei Li, Â Jingyun Liang, Â Jiezhang Cao, Â Yulun Zhang, Â <em>Hao Tang</em>, Â Dengping Fan, Â Radu Timofte, Â Luc Van Gool</div>

          <div class="periodical">
          
          <em>Springer Machine Intelligence Research (MIR)</em>, 2023
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://link.springer.com/article/10.1007/s11633-023-1466-0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/cszn/SCUNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ICCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Learning Concordant Attention via Target-aware Alignment for Visible-Infrared Person Re-identification</div>
          <!-- Author -->
          <div class="author">
          
          Â Jianbing Wu, Â Hong Liu, Â Yuxin Su, Â Wei Shi, Â <em>Hao Tang</em></div>

          <div class="periodical">
          
            In <em>ICCV</em> 2023, Paris, France
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Learning_Concordant_Attention_via_Target-aware_Alignment_for_Visible-Infrared_Person_Re-identification_ICCV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning" rel="external nofollow noopener" target="_blank">ICML</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">SpeedDETR: Speed-aware Transformers for End-to-end Object Detection</div>
          <!-- Author -->
          <div class="author">
          
          Â Peiyan Dong, Â Zhenglun Kong, Â Xin Meng, Â Peng Zhang, Â <em>Hao Tang*</em>, Â Yanzhi Wang, Â Chih-Hsien Chou</div>

          <div class="periodical">
          
            In <em>ICML</em> 2023, Hawaii, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=5VdcSxrlTK" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/PeiyanFlying/SpeedDETR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Joint_Conference_on_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">IJCAI</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <div class="title">Data Level Lottery Ticket Hypothesis for Vision Transformers</div>
          <!-- Author -->
          <div class="author">
          
          Â Xuan Shen, Â Zhenglun Kong, Â Minghai Qin, Â Peiyan Dong, Â Geng Yuan, Â Xin Meng, Â <em>Hao Tang</em>, Â Xiaolong Ma, Â Yanzhi Wang</div>

          <div class="periodical">
          
            In <em>IJCAI</em> 2023, Macao, China
          
          </div>
        
          <div class="links">
            <a href="https://arxiv.org/pdf/2211.01484.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/shawnricecake/vit-lottery-ticket-input" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Graph Transformer GANs for Graph-Constrained House Generation</div>
          <!-- Author -->
          <div class="author">
          
          Â <em>Hao Tang</em>, Â Zhenyu Zhang, Â Humphrey Shi, Â Bo Li, Â Ling Shao, Â Nicu Sebe, Â Radu Timofte, Â Luc Van Gool</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2303.08225.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Unsupervised Deep Probabilistic Approach for Partial Point Cloud Registration</div>
          <!-- Author -->
          <div class="author">
          
          Â Guofeng Mei, Â <em>Hao Tang</em>, Â Xiaoshui Huang, Â Weijie Wang, Â Juan Liu, Â Jian Zhang, Â Luc Van Gool, Â Qiang Wu  </div>
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2303.13290" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/gfmei/UDPReg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network</div>
          <!-- Author -->
          <div class="author">
          
          Â Xuan Shen, Â Yaohua Wang, Â Ming Lin, Â Yilun Huang, Â <em>Hao Tang</em>, Â Xiuyu Sun, Â Yanzhi Wang </div>
          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2303.02165.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/alibaba/lightweight-neural-architecture-search" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â Ming Tao, Â Bingkun Bao, Â <em>Hao Tang</em>, Â Changsheng Xu</div>
          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2301.12959.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/tobran/GALIP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations" rel="external nofollow noopener" target="_blank">ICLR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Edge Guided GANs with Contrastive Learning for Semantic Image Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â <em>Hao Tang</em>, Â Xiaojuan Qi, Â Guolei Sun, Â Dan Xu, Â Nicu Sebe, Â Radu Timofte, Â Luc Van Gool</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>ICLR</em> 2023, Kigali, Rwanda
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2003.13898" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/ECGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">3D-Aware Semantic-Guided Generative Model for Human Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â Jichao Zhang, Â Enver Sangineto, Â <em>Hao Tang</em>, Â Aliaksandr Siarohin, Â Zhun Zhong, Â Nicu Sebe, Â Wei Wang</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>ECCV</em> 2022, Tel Aviv, Israel
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2112.01422.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/zhangqianhui/3DSGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Towards Interpretable Video Super-Resolution via Alternative Optimization</div>
          <!-- Author -->
          <div class="author">
          
          Â Jiezhang Cao, Â Jingyun Liang, Â Kai Zhang, Â Wenguan Wang, Â Qin Wang, Â Yulun Zhang, Â <em>Hao Tang</em>, Â Luc Van Gool</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>ECCV</em> 2022, Tel Aviv, Israel
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2207.10765.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/caojiezhang/DAVSR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="cai2022mask" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation</div>
          <!-- Author -->
          <div class="author">
          
          Â Wenhao Li, Â Hong Liu, Â <em>Hao Tang</em>, Â Pichao Wang, Â Luc Van Gool</div>

          <div class="periodical">
          
            In <em>CVPR</em> 2022, New Orleans, USA
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_MHFormer_Multi-Hypothesis_Transformer_for_3D_Human_Pose_Estimation_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Vegetebird/MHFormer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR <br> Oral</a></abbr></div>

        <!-- Entry bib key -->
        <div id="cai2022mask" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â Ming Tao, Â <em>Hao Tang</em>, Â Fei Wu, Â Xiaoyuan Jing, Â Bingkun Bao, Â Changsheng Xu</div>

          <div class="periodical">
          
              In <em>CVPR</em> 2022, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_DF-GAN_A_Simple_and_Effective_Baseline_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/tobran/DF-GAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


</ol>
          </div>    
    <!-- <a href="https://clustrmaps.com/site/19ncr" title="Visit tracker" rel="external nofollow noopener" target="_blank"><img src="//www.clustrmaps.com/map_v2.png?d=jIdUd0dDYkE8CiqptfhnfiWcZHCc5p62dIsontyW-FQ&amp;cl=ffffff" style="width: 0px;"></a> -->
  </article>

</div>

    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        Â©Copyright 2012-2026 Hao Tang. Last updated: Feb 5, 2026.

        <!-- Default Statcounter code for Hao Tang' homepage
        https://ha0tang.github.io/ -->
        <script type="text/javascript">
        var sc_project=12830645; 
        var sc_invisible=1; 
        var sc_security="11b4016e"; 
        </script>
        <script type="text/javascript"
        src="https://www.statcounter.com/counter/counter.js"
        async></script>
        <noscript><div class="statcounter"><a title="Web Analytics
        Made Easy - Statcounter" href="https://statcounter.com/"
        target="_blank"><img class="statcounter"
        src="https://c.statcounter.com/12830645/0/11b4016e/1/"
        alt="Web Analytics Made Easy - Statcounter"
        referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
        <!-- End of Statcounter Code -->

      </div>
    </footer>


    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
