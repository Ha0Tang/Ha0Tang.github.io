<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Hao Tang</title>
    <meta name="author" content="Hao Tang">
    <meta name="description" content="&lt;p&gt; Computer Vision Lab, ETH Zürich, Switzerland&lt;br&gt; Office: ETF C 108, Sternwartstrasse 7, 8092 Zürich, Switzerland&lt;br&gt; Email: bjdxtanghao@gmail.com&lt;/p&gt;
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9C%8C%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              
              <!--  -->
              <!-- Blog -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li> -->
              
              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/services/">Services</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/others/">Others</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <div class="post">


  <article>
    <div class="row">
        <div class="col-sm-3">
            
            <img style="width: 170px;border-radius: 10px" src="/assets/img/th.png">
            
        </div>
    
        <div class="col-sm-9">
            <h1 class="post-title">
            Hao Tang
            </h1>
            <p class="desc"></p>
<p> Computer Vision Lab, ETH Zürich, Switzerland<br> Office: ETF C 108, Sternwartstrasse 7, 8092 Zürich, Switzerland<br> Email: hao.tang@vision.ee.ethz.ch</p>

            <div class="social">
                <div class="contact-icons">
                <a href="https://scholar.google.com/citations?user=9zJkeEMAAAAJ&hl=en" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/Ha0Tang" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/hao-tang-887475138/" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://dblp.org/pid/07/5751-5.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a>
            <a href="https://twitter.com/HaoTang_ai" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            

                </div>
            </div>
   
        </div>
    </div>

    <div class="clearfix">
      <p>I am currently a postdoctoral researcher with Computer Vision Lab, ETH Zurich, Switzerland. I received my master’s degree from the School of Electronics and Computer Engineering, Peking University, China and the Ph.D. degree from the Multimedia and Human Understanding Group, University of Trento, Italy. I was a visiting scholar in the Department of Engineering Science at the University of Oxford. 
      </p>

      <p>My research interests are deep learning, machine learning, and their applications to computer vision. Specifically, I focus on GAN (e.g., image generation, image translation, text-to-image synthesis/editing, person image synthesis, semantic image synthesis, style transfer)、multi-modalities (e.g., audio-to-video synthesis, language-vision models)、edical image enhancement and analysis、diffusion models、3D (e.g., nerf, 3D-aware image/video generation)、low-level vision (image/video restoration, super-resolution, denoising, deblurring).

<p><strong>For prospective collaborators</strong>: We have multiple positions for Postdoc/Ph.D./Master/Intern researchers. If you are interested in joining/visitng Computer Vision Lab or remotely working with us, please email me with your self-introduction, the project of interest, and CV to hao.tang@vision.ee.ethz.ch.</p>

    </div>


                
          <div class="news">
            <h2>News</h2>
            <div class="table-responsive" style="max-height: 20vw">
              <table class="table table-sm table-borderless">
               
                <tr>
                  <!-- <th scope="row">Dec 9, 2022</th> -->
                  <th scope="row">Dec 2022</th>
                  <td>
                    I am serving as a Senior Program Committee (SPC) member for <strong>IJCAI 2023</strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Nov 21, 2022</th> -->
                  <th scope="row">Nov 2022</th>
                  <td>
                    I am serving as an Area Chair for <strong>ICCV 2023</strong>. We have 2 papers accepted to <strong>AAAI 2023</strong>. They are about lightweight SR (<a href="https://github.com/Sun1992/HPUN" rel="external nofollow noopener" target="_blank">HPUN</a>) and hyperspectral image denoising (<a href="https://github.com/MyuLi/SST" rel="external nofollow noopener" target="_blank">SST</a>).
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Oct 4, 2022</th> -->
                  <th scope="row">Oct 2022</th>
                  <td>
                    I am serving as an Area Chair for <strong>CVPR 2023</strong>. We have 1 paper accepted to <strong>TIP</strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Sep 15, 2022</th> -->
                  <th scope="row">Sep 2022</th>
                  <td>
                    We have 2 papers accepted to <strong>NeurIPS 2022</strong>. They are about image restoration (<a href="https://github.com/zhengchen1999/CAT" rel="external nofollow noopener" target="_blank">CAT</a>) and  spectral compressive imaging (<a href="https://github.com/caiyuanhao1998/MST" rel="external nofollow noopener" target="_blank">DAUHST</a>).
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Jul 15, 2022</th> -->
                  <th scope="row">Jul 2022</th>
                  <td>
                    I am serving as a Senior Program Committee (SPC) member for <strong>AAAI 2023</strong> and reviewer for <strong>ICLR 2023</strong>. We have 5 papers accepted to <strong>ECCV 2022</strong> and 1 paper accepted to <strong>TPAMI</strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Jun 15, 2022</th> -->
                  <th scope="row">Jun 2022</th>
                  <td>
                    We have released the codebase for <a href="https://github.com/caiyuanhao1998/MST" rel="external nofollow noopener" target="_blank">Spectral Compressive Imaging</a>. We have 1 paper accepted to <strong>TPAMI</strong> and 2 papers accepted to <strong>ACM MM 2022</strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">May 15, 2022</th> -->
                  <th scope="row">May 2022</th>
                  <td>
                    We have 2 papers accepted to <strong>ICML 2022</strong>. Our paper <a href="https://github.com/yulunzhang/RCAN" rel="external nofollow noopener" target="_blank">RCAN</a> ranks top 10 most influential papers based on citations in ECCV 2018. See <a href="https://www.paperdigest.org/2022/05/most-influential-eccv-papers-2022-05/" rel="external nofollow noopener" target="_blank">Most Influential ECCV Papers</a>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Apr 21, 2022</th> -->
                  <th scope="row">Apr 2022</th>
                  <td>
                    We have 2 papers accepted to <strong>IJCAI 2022</strong>. We won the first place in <a href="https://codalab.lisn.upsaclay.fr/competitions/721" rel="external nofollow noopener" target="_blank">NTIRE Spectral Reconstruction Challenge</a> at CVPR, 2022. The <a href="https://arxiv.org/abs/2204.07908" rel="external nofollow noopener" target="_blank">paper</a> and <a href="https://github.com/caiyuanhao1998/MST-plus-plus" rel="external nofollow noopener" target="_blank">codebase</a> of our solution MST++ have been released.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Mar 30, 2022</th> -->
                  <th scope="row">Mar 2022</th>
                  <td>
                    We have 3 papers (two about spectral compressive imaging, one about interpretable image SR) accepted to <strong>CVPR 2022</strong>. I am serving as reviewer for <strong>ECCV 2022</strong> and <strong>NeurIPS 2022</strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Jan 12, 2022</th> -->
                  <th scope="row">Jan 2022</th>
                  <td>
                    We have 1 paper accepted to <strong>TPAMI</strong> and another one accepted to <strong>ICLR 2022</strong>. I am serving as reviewer for <strong>ACM MM 2022</strong> and <strong>MICCAI 2022</strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Dec 21, 2021</th> -->
                  <th scope="row">Dec 2021</th>
                  <td>
                    I am serving as SPC for <strong>IJCAI 2022</strong> and reviewer for <strong>ICML 2022</strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Sep 21, 2021</th> -->
                  <th scope="row">Sep 2021</th>
                  <td>
                    3 papers are accepted to <strong>NeurIPS 2021</strong>. I am serving as reviewer for <strong><a href="https://cvpr2022.thecvf.com/" rel="external nofollow noopener" target="_blank">CVPR 2022</a></strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Jul 21, 2021</th> -->
                  <th scope="row">Jul 2021</th>
                  <td>
                    3 papers are accepted to <strong>ICCV 2021</strong>. I accept the invitation to the novel Program Committee Board of IJCAI with service time from 2022 to 2024.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Jun 21, 2021</th> -->
                  <th scope="row">Jun 2021</th>
                  <td>
                    1 paper is accepted to <strong>TCYB</strong> and <strong>TIP</strong>. I am serving as reviewer for <a href="https://iclr.cc/" rel="external nofollow noopener" target="_blank"><strong>ICLR 2022</strong></a>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Apr 21, 2021</th> -->
                  <th scope="row">Apr 2021</th>
                  <td>
                    Our real-world image denoising paper is accepted to <strong>IJCAI 2021</strong>. I am serving as reviewer  for <strong><a href="https://nips.cc/Conferences/2021/" rel="external nofollow noopener" target="_blank">NeurIPS 2021</a></strong> and <strong><a href="https://2021.acmmm.org/" rel="external nofollow noopener" target="_blank">ACM MM 2021</a></strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Mar 21, 2021</th> -->
                  <th scope="row">Mar 2021</th>
                  <td>
                    Our papers about MR image enhancement and real-world image denoising are accepted to <strong>CVPR 2021</strong>. One paper about MR image super-resolution is accepted to <strong>TCSVT</strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Feb 21, 2021</th> -->
                  <th scope="row">Feb 2021</th>
                  <td>
                    Our paper <a href="https://github.com/yulunzhang/RDN" rel="external nofollow noopener" target="_blank">RDN</a> ranks top 10 most influential papers based on citations in CVPR 2018. See <a href="https://www.paperdigest.org/2021/02/most-influential-cvpr-papers/" rel="external nofollow noopener" target="_blank">Most Influential CVPR Papers</a>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Jan 21, 2021</th> -->
                  <th scope="row">Jan 2021</th>
                  <td>
                    Our <a href="https://arxiv.org/abs/2012.09243" rel="external nofollow noopener" target="_blank">Neural Pruning</a> paper (<a href="https://github.com/MingSun-Tse/Regularization-Pruning" rel="external nofollow noopener" target="_blank">code</a>) is accepted to <strong>ICLR 2021</strong>. I am serving as reviewer  for <strong><a href="http://iccv2021.thecvf.com/home" rel="external nofollow noopener" target="_blank">ICCV 2021</a></strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Dec 21, 2020</th> -->
                  <th scope="row">Dec 2020</th>
                  <td>
                    I am serving as a Senior Program Committee (SPC) member for <strong><a href="https://ijcai-21.org/" rel="external nofollow noopener" target="_blank">IJCAI 2021</a></strong> and PC member for <strong><a href="https://icml.cc/Conferences/2021" rel="external nofollow noopener" target="_blank">ICML 2021</a></strong>, <strong><a href="https://miccai2021.org/en/" rel="external nofollow noopener" target="_blank">MICCAI 2021</a></strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Sep 21, 2020</th> -->
                  <th scope="row">Sep 2020</th>
                  <td>
                    We have 1 paper accepted to <strong>TNNLS</strong> and another one accepted to <strong>NeurIPS 2020</strong>. Code will be available soon.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Jul 21, 2020</th> -->
                  <th scope="row">Jul 2020</th>
                  <td>
                    We have 2 papers accepted to <strong>ECCV 2020</strong>. Code/data will be available soon.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Jun 21, 2020</th> -->
                  <th scope="row">Jun 2020</th>
                  <td>
                    We release the <a href="https://github.com/SHI-Labs/Pyramid-Attention-Networks" rel="external nofollow noopener" target="_blank">code</a> for our paper: <a href="https://arxiv.org/pdf/2004.13824.pdf" rel="external nofollow noopener" target="_blank">Pyramid Attention Networks for Image Restoration</a>
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Feb 21, 2020</th> -->
                  <th scope="row">Feb 2020</th>
                  <td>
                    We have 3 papers accepted to <strong>CVPR 2020</strong>. Congratulations to all authors!
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Jan 21, 2020</th> -->
                  <th scope="row">Jan 2020</th>
                  <td>
                    We have 1 paper accepted to <strong>TPAMI</strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Dec 21, 2019</th> -->
                  <th scope="row">Dec 2019</th>
                  <td>
                    We release the TensforFlow code and pre-trained models for ICCV19MST at <a href="https://github.com/yulunzhang/MST" rel="external nofollow noopener" target="_blank">MST</a>. The PyTorch code for MST is on the way.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Apr 21, 2019</th> -->
                  <th scope="row">Apr 2019</th>
                  <td>
                    We release all the train/test codes and pre-trained models for ICLR19RNAN at <a href="https://github.com/yulunzhang/RNAN" rel="external nofollow noopener" target="_blank">RNAN</a>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Dec 25, 2018</th> -->
                  <th scope="row">Dec 2018</th>
                  <td>
                    We have 1 paper accepted to <strong>ICLR 2019</strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Jul 21, 2018</th> -->
                  <th scope="row">Jul 2018</th>
                  <td>
                    We have 1 paper accepted to <strong>ACM MM 2018</strong> and 1 paper accepted to <strong>ECCV 2018</strong>. PyTorch version for our CVPR18RDN has been implemented by Nguyễn Trần Toàn (trantoan060689@gmail.com) and merged into <a href="https://github.com/thstkdgus35/EDSR-PyTorch" rel="external nofollow noopener" target="_blank">EDSR-PyTorch</a>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Jun 21, 2018</th> -->
                  <th scope="row">Jun 2018</th>
                  <td>
                    I accept the invitation to serve as Program Committee member of <strong>AAAI 2019</strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Feb 21, 2018</th> -->
                  <th scope="row">Feb 2018</th>
                  <td>
                    I will intern to <strong><a href="https://research.adobe.com/" rel="external nofollow noopener" target="_blank">Adobe Research</a></strong> (San Jose, CA) this summer. We have 1 paper accepted to <strong>CVPR 2018</strong> as spotlight. All the codes are available at <a href="https://github.com/yulunzhang/RDN" rel="external nofollow noopener" target="_blank">Github</a>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Jul 21, 2017</th> -->
                  <th scope="row">Jul 2017</th>
                  <td>
                    I recieve ‘<strong>Excellent Graduate of Beijing</strong>’ award, ‘<strong>Excellent Graduate in Department of Automation, Tsinghua University</strong>’ award, <strong>Excellent Master Thesis Award, Tsinghua University</strong>’ award, and <strong><a href="http://suisf.sz.edu.cn/20170713gg.htm" rel="external nofollow noopener" target="_blank">Shenzhen Universiade International Scholarship</a></strong>.
 
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">May 21, 2017</th> -->
                  <th scope="row">May 2017</th>
                  <td>
                    We have 1 IEEE  <strong>TSMC</strong> paper accepted. <a href="http://yulunzhang.com/img/CVPR17NTIRE-SecondAward.jpg" rel="external nofollow noopener" target="_blank">Our team (HelloSR: Xintao Wang, Yapeng Tian, Ke Yu, Yulun Zhang, Shixiang Wu, Chao Dong, Liang Lin, Yu Qiao) ranks <strong>2nd</strong> place</a> in <a href="http://www.vision.ee.ethz.ch/ntire17/" rel="external nofollow noopener" target="_blank">NTIRE Image Super-Resolution Challenge</a>.
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

    

    
      
          <div class="publications">
            <h2>Recent Publications</h2>
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS Spotlight</a></abbr></div>

        <!-- Entry bib key -->
        <div id="chen2022cross" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Cross Aggregation Transformer for Image Restoration</div>
          <!-- Author -->
          <div class="author">
          

          Zheng Chen, <em>Yulun Zhang</em>, Jinjin Gu, Yongbing Zhang, Linghe Kong, and Xin Yuan</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> 2022
          </div> -->
          <div class="periodical">
          
            <em>In Advances in Neural Information Processing Systems</em>
          
          
            (<b>NeurIPS Spotlight</b>),
          
          
          2022
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=wQ2QNNP8GtM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/zhengchen1999/CAT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV Oral</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Modeling Mask Uncertainty in Hyperspectral Image Reconstruction</div>
          <!-- Author -->
          <div class="author">
          

          Jiamian Wang, <em>Yulun Zhang</em>, Xin Yuan, Ziyi Meng, and Zhiqiang Tao</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            <em>In European Conference on Computer Vision</em>
          
          
            (<b>ECCV Oral</b>),
          
          
          2022
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2112.15362" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Jiamian-Wang/mask_uncertainty_spectral_SCI" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning" rel="external nofollow noopener" target="_blank">ICML</a></abbr></div>

        <!-- Entry bib key -->
        <div id="lin2022unsupervised" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Unsupervised Flow-Aligned Sequence-to-Sequence Learning for Video Restoration</div>
          <!-- Author -->
          <div class="author">
          

          Jing Lin<sup>†</sup>, Xiaowan Hu<sup>†</sup>, Yuanhao Cai, Haoqian Wang, Youliang Yan, Xueyi Zou, <em>Yulun Zhang</em>, and Luc Van Gool</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In International Conference on Machine Learning</em> 2022
          </div> -->
          <div class="periodical">
          
            <em>In International Conference on Machine Learning</em>
          
          
            (<b>ICML</b>),
          
          
          2022
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://proceedings.mlr.press/v162/lin22d/lin22d.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/linjing7/VR-Baseline" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="cai2022mask" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction</div>
          <!-- Author -->
          <div class="author">
          

          Yuanhao Cai<sup>†</sup>, Jing Lin<sup>†</sup>, Xiaowan Hu, Haoqian Wang, Xin Yuan, <em>Yulun Zhang</em>, Radu Timofte, and Luc Van Gool</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In Computer Vision and Pattern Recognition</em> 2022
          </div> -->
          <div class="periodical">
          
            <em>In Computer Vision and Pattern Recognition</em>
          
          
            (<b>CVPR</b>),
          
          
          2022
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cai_Mask-Guided_Spectral-Wise_Transformer_for_Efficient_Hyperspectral_Image_Reconstruction_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/caiyuanhao1998/MST" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>
</ol>
          </div>

    

    
    <a href="https://clustrmaps.com/site/19ncr" title="Visit tracker" rel="external nofollow noopener" target="_blank"><img src="//www.clustrmaps.com/map_v2.png?d=jIdUd0dDYkE8CiqptfhnfiWcZHCc5p62dIsontyW-FQ&amp;cl=ffffff" style="width: 0px;"></a>
  </article>

</div>

    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2022 Yulun  Zhang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
