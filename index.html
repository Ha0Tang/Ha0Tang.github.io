<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Hao Tang</title>
    <meta name="author" content="Hao Tang">
    <meta name="description" content="&lt;p&gt; Computer Vision Lab, ETH ZÃ¼rich, Switzerland&lt;br&gt; Office: ETF C 108, Sternwartstrasse 7, 8092 ZÃ¼rich, Switzerland&lt;br&gt; Email: bjdxtanghao@gmail.com&lt;/p&gt;
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;">

    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              
              <!--  -->
              <!-- Blog -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li> -->
              
              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/services/">Services</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/others/">Others</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <div class="post">


  <article>
    <div class="row">
        <div class="col-sm-3">
            
            <img style="width: 170px;border-radius: 10px" src="/assets/img/th.png">
            
        </div>
    
        <div class="col-sm-9">
            <h1 class="post-title">
            Hao Tang
            </h1>
            <p class="desc"></p>
<p>Robotics Institute, Carnegie Mellon University<br> Office: Smith Hall, 5000 Forbes Av,
Pittsburgh, PA 15213, USAðŸ‡ºðŸ‡¸<br> Email: bjdxtanghao@gmail.com</p>

            <div class="social">
                <div class="contact-icons">
                <a href="https://scholar.google.com/citations?user=9zJkeEMAAAAJ&hl=en" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/Ha0Tang" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/hao-tang-887475138/" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://dblp.org/pid/07/5751-5.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a>
            <a href="https://twitter.com/HaoTang_ai" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            <a href="https://orcid.org/0000-0002-2077-1246" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="fab fa-orcid"></i></a>
            <a href="https://arxiv.org/a/tang_h_1.html" title="Arxiv" target="_blank" rel="noopener noreferrer"><i class="ai ai-arxiv"></i></a>
                </div>
            </div>
   
        </div>
    </div>

    <div class="clearfix">
      <p>Hey, thanks for stopping by! ðŸ‘‹</p>

      <p>
        <!-- I am currently a postdoctoral researcher with Robotics Institute, Carnegie Mellon University, USAðŸ‡ºðŸ‡¸. Before this, I was a postdoctoral researcher with Computer Vision Lab, ETH ZÃ¼rich, SwitzerlandðŸ‡¨ðŸ‡­. I received my masterâ€™s degree from the School of Electronics and Computer Engineering, Peking University, ChinaðŸ‡¨ðŸ‡³, and my Ph.D. degree from the Multimedia and Human Understanding Group, University of Trento, ItalyðŸ‡®ðŸ‡¹. I was a visiting scholar in the Department of Engineering Science at the University of Oxford, UKðŸ‡¬ðŸ‡§. I was also an visiting intern in the IIAI, UAEðŸ‡¦ðŸ‡ª. -->
        I am currently a postdoctoral researcher at the Robotics Institute, Carnegie Mellon University, USAðŸ‡ºðŸ‡¸, working under the guidance of Prof. <a href="https://scholar.google.com/citations?hl=en&user=YB8_6gkAAAAJ">Fernando De la Torre</a>. Before this, I held a postdoctoral position at the Computer Vision Lab, ETH ZÃ¼rich, SwitzerlandðŸ‡¨ðŸ‡­, under the supervision of Prof. <a href="https://scholar.google.be/citations?user=u3MwH5kAAAAJ&hl=en">Radu Timofte</a> and Prof. <a href="https://scholar.google.com/citations?user=TwMib_QAAAAJ&hl=en">Luc Van Gool</a>. My academic journey includes earning a masterâ€™s degree from the School of Electronics and Computer Engineering, Peking University, ChinaðŸ‡¨ðŸ‡³, under the mentorship of Prof. <a href="https://scholar.google.com/citations?hl=en&user=4CQKG8oAAAAJ">Hong Liu</a>, and completing my Ph.D. studies with the Multimedia and Human Understanding Group, University of Trento, ItalyðŸ‡®ðŸ‡¹, under the mentorship of Prof. <a href="https://scholar.google.com/citations?hl=en&user=stFCYOAAAAAJ">Nicu Sebe</a>. Additionally, I had the privilege of being a visiting scholar in the Department of Engineering Science at the University of Oxford, UKðŸ‡¬ðŸ‡§, under the guidance of Prof. <a href="https://scholar.google.com/citations?hl=en&user=kPxa2w0AAAAJ">Philip Torr</a>. Furthermore, I undertook a visiting internship at the IIAI, UAEðŸ‡¦ðŸ‡ª, under the supervision of Prof. <a href="https://scholar.google.com/citations?hl=en&user=z84rLjoAAAAJ">Ling Shao</a>.
      </p>

      <p>My research interests are AIGC, AI4Science, machine learning, and computer vision. Specifically, I focus on:

      <div>
        <ul>
            <li>Generative AI (GANs, diffusion models) and its applications (e.g., image generation, image translation, text-to-image synthesis/editing, person image synthesis, semantic image synthesis, style transfer, video generation, house layout generation)</li>
            <li>Low-level vision (image/video restoration, super-resolution, denoising, deblurring, HDR deghosting)</li>
            <li>Multi-modalities (e.g., audio-to-video synthesis, language-vision models)</li>
            <li>Medical image enhancement and analysis</li>
            <li>3D vision (e.g., nerf, 3D-aware image/video generation, object reconstruction/generation, 3D pose transfer)</li>
            <li>Point cloud registration and segmentation</li>
            <li>Human pose estimation and motion prediction</li>
            <li>Depth estimation</li>
            <li>Semantic segmentation</li>
            <li>Object detection</li>
            <li>Efficient network design (e.g., pruning, knowledge distillation, quantization, NAS)</li>
            <li>Network robustness</li>
        </ul>    
    </div>

      <style>
      .box {
        max-width: 100%;
        width: auto;
        height: auto;
        background-color: #404040;
        padding: 20px;
        border-radius: 10px;
        margin: 20px auto;
        color: #333;
      }
      </style>

      <div class="box">
           <!-- <p><strong>If this resonates with you, we are actively hiring.</strong> For prospective collaborators, we have multiple positions for Postdoc/Ph.D./Master/Intern researchers. If you are interested in joining/visitng ETH Computer Vision Lab or remotely working with us, please email me with your self-introduction, the project of interest (what is the problem you are trying to solve? and how are you trying to solve this problem (be as specific as possible)?), and CV to hao.tang@vision.ee.ethz.ch. </p> -->
            <p><strong>If this resonates with you, we are actively hiring.</strong></p>

            <p>For prospective collaborators, we have multiple positions for Postdoc/Ph.D./Master/Intern researchers. If you are interested in joining/visitng our lab or remotely working with us, please email me with your self-introduction, the project of interest (what is the problem you are trying to solve? and how are you trying to solve this problem (be as specific as possible)?), transcript, and CV to bjdxtanghao@gmail.com.</p>

            <p>For CMU undergraduate and master students, please email me with your self-introduction, the project of interest, transcript, and CV to haotang2@cmu.edu. </p>

            <!-- <p>For ETH undergraduate and master students and students in the European area, please refer to <a href="https://vision.ee.ethz.ch/education/student-projects/icu.html" rel="external nofollow noopener" target="_blank">this page</a> (Connecting 3D and 2D, Deep Learning for Motion Prediction and Generation, Deep Learning for Space Design Generation, GANs Meet Nerfs, Deep Learning for Virtual Try-On, Deep Learning for Video Generation).</p>   -->
      </div>

    </div>


          <div class="news">
            <h2>News</h2>
            <div class="table-responsive" style="max-height: 20vw">
              <table class="table table-sm table-borderless">
                <th scope="row">2024-01</th>
                  <td>
                    We have 1 paper (Architectural Layout Generation) accepted to <strong>TPAMI 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-12</th>
                  <td>
                    We have 1 paper (Sign Pose Sequence Generation) accepted to <strong>AAAI 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-10</th>
                  <td>
                    We have 4 papers (BEV Perception + Efficient ViT + 3D Motion Transfer + Graph Distillation) accepted to <strong>NeurIPS 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-07</th>
                  <td>
                    We have 1 paper (Semantic Image Synthesis) accepted to <strong>TPAMI 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-06</th>
                  <td>
                    We have 1 paper (Visible-Infrared Person Re-ID) accepted to <strong>ICCV 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-05</th>
                  <td>
                    We have 2 papers accepted to <strong>CVPRW 2023</strong> and 1 paper accepted to <strong>J-STSP</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-04</th>
                  <td>
                    We have 1 paper (Object Detection) accepted to <strong>ICML 2023</strong>, 2 papers (Lottery Ticket Hypothesis for ViT + Zero-shot Character Recognition) accepted to <strong>IJCAI 2023</strong>, 1 paper accepted to <strong>PR</strong>, and 1 paper accepted to <strong>TGRS</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-03</th>
                  <td>
                    We have 6 papers accepted to <strong>CVPR 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-02</th>
                  <td>
                    We have 3 papers accepted to <strong>ICASSP 2023</strong> and 1 paper accepted to <strong>TCSVT</strong>.
                  </td>
                </tr> 
               
                <tr>
                  <th scope="row">2023-01</th>
                  <td>
                    We have 1 paper accepted to <strong>ICLR 2023</strong> and 1 paper accepted to <strong>TMM</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-11</th>
                  <td>
                    We have 4 papers accepted to <strong>AAAI 2023</strong>, 1 paper accepted to <strong>IJCV</strong>, 1 paper accepted to <strong>TIP</strong>, and 1 paper accepted to <strong>TCSVT</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2022-10</th>
                  <td>
                    We have 1 paper accepted to <strong>BMVC 2022</strong> and 1 paper accepted to <strong>TPAMI</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-09</th>
                  <td>
                    We have 1 paper accepted to <strong>TAFFC</strong> and 1 paper accepted to <strong>TGRS</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-07</th>
                  <td>
                     We have 5 papers accepted to <strong>ECCV 2022</strong>, 1 paper accepted to <strong>TIP</strong>, and 1 paper accepted to <strong>PR</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-06</th>
                  <td>
                     We have 2 papers accepted to <strong>ACM MM 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-04</th>
                  <td>
                    We have 1 paper accepted to <strong>IJCAI 2022</strong>, 1 paper accepted to <strong>TGRS</strong>, and 1 paper accepted to <strong>TMM</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2022-03</th>
                  <td>
                    We have 5 papers accepted to <strong>CVPR 2022</strong>, 1 paper accepted to <strong>TPAMI</strong>, and 1 paper accepted to <strong>TMM</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-12</th>
                  <td>
                    We have 2 papers accepted to <strong>AAAI 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-11</th>
                  <td>
                    We have 1 paper accepted to <strong>TIP</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-10</th>
                  <td>
                    We have 3 papers accepted to <strong>BMVC 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Oct 4, 2022</th> -->
                  <th scope="row">2021-08</th>
                  <td>
                    We have 1 paper accepted to <strong>TIP</strong> and 1 paper accepted to <strong>TNNLS</strong>.
                  </td>
                </tr>
                <tr>
                  <!-- <th scope="row">Oct 4, 2022</th> -->
                  <th scope="row">2021-07</th>
                  <td>
                    We have 2 papers accepted to <strong>ICCV 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <!-- <th scope="row">Oct 4, 2022</th> -->
                  <th scope="row">2021-06</th>
                  <td>
                    We have 1 paper accepted to <strong>ACM MM 2021</strong> and 1 paper accepted to <strong>TMM</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2020-08</th>
                  <td>
                    We have 1 paper accepted to <strong>BMVC 2020</strong>, 2 papers accepted to <strong>ACM MM 2020</strong>, and 1 paper accepted to <strong>TIP</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2020-07</th>
                  <td>
                    We have 1 paper accepted to <strong>ECCV 2020</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2020-05</th>
                  <td>
                    We have 1 paper accepted to <strong>TNNLS</strong> and 1 paper accepted to <strong>TGRS</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2020-02</th>
                  <td>
                    We have 1 paper accepted to <strong>CVPR 2020</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2019-07</th>
                  <td>
                    We have 1 paper accepted to <strong>ACM MM 2019</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2019-02</th>
                  <td>
                    We have 1 paper accepted to <strong>CVPR 2019</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2018-06</th>
                  <td>
                    We have 1 paper accepted to <strong>ACM MM 2018</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2018-02</th>
                  <td>
                    We have 1 paper accepted to <strong>CVPR 2018</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2016-07</th>
                  <td>
                    We have 1 paper accepted to <strong>IJCAI 2016</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2015-08</th>
                  <td>
                    We have 1 paper accepted to <strong>ACM MM 2015</strong>.
                  </td>
                </tr>
                
              </table>
            </div> 
          </div>
      
          <div class="publications">
            <h2>Featured Publications</h2>
            <h5>(Including NeurIPS, ICML, ICLR, CVPR, ICCV, ECCV, AAAI, IJCAI, ACM MM)</h5>
            <p class="post-description"><sup>â€ </sup>Equal Contribution,  <sup>*</sup>Corresponding Author(s)</p>
            <ol class="bibliography">

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div>

        <!-- Entry bib key -->
        <div id="chen2022cross" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">G2P-DDM: Generating Sign Pose Sequence from Gloss Sequence with Discrete Diffusion Model</div>
          <!-- Author -->
          <div class="author">
          
          Pan Xie, Â Qipeng Zhang, Â Peng Taiying, Â <em>Hao Tang*</em>, Â Yao Du, Â Zexian Li</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>AAAI</em> 2024, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2208.09141.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://slpdiffusier.github.io/g2p-ddm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">HotBEV: Hardware-oriented Transformer-based Multi-View 3D Detector for BEV Perception</div>
          <!-- Author -->
          <div class="author">

          Â Peiyan Dong, Â Zhenglun Kong, Â Xin Meng, Â Pinrui Yu, Â Yifan Gong, Â Geng Yuan, Â <em>Hao Tang*</em>, Yanzhi Wang</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=3Cj67k38st" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/PeiyanFlying/HotBEV" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">PackQViT: Faster Sub-8-bit Vision Transformers via Full and Packed Quantization on the Mobile</div>
          <!-- Author -->
          <div class="author">

        Â Peiyan Dong, Â Lei Lu, Â Chao Wu, Â Cheng Lyu, Â Geng Yuan, Â <em>Hao Tang*</em>, Yanzhi Wang</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=N56hAiQvot" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/PeiyanFlying/PackQViT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">LART: Neural Correspondence Learning with Latent Regularization Transformer for 3D Motion Transfer</div>
          <!-- Author -->
          <div class="author">

        Â Haoyu Chen, Â <em>Hao Tang</em>, Â Radu Timofte, Â Luc Van Gool, Â Guoying Zhao</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=g27BggUT3L" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/mikecheninoulu/LART" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Does Graph Distillation See Like Vision Dataset Counterpart?</div>
          <!-- Author -->
          <div class="author">

        Â Beining Yang, Â Kai Wang, Â Qingyun Sun, Â Cheng Ji, Â Xingcheng Fu, Â <em>Hao Tang</em>, Â Yang You, Â Jianxin Li</div>

          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2310.09192.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/RingBDStack/SGDD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ICCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Learning Concordant Attention via Target-aware Alignment for Visible-Infrared Person Re-identification</div>
          <!-- Author -->
          <div class="author">
          
          Â Jianbing Wu, Â Hong Liu, Â Yuxin Su, Â Wei Shi, Â <em>Hao Tang</em></div>

          <div class="periodical">
          
            In <em>ICCV</em> 2023, Paris, France
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Learning_Concordant_Attention_via_Target-aware_Alignment_for_Visible-Infrared_Person_Re-identification_ICCV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning" rel="external nofollow noopener" target="_blank">ICML</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">SpeedDETR: Speed-aware Transformers for End-to-end Object Detection</div>
          <!-- Author -->
          <div class="author">
          
          Â Peiyan Dong, Â Zhenglun Kong, Â Xin Meng, Â Peng Zhang, Â <em>Hao Tang*</em>, Â Yanzhi Wang, Â Chih-Hsien Chou</div>

          <div class="periodical">
          
            In <em>ICML</em> 2023, Hawaii, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=5VdcSxrlTK" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/PeiyanFlying/SpeedDETR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Joint_Conference_on_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">IJCAI</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <div class="title">Data Level Lottery Ticket Hypothesis for Vision Transformers</div>
          <!-- Author -->
          <div class="author">
          
          Â Xuan Shen, Â Zhenglun Kong, Â Minghai Qin, Â Peiyan Dong, Â Geng Yuan, Â Xin Meng, Â <em>Hao Tang</em>, Â Xiaolong Ma, Â Yanzhi Wang</div>

          <div class="periodical">
          
            In <em>IJCAI</em> 2023, Macao, China
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2211.01484.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/shawnricecake/vit-lottery-ticket-input" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Graph Transformer GANs for Graph-Constrained House Generation</div>
          <!-- Author -->
          <div class="author">
          
          Â <em>Hao Tang</em>, Â Zhenyu Zhang, Â Humphrey Shi, Â Bo Li, Â Ling Shao, Â Nicu Sebe, Â Radu Timofte, Â Luc Van Gool  </div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2303.08225.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Unsupervised Deep Probabilistic Approach for Partial Point Cloud Registration</div>
          <!-- Author -->
          <div class="author">
          
          Â Guofeng Mei, Â <em>Hao Tang</em>, Â Xiaoshui Huang, Â Weijie Wang, Â Juan Liu, Â Jian Zhang, Â Luc Van Gool, Â Qiang Wu  </div>
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2303.13290" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/gfmei/UDPReg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network</div>
          <!-- Author -->
          <div class="author">
          
          Â Xuan Shen, Â Yaohua Wang, Â Ming Lin, Â Yilun Huang, Â <em>Hao Tang</em>, Â Xiuyu Sun, Â Yanzhi Wang </div>
          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2303.02165.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/alibaba/lightweight-neural-architecture-search" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â Ming Tao, Â Bingkun Bao, Â <em>Hao Tang</em>, Â Changsheng Xu </div>
          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2301.12959.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/tobran/GALIP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations" rel="external nofollow noopener" target="_blank">ICLR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Edge Guided GANs with Contrastive Learning for Semantic Image Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â <em>Hao Tang</em>, Â Xiaojuan Qi, Â Guolei Sun, Â Dan Xu, Â Nicu Sebe, Â Radu Timofte, Â Luc Van Gool </div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>ICLR</em> 2023, Kigali, Rwanda
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2003.13898" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/ECGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">3D-Aware Semantic-Guided Generative Model for Human Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Jichao Zhang, Â Enver Sangineto, Â <em>Hao Tang</em>, Â Aliaksandr Siarohin, Â Zhun Zhong, Â Nicu Sebe, Â Wei Wang</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>ECCV</em> 2022, Tel Aviv, Israel
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2112.01422.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/zhangqianhui/3DSGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Towards Interpretable Video Super-Resolution via Alternative Optimization</div>
          <!-- Author -->
          <div class="author">
          
          Jiezhang Cao, Â Jingyun Liang, Â Kai Zhang, Â Wenguan Wang, Â Qin Wang, Â Yulun Zhang, Â <em>Hao Tang</em>, Â Luc Van Gool </div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>ECCV</em> 2022, Tel Aviv, Israel
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2207.10765.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/caojiezhang/DAVSR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="cai2022mask" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation</div>
          <!-- Author -->
          <div class="author">
          
          Wenhao Li, Â Hong Liu, Â <em>Hao Tang</em>, Â Pichao Wang, Â Luc Van Gool</div>
          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In Computer Vision and Pattern Recognition</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2022, New Orleans, USA
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_MHFormer_Multi-Hypothesis_Transformer_for_3D_Human_Pose_Estimation_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Vegetebird/MHFormer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR Oral</a></abbr></div>

        <!-- Entry bib key -->
        <div id="cai2022mask" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Ming Tao, Â <em>Hao Tang</em>, Â Fei Wu, Â Xiaoyuan Jing, Â Bingkun Bao, Â Changsheng Xu</div>
          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In Computer Vision and Pattern Recognition</em> 2022
          </div> -->
          <div class="periodical">
          
              In <em>CVPR</em> 2022, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_DF-GAN_A_Simple_and_Effective_Baseline_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/tobran/DF-GAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/ACM_Multimedia" rel="external nofollow noopener" target="_blank">ACM MM Best Paper Candidate</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">GestureGAN for Hand Gesture-to-Gesture Translation in the Wild</div>
          <!-- Author -->
          <div class="author">
          
          <em>Hao Tang</em>, Â Wei Wang, Â Dan Xu, Â Yan Yan, Â Nicu Sebe</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
          
            In <em> ACM MM</em> 2018, Seoul, South Korea
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://dl.acm.org/doi/pdf/10.1145/3240508.3240704" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GestureGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

</ol>
          </div>

    
    <!-- <a href="https://clustrmaps.com/site/19ncr" title="Visit tracker" rel="external nofollow noopener" target="_blank"><img src="//www.clustrmaps.com/map_v2.png?d=jIdUd0dDYkE8CiqptfhnfiWcZHCc5p62dIsontyW-FQ&amp;cl=ffffff" style="width: 0px;"></a> -->
  </article>

</div>

    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        Â© Copyright 2015-2023 Hao Tang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. 

        Last updated: Feb 8, 2024.

        <!-- Default Statcounter code for Hao Tang' homepage
        https://ha0tang.github.io/ -->
        <script type="text/javascript">
        var sc_project=12830645; 
        var sc_invisible=1; 
        var sc_security="11b4016e"; 
        </script>
        <script type="text/javascript"
        src="https://www.statcounter.com/counter/counter.js"
        async></script>
        <noscript><div class="statcounter"><a title="Web Analytics
        Made Easy - Statcounter" href="https://statcounter.com/"
        target="_blank"><img class="statcounter"
        src="https://c.statcounter.com/12830645/0/11b4016e/1/"
        alt="Web Analytics Made Easy - Statcounter"
        referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
        <!-- End of Statcounter Code -->

      </div>
    </footer>




    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
