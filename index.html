<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Hao Tang</title>
    <meta name="author" content="Hao Tang">
    <meta name="description" content="&lt;p&gt; Computer Vision Lab, ETH ZÃ¼rich, Switzerland&lt;br&gt; Office: ETF C 108, Sternwartstrasse 7, 8092 ZÃ¼rich, Switzerland&lt;br&gt; Email: bjdxtanghao@gmail.com&lt;/p&gt;
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;">

    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              
              <!--  -->
              <!-- Blog -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li> -->
              
              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/services/">Services</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/others/">Others</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <div class="post">


  <article>
    <div class="row">
        <!-- <div class="col-sm-3"> -->
            
            <!-- <img style="width: 170px;border-radius: 10px" src="/assets/img/th.png"> -->
            
        <!-- </div> -->
    
        <div class="col-sm-9">
            <h1 class="post-title">
            Hao Tang
            </h1>
            <p class="desc"></p>
          <p>School of Computer Science, Peking University<br> Office: 5 Yiheyuan Road, Haidian District, Beijing, 100871, ChinağŸ‡¨ğŸ‡³<br> Email: bjdxtanghao@gmail.com</p>

            <div class="social">
                <div class="contact-icons">
                <a href="https://scholar.google.com/citations?user=9zJkeEMAAAAJ&hl=en" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/Ha0Tang" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/hao-tang-887475138/" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://dblp.org/pid/07/5751-5.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a>
            <a href="https://twitter.com/HaoTang_ai" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            <a href="https://orcid.org/0000-0002-2077-1246" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="fab fa-orcid"></i></a>
            <a href="https://arxiv.org/a/tang_h_1.html" title="Arxiv" target="_blank" rel="noopener noreferrer"><i class="ai ai-arxiv"></i></a>
            <a href="https://zhuanlan.zhihu.com/p/710908497" title="Zhihu" target="_blank" rel="noopener noreferrer"><i class="fab fa-zhihu"></i></a>
                </div>
            </div>
   
        </div>
    </div>

    <div class="clearfix">
      <p>Hey, thanks for stopping by! ğŸ‘‹</p>

      <p>
        I am a tenure-track Assistant Professor at the School of Computer Science, Peking University, ChinağŸ‡¨ğŸ‡³. Previously, I held postdoctoral positions at both CMU, USAğŸ‡ºğŸ‡¸, and ETH ZÃ¼rich, SwitzerlandğŸ‡¨ğŸ‡­. My academic journey includes earning a master's degree from Peking University, ChinağŸ‡¨ğŸ‡³, and completing my Ph.D. at University of Trento, ItalyğŸ‡®ğŸ‡¹. Additionally, I had the privilege of being a visiting Ph.D. student at the University of Oxford, UKğŸ‡¬ğŸ‡§. Furthermore, I undertook a visiting internship at IIAI, UAEğŸ‡¦ğŸ‡ª. My research interests are AIGC, AI4Science (especially AI4Healthcare), computer vision, embodied AI, LLM, and the applications of such tools to scientific domains.
      </p>
      
<!--       <p>
        Beyond academia, I have also had the honor of serving as a senior technical advisor for numerous AI startups, including those in USAğŸ‡ºğŸ‡¸, UKğŸ‡¬ğŸ‡§, RomaniağŸ‡·ğŸ‡´, and ChinağŸ‡¨ğŸ‡³, with technologies ranging from Efficient AI to AIGC to AI4Blockchain, etc.
      </p> -->


<!--       <div>
        <ul>
            <li>Generative AI (e.g., GANs, diffusion models) and its applications (e.g., image generation, image translation, text-to-image synthesis/editing, person image synthesis, semantic image synthesis, style transfer, video generation, graph-based generation, layout generation)</li>
            <li>AI for Science</li>
            <li>Large language model (LLM)</li>
            <li>Embodied AI</li>
            <li>Efficient AI (e.g., pruning, distillation, quantization, NAS)</li>
            <li>Interpretability AI</li>
            <li>Robustness AI</li>
            <li>Multi-modalities (e.g., audio-to-video synthesis, text-to-music synthesis, language-vision, point cloud)</li>
            <li>Low-level vision (image/video restoration, super-resolution, denoising, deblurring, HDR deghosting)</li>
            <li>High-level vision (depth estimation, segmentation, detection, recognition)</li>
            <li>3D vision (e.g., nerf, 3D-aware image/video generation, object reconstruction/generation, 3D pose transfer)</li>
            <li>Medical image enhancement and analysis</li>
            <li>Human pose estimation and motion prediction</li>
        </ul>    
    </div> -->

<!--       <style>
      .box {
        max-width: 100%;
        width: auto;
        height: auto;
        background-color: #333333; /* ä½¿ç”¨åå…­è¿›åˆ¶è¡¨ç¤ºæ³•è¡¨ç¤ºæµ…ç°è‰²èƒŒæ™¯ */
        color: hsl(0, 70%, 30%); /* ä½¿ç”¨ HSL è¡¨ç¤ºæ³•è¡¨ç¤ºè¾ƒæ·±çš„æ–‡å­—é¢œè‰² */
        padding: 20px;
        border-radius: 10px;
        margin: 20px auto;
      }
      </style> -->
      <h2>Position Openings</h2>
      <div class="box">
            <p><strong style="color: red;">If this resonates with you, we are actively hiring. </strong></p>

            <p>
            For prospective collaborators interested in AIGC and AI4Science, we have multiple positions available for Postdoc/Ph.D./Master/Intern researchers. We welcome applicants from diverse disciplines, including Computer Scienceã€Physicsã€Chemistryã€Financeã€Agricultureã€Biologyã€Medicineã€Meteorologyã€Geographyã€Architectureã€and more. If you are interested in using AI to solve problems in your field, you are encouraged to apply. Please email me with your self-introduction, the project of interest (including the problem you are trying to solve and how you plan to solve it, being as specific as possible), your transcript, and CV. Send your application to haotang@pku.edu.cn. I might not be able to respond to all emails due to large volume.</p>

            <p>In my lab, you will receive personalized, one-on-one mentorship to guide your research journey. Many of the students and interns I've co-mentored have gone on to pursue further studies at prestigious institutions such as MIT, Harvard, University of Toronto, Caltech, NTU, NUS, and TUM, among others.</p>
      </div>

      <h2>International Collaborations</h2>
      <div class="box">
            <p><strong style="color: red;">By joining my lab, you will have the opportunity to collaborate and engage with some of the world's top researchers, expanding your academic network and gaining valuable insights from leading experts in your field. </strong></p>

            <p>
            I maintain strong collaborative relationships with several leading international research institutions, including MIT, Harvard, Stanford University, CMU, ETH ZÃ¼rich, University of Michigan, University of Oxford, University of Cambridge, University of Trento, Northeastern University, University of Texas at Austin, TUM, University of WÃ¼rzburg, Inria, University of Lille, University of Oulu, NUS, NTU, National Institute of Informatics, Sungkyunkwan University, University of Adelaide, ANU, Monash University, IIAI, University of Hong Kong, and Hong Kong University of Science and Technology, etc. I am deeply grateful for the opportunities to collaborate with such esteemed institutions and for the valuable contributions they have made to our joint research efforts.
      </div>

    </div>


          <div class="news">
            <h2>News</h2>
            <div class="table-responsive" style="max-height: 20vw">
              <table class="table table-sm table-borderless">

                <th scope="row">2024-09</th>
                  <td>
                    ğŸ‰ğŸ‰ğŸ‰I was elected as one of the <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/7" target="_blank">World's Top 2% Scientists in 2024 by Stanford University</a>,
                    and we have 1 paper (Camera-Agnostic Attack) accepted to <strong>NeurIPS 2024</strong> and 1 paper (Medical Image Segmentation) accepted to <strong>ACCV 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-08</th>
                  <td>
                    ğŸ‰ğŸ‰ğŸ‰I was invited as a speaker at <a href="https://cv-ac.github.io/MiGA2/" target="_blank">the 2nd Workshop & Challenge on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA)</a> at IJCAI 2024 and we have 2 papers (Guided Image Translation + 3D Human Pose Estimation) accepted to <strong>PR 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-07</th>
                  <td>
                    We have 6 papers (Motion Mamba + Dataset Growth + Story Visualization and Completion + Diffusion Model for Semantic Image Synthesis + Generalizable Image Editing + 3D Semantic Segmentation) accepted to <strong>ECCV 2024</strong>, 1 paper (Survey about Physical Adversarial Attack) accepted to <strong>TPAMI 2024</strong>, and two papers (Talking Head Avatar + Story Visualization and Continuation) accepted to <strong>ACM MM 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-06</th>
                  <td>
                    ğŸ‰ğŸ‰ğŸ‰I joined <strong>Peking University</strong> as an Assistant Professor.
                  </td>
                </tr> 

                <th scope="row">2024-04</th>
                  <td>
                    ğŸ‰ğŸ‰ğŸ‰I received offers from <strong>MIT</strong> and <strong>Harvard University</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-02</th>
                  <td>
                    We have 7 papers (Explanation for ViT + Faithfulness of ViT + Diffusion Policy for Versatile Navigation + Subject-Driven Generation [Final rating: 455] + Diffusion Model for 3D Hand Pose Estimation + Adversarial Learning for 3D Pose Transfer + Efficient Diffusion Distillation [224->235]) accepted to <strong>CVPR 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-01</th>
                  <td>
                    We have 1 paper (Architectural Layout Generation) accepted to <strong>TPAMI 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-12</th>
                  <td>
                    We have 1 paper (Sign Pose Sequence Generation) accepted to <strong>AAAI 2024</strong>.
                  </td>
                </tr> 


                <th scope="row">2023-10</th>
                  <td>
                    ğŸ‰ğŸ‰ğŸ‰I was elected as one of the <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/6" target="_blank">World's Top 2% Scientists in 2023 by Stanford University</a>
                    and we have 4 papers (BEV Perception + Efficient ViT + 3D Motion Transfer + Graph Distillation) accepted to <strong>NeurIPS 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-09</th>
                  <td>
                    We have 1 paper (Practical Blind Image Denoising) accepted to <strong>MIR 2023</strong> and 1 paper (Diffusion Model for HDR Deghosting) accepted to <strong>TCSVT 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-08</th>
                  <td>
                    ğŸ‰ğŸ‰ğŸ‰I received an offer from <strong>CMU</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-07</th>
                  <td>
                    We have 1 paper (Semantic Image Synthesis) accepted to <strong>TPAMI 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-06</th>
                  <td>
                    We have 1 paper (Visible-Infrared Person Re-ID) accepted to <strong>ICCV 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-05</th>
                  <td>
                    We have 2 papers (Image Restoration Dataset + 3D-Aware Video Generation) accepted to <strong>CVPRW 2023</strong> and 1 paper (3D Face Generation) accepted to <strong>JSTSP 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-04</th>
                  <td>
                    We have 1 paper (Speed-Aware Object Detection) accepted to <strong>ICML 2023</strong>, 2 papers (Lottery Ticket Hypothesis for ViT + Zero-shot Character Recognition) accepted to <strong>IJCAI 2023</strong>, 1 paper (3D Human Pose Estimation) accepted to <strong>PR 2023</strong>, and 1 paper (SAR Target Recognition) accepted to <strong>TGRS 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-03</th>
                  <td>
                    We have 6 papers (HDR Deghosting + Point Cloud Registration + Graph-Constrained House Generation + Mathematical Architecture Design + Text-to-Image Synthesis + Efficient Semantic Segmentation) accepted to <strong>CVPR 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-02</th>
                  <td>
                    We have 3 papers (Camouflaged Object Detection + Brain Vessel Image Segmentation + Cross-View Image Translation) accepted to <strong>ICASSP 2023</strong> and 1 paper (Camouflaged Object Detection) accepted to <strong>TCSVT 2023</strong>.
                  </td>
                </tr> 
               
                <tr>
                  <th scope="row">2023-01</th>
                  <td>
                    We have 1 paper (Semantic Image Synthesis) accepted to <strong>ICLR 2023</strong> and 1 paper (Human Reaction Generation) accepted to <strong>TMM 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-11</th>
                  <td>
                    We have 4 papers (Real-Time Segmentation + Wearable Design + Efficient ViT Training + Text-Guided Image Editing) accepted to <strong>AAAI 2023</strong>, 1 paper accepted (Person Pose and Facial Image Synthesis) to <strong>IJCV 2022</strong>, 1 paper (Salient Object Detection) accepted to <strong>TIP 2022</strong>, and 1 paper (Object Detection Transformer) accepted to <strong>TCSVT 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2022-10</th>
                  <td>
                    We have 1 paper (Sinusoidal Neural Radiance Fields) accepted to <strong>BMVC 2022</strong> and 1 paper (Guided Image-to-Image Translation) accepted to <strong>TPAMI 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-09</th>
                  <td>
                    We have 1 paper (Facial Expression Translation) accepted to <strong>TAFFC 2022</strong> and 1 paper (Ship Detection) accepted to <strong>TGRS 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-07</th>
                  <td>
                     We have 5 papers (Real-Time SR + Video SR + Soft Token Pruning for ViT + 3D-Aware Human Synthesis + Video Semantic Segmentation) accepted to <strong>ECCV 2022</strong>, 1 paper (Gaze Correction and Animation) accepted to <strong>TIP 2022</strong>, and 1 paper (Cross-view Panorama Image Synthesis) accepted to <strong>PR 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-06</th>
                  <td>
                     We have 2 papers (Character Image Restoration + Character Image Denoising) accepted to <strong>ACM MM 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-04</th>
                  <td>
                    We have 1 paper (Real-Time Portrait Stylization) accepted to <strong>IJCAI 2022</strong>, 1 paper (Wide-Context Transformer for Semantic Segmentation) accepted to <strong>TGRS 2022</strong>, and 1 paper (Incremental Learning for Semantic Segmentation) accepted to <strong>TMM 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2022-03</th>
                  <td>
                    We have 5 papers (Text-to-Image Synthesis + 3D Human Pose Estimation + Text-Driven Image Manipulation + 3D Face Modeling + 3D Face Restoration) accepted to <strong>CVPR 2022</strong>, 1 paper (Image Generation) accepted to <strong>TPAMI 2022</strong>, and 1 paper (Cross-View Panorama Image Synthesis) accepted to <strong>TMM 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-12</th>
                  <td>
                    We have 2 papers (Generalized 3D Pose Transfer + Audio-Visual Speaker Tracking) accepted to <strong>AAAI 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-11</th>
                  <td>
                    We have 1 paper (Building Extraction in VHR Remote Sensing Images) accepted to <strong>TIP 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-10</th>
                  <td>
                    We have 3 papers (Cross-View Image Translation + Data-driven 3D Animation + Natural Image Matting) accepted to <strong>BMVC 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-08</th>
                  <td>
                    We have 1 paper (Layout-to-Image Translation) accepted to <strong>TIP 2021</strong> and 1 paper (Unpaired Image-to-Image Translation) accepted to <strong>TNNLS 2021</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2021-07</th>
                  <td>
                    We have 2 papers (Continuous Pixel-Wise Prediction + Unsupervised 3D Pose Transfer) accepted to <strong>ICCV 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-06</th>
                  <td>
                    We have 1 paper (Cross-View Exocentric to Egocentric Video Synthesis) accepted to <strong>ACM MM 2021</strong> and 1 paper (Total Generate) accepted to <strong>TMM 2021</strong>.
                  </td>
                </tr> 

                <th scope="row">2021-05</th>
                  <td>
                    ğŸ‰ğŸ‰ğŸ‰I received an offer from <strong>ETH Zurich</strong>.
                  </td>
                </tr> 

                <th scope="row">2020-09</th>
                  <td>
                    ğŸ‰ğŸ‰ğŸ‰I received an offer from <strong>IIAI</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2020-08</th>
                  <td>
                    We have 1 paper (Person Image Generation) accepted to <strong>BMVC 2020</strong>, 2 papers (Semantic Image Synthesis + Unsupervised Gaze Correction and Animation) accepted to <strong>ACM MM 2020</strong>, and 1 paper (Controllable Image-to-Image Translation) accepted to <strong>TIP 2020</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2020-07</th>
                  <td>
                    We have 1 paper (Person Image Generation) accepted to <strong>ECCV 2020</strong>.
                  </td>
                </tr>

                <tr>
                  <th scope="row">2020-05</th>
                  <td>
                    We have 1 paper (Deep Dictionary Learning and Coding) accepted to <strong>TNNLS 2020</strong> and 1 paper (Semantic Segmentation of Remote Sensing Images) accepted to <strong>TGRS 2020</strong>.
                  </td>
                </tr>

                <tr>
                  <th scope="row">2020-02</th>
                  <td>
                    We have 1 paper (Semantic-Guided Scene Generation) accepted to <strong>CVPR 2020</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2019-07</th>
                  <td>
                    We have 1 paper (Keypoint-Guided Image Generation) accepted to <strong>ACM MM 2019</strong>.
                  </td>
                </tr>

                <th scope="row">2019-05</th>
                  <td>
                    ğŸ‰ğŸ‰ğŸ‰I received an offer from <strong>University of Oxford</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2019-02</th>
                  <td>
                    We have 1 paper (Cross-View Image Translation) accepted to <strong>CVPR 2019</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2018-06</th>
                  <td>
                    We have 1 paper (Hand Gesture-to-Gesture Translation) accepted to <strong>ACM MM 2018</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2018-02</th>
                  <td>
                    We have 1 paper (Monocular Depth Estimation) accepted to <strong>CVPR 2018</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2016-07</th>
                  <td>
                    We have 1 paper (Large Scale Image Retrieval) accepted to <strong>IJCAI 2016</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2015-08</th>
                  <td>
                    We have 1 paper (Gender Classification) accepted to <strong>ACM MM 2015</strong>.
                  </td>
                </tr>
                
              </table>
            </div> 
          </div>
      
          <div class="publications">
            <h2>Featured Publications</h2>
            <h5>(Including CVPR, NeurIPS, TPAMI, ICML, ICLR, ICCV, ECCV, AAAI, IJCAI, ACM MM)</h5>
            <p class="post-description"><sup>â€ </sup>My Students or Interns,  <sup>*</sup>Corresponding Author(s)</p>
            <ol class="bibliography">

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Toward Zero-Shot Learning for Visual Dehazing of Urological Surgical Robots</div>
          <!-- Author -->
          <div class="author">
          
          Â Renkai Wu, Â Xianjin Wang, Â Pengchen Liang, Â Zhenyu Zhang, Â Qing Chang*, Â <em>Hao Tang*</em></div>


          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2410.01395" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/wurenkai/RSF-Dehaze" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Barbie: Text to Barbie-Style 3D Avatars</div>
          <!-- Author -->
          <div class="author">
          
          Â Xiaokun Sun, Â Zhenyu Zhang, Â Ying Tai, Â Qian Wang, Â <em>Hao Tang</em>, Â Zili Yi, Â Jian Yang</em></div>


          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2408.09126" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/2017211801/Barbie" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">InfiniMotion: Mamba Boosts Memory in Transformer for Arbitrary Long Motion Generation</div>
          <!-- Author -->
          <div class="author">
          
          Â Zeyu Zhang, Â Akide Liu, Â Qi Chen, Â Feng Chen, Â Ian Reid, Â Richard Hartley, Â Bohan Zhuang, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2407.10061" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://steve-zeyu-zhang.github.io/InfiniMotion/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">A Survey on Multimodal Wearable Sensor-based Human Action Recognition</div>
          <!-- Author -->
          <div class="author">
          
          Â Jianyuan Ni, Â <em>Hao Tang</em>, Â Syed Tousiful Haque, Â Yan Yan, Â Anne HH Ngu</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.15349" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Enlighten-Your-Voice: When Multimodal Meets Zero-shot Low-light Image
Enhancement</div>
          <!-- Author -->
          <div class="author">
        
          Â Xiaofeng Zhang, Â Zishan Xu, Â <em>Hao Tang</em>, Â Chaochen Gu, Â Wei Chen, Â Shanying Zhu, Â Xinping Guan</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2312.10109v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation</div>
          <!-- Author -->
          <div class="author">
          
          Â Bin Xie, Â <em>Hao Tang</em>, Â Bin Duan, Â Dawen Cai, Â Yan Yan</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.14103" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Efficient Pruning of Large Language Model with Adaptive Estimation Fusion</div>
          <!-- Author -->
          <div class="author">
          
          Â Jun Liu, Â Chao Wu, Â Changdi Yang, Â <em>Hao Tang*</em>, Â Haoye Dong, Â Zhenglun Kong, Â Geng Yuan, Â Wei Niu, Â Dong Huang*, Â Yanzhi Wang*</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.10799" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">StableGarment: Garment-Centric Generation via Stable Diffusion</div>
          <!-- Author -->
          <div class="author">
          
          Â Rui Wang, Â Hailong Guo, Â Jiaming Liu, Â Huaxia Li, Â Haibo Zhao, Â Xu Tang, Â Yao Hu, Â <em>Hao Tang</em>, Â Peipei Li</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.10783" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" rel="external nofollow noopener" target="_blank">TPAMI</a></abbr></div>

        <!-- Entry bib key -->
        <div id="xia2022gan" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Physical Adversarial Attack Neets Computer Vision: A Decade Survey</div>
          <!-- Author -->
          <div class="author">
          

          Â Hui Wei, Â <em>Hao Tang</em>, Â Xuemei Jia, Â Zhixiang Wang, Â Hanxun Yu, Â Zhubo Li, Â Shin'ichi Satoh, Â Luc Van Gool, Â Zheng Wang</div>

          <div class="periodical">
          
            <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2209.15179" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/weihui1308/PAA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Motion Mamba: Efficient and Long Sequence Motion Generation with Hierarchical and Bidirectional Selective SSM</div>
          <!-- Author -->
          <div class="author">
          
          Â Zeyu Zhang, Â Akide Liu, Â Ian Reid, Â Richard Hartley, Â Bohan Zhuang, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.07487" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/steve-zeyu-zhang/MotionMamba/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">3D Weakly Supervised Semantic Segmentation with 2D Vision-Language Guidance</div>
          <!-- Author -->
          <div class="author">
          
          Â Xiaoxu Xu, Â Yitian Yuan, Â Jinlong Li, Â Qiudan Zhang, Â Zequn Jie, Â Lin Ma, Â <em>Hao Tang</em>, Â Nicu Sebe, Â Xu Wang</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2407.09826v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/xuxiaoxxxx/3DSS-VLG/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">StoryImager: A Unified and Efficient Framework for Coherent Story Visualization and Completion</div>
          <!-- Author -->
          <div class="author">
          
          Â Ming Tao, Â Bingkun Bao, Â <em>Hao Tang</em>, Â Yaowei Wang, Â Changsheng Xu</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.05979" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/tobran/StoryImager" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">InstructGIE: Towards Generalizable Image Editing</div>
          <!-- Author -->
          <div class="author">
        
          Â Zichong Meng, Â Changdi Yang, Â Jun Liu, Â <em>Hao Tang*</em>, Â Pu Zhao*, Â Yanzhi Wang*</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.05018" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://cr8br0ze.github.io/InstructGIE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior</div>
          <!-- Author -->
          <div class="author">
          
          Â Huan-ang Gao, Â Mingju Gao, Â Jiaju Li, Â Wenyi Li, Â Rong Zhi, Â <em>Hao Tang</em>, Â Hao Zhao</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.09638" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://air-discover.github.io/SCP-Diff/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Dataset Growth</div>
          <!-- Author -->
          <div class="author">
          
          Â Ziheng Qin, Â Zhaopan Xu, Â Yukun Zhou, Â Zangwei Zheng, Â Zebang Cheng, Â <em>Hao Tang</em>, Â Lei Shang, Â Baigui Sun, Â Xiaojiang Peng, Â Radu Timofte, Â Hongxun Yao, Â Kai Wang, Â Yang You</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2405.18347" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/NUS-HPC-AI-Lab/InfoGrowth" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR <br> Highlight</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud</div>
          <!-- Author -->
          <div class="author">
          
          Â Wencan Cheng, Â <em>Hao Tang</em>, Â Luc Van Gool, Â Jong Hwan Ko </div>
          
          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.03159" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/cwc1260/HandDiff" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Versatile Navigation under Partial Observability via Value-guided Diffusion Policy</div>
          <!-- Author -->
          <div class="author">
          
          Â Gengyu Zhang, Â <em>Hao Tang</em>, Â Yan Yan </div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.02176" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Towards Robust 3D Pose Transfer with Adversarial Learning</div>
          <!-- Author -->
          <div class="author">
          
          Â Haoyu Chen, Â <em>Hao Tang</em>, Â Ehsan Adeli, Â Guoying Zhao </div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.02242" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">On the Faithfulness of Vision Transformer Explanations</div>
          <!-- Author -->
          <div class="author">
          
          Â Junyi Wu, Â Weitai Kang, Â <em>Hao Tang</em>, Â Yuan Hong, Â Yan Yan </div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.01415" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer</div>
          <!-- Author -->
          <div class="author">
          
          Â Junyi Wu, Â Bin Duan, Â Weitai Kang, Â <em>Hao Tang</em>, Â Yan Yan </div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.14552" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation</div>
          <!-- Author -->
          <div class="author">
          
          Â Yuxuan Zhang, Â Jiaming Liu, Â Yiren Song, Â Rui Wang, Â <em>Hao Tang</em>, Â Jinpeng Yu, Â Huaxia Li, Â Xu Tang, Â Yao Hu, Â Han Pan, Â Zhongliang Jing</div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2312.16272" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Xiaojiu-z/SSR_Encoder" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR <br> Workshop</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Towards Online Real-Time Memory-based Video Inpainting Transformers</div>
          <!-- Author -->
          <div class="author">
          
          Â Guillaume Thiry, Â <em>Hao Tang*</em>, Â Radu Timofte, Â Luc Van Gool</div>
          
          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.16161" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div>

        <!-- Entry bib key -->
        <div id="chen2022cross" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">G2P-DDM: Generating Sign Pose Sequence from Gloss Sequence with Discrete Diffusion Model</div>
          <!-- Author -->
          <div class="author">
          
          Â Pan Xie, Â Qipeng Zhang, Â Peng Taiying, Â <em>Hao Tang*</em>, Â Yao Du, Â Zexian Li</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>AAAI</em> 2024, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2208.09141.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://slpdiffusier.github.io/g2p-ddm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">HotBEV: Hardware-oriented Transformer-based Multi-View 3D Detector for BEV Perception</div>
          <!-- Author -->
          <div class="author">

          Â Peiyan Dong, Â Zhenglun Kong, Â Xin Meng, Â Pinrui Yu, Â Yifan Gong, Â Geng Yuan, Â <em>Hao Tang*</em>, Yanzhi Wang</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=3Cj67k38st" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/PeiyanFlying/HotBEV" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">PackQViT: Faster Sub-8-bit Vision Transformers via Full and Packed Quantization on the Mobile</div>
          <!-- Author -->
          <div class="author">

        Â Peiyan Dong, Â Lei Lu, Â Chao Wu, Â Cheng Lyu, Â Geng Yuan, Â <em>Hao Tang*</em>, Yanzhi Wang</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=N56hAiQvot" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/PeiyanFlying/PackQViT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">LART: Neural Correspondence Learning with Latent Regularization Transformer for 3D Motion Transfer</div>
          <!-- Author -->
          <div class="author">

        Â Haoyu Chen, Â <em>Hao Tang</em>, Â Radu Timofte, Â Luc Van Gool, Â Guoying Zhao</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=g27BggUT3L" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/mikecheninoulu/LART" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Does Graph Distillation See Like Vision Dataset Counterpart?</div>
          <!-- Author -->
          <div class="author">

        Â Beining Yang, Â Kai Wang, Â Qingyun Sun, Â Cheng Ji, Â Xingcheng Fu, Â <em>Hao Tang</em>, Â Yang You, Â Jianxin Li</div>

          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2310.09192.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/RingBDStack/SGDD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://link.springer.com/journal/11633" rel="external nofollow noopener" target="_blank">MIR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Practical Blind Image Denoising via Swin-Conv-UNet and Data Synthesis</div>
          <!-- Author -->
          <div class="author">

        Â Kai Zhang, Â Yawei Li, Â Jingyun Liang, Â Jiezhang Cao, Â Yulun Zhang, Â <em>Hao Tang</em>, Â Dengping Fan, Â Radu Timofte, Â Luc Van Gool</div>

          <div class="periodical">
          
          <em>Springer Machine Intelligence Research (MIR)</em>, 2023
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://link.springer.com/article/10.1007/s11633-023-1466-0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/cszn/SCUNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ICCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Learning Concordant Attention via Target-aware Alignment for Visible-Infrared Person Re-identification</div>
          <!-- Author -->
          <div class="author">
          
          Â Jianbing Wu, Â Hong Liu, Â Yuxin Su, Â Wei Shi, Â <em>Hao Tang</em></div>

          <div class="periodical">
          
            In <em>ICCV</em> 2023, Paris, France
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Learning_Concordant_Attention_via_Target-aware_Alignment_for_Visible-Infrared_Person_Re-identification_ICCV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning" rel="external nofollow noopener" target="_blank">ICML</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">SpeedDETR: Speed-aware Transformers for End-to-end Object Detection</div>
          <!-- Author -->
          <div class="author">
          
          Â Peiyan Dong, Â Zhenglun Kong, Â Xin Meng, Â Peng Zhang, Â <em>Hao Tang*</em>, Â Yanzhi Wang, Â Chih-Hsien Chou</div>

          <div class="periodical">
          
            In <em>ICML</em> 2023, Hawaii, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=5VdcSxrlTK" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/PeiyanFlying/SpeedDETR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Joint_Conference_on_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">IJCAI</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <div class="title">Data Level Lottery Ticket Hypothesis for Vision Transformers</div>
          <!-- Author -->
          <div class="author">
          
          Â Xuan Shen, Â Zhenglun Kong, Â Minghai Qin, Â Peiyan Dong, Â Geng Yuan, Â Xin Meng, Â <em>Hao Tang</em>, Â Xiaolong Ma, Â Yanzhi Wang</div>

          <div class="periodical">
          
            In <em>IJCAI</em> 2023, Macao, China
          
          </div>
        
          <div class="links">
            <a href="https://arxiv.org/pdf/2211.01484.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/shawnricecake/vit-lottery-ticket-input" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Graph Transformer GANs for Graph-Constrained House Generation</div>
          <!-- Author -->
          <div class="author">
          
          Â <em>Hao Tang</em>, Â Zhenyu Zhang, Â Humphrey Shi, Â Bo Li, Â Ling Shao, Â Nicu Sebe, Â Radu Timofte, Â Luc Van Gool</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2303.08225.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Unsupervised Deep Probabilistic Approach for Partial Point Cloud Registration</div>
          <!-- Author -->
          <div class="author">
          
          Â Guofeng Mei, Â <em>Hao Tang</em>, Â Xiaoshui Huang, Â Weijie Wang, Â Juan Liu, Â Jian Zhang, Â Luc Van Gool, Â Qiang Wu  </div>
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2303.13290" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/gfmei/UDPReg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network</div>
          <!-- Author -->
          <div class="author">
          
          Â Xuan Shen, Â Yaohua Wang, Â Ming Lin, Â Yilun Huang, Â <em>Hao Tang</em>, Â Xiuyu Sun, Â Yanzhi Wang </div>
          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2303.02165.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/alibaba/lightweight-neural-architecture-search" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â Ming Tao, Â Bingkun Bao, Â <em>Hao Tang</em>, Â Changsheng Xu</div>
          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2301.12959.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/tobran/GALIP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations" rel="external nofollow noopener" target="_blank">ICLR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Edge Guided GANs with Contrastive Learning for Semantic Image Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â <em>Hao Tang</em>, Â Xiaojuan Qi, Â Guolei Sun, Â Dan Xu, Â Nicu Sebe, Â Radu Timofte, Â Luc Van Gool</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>ICLR</em> 2023, Kigali, Rwanda
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2003.13898" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/ECGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">3D-Aware Semantic-Guided Generative Model for Human Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â Jichao Zhang, Â Enver Sangineto, Â <em>Hao Tang</em>, Â Aliaksandr Siarohin, Â Zhun Zhong, Â Nicu Sebe, Â Wei Wang</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>ECCV</em> 2022, Tel Aviv, Israel
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2112.01422.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/zhangqianhui/3DSGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Towards Interpretable Video Super-Resolution via Alternative Optimization</div>
          <!-- Author -->
          <div class="author">
          
          Â Jiezhang Cao, Â Jingyun Liang, Â Kai Zhang, Â Wenguan Wang, Â Qin Wang, Â Yulun Zhang, Â <em>Hao Tang</em>, Â Luc Van Gool</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>ECCV</em> 2022, Tel Aviv, Israel
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2207.10765.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/caojiezhang/DAVSR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="cai2022mask" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation</div>
          <!-- Author -->
          <div class="author">
          
          Â Wenhao Li, Â Hong Liu, Â <em>Hao Tang</em>, Â Pichao Wang, Â Luc Van Gool</div>

          <div class="periodical">
          
            In <em>CVPR</em> 2022, New Orleans, USA
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_MHFormer_Multi-Hypothesis_Transformer_for_3D_Human_Pose_Estimation_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Vegetebird/MHFormer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR Oral</a></abbr></div>

        <!-- Entry bib key -->
        <div id="cai2022mask" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â Ming Tao, Â <em>Hao Tang</em>, Â Fei Wu, Â Xiaoyuan Jing, Â Bingkun Bao, Â Changsheng Xu</div>

          <div class="periodical">
          
              In <em>CVPR</em> 2022, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_DF-GAN_A_Simple_and_Effective_Baseline_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/tobran/DF-GAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/ACM_Multimedia" rel="external nofollow noopener" target="_blank">ACM MM Best<br>Paper Candidate</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">GestureGAN for Hand Gesture-to-Gesture Translation in the Wild</div>
          <!-- Author -->
          <div class="author">
          
          Â <em>Hao Tang</em>, Â Wei Wang, Â Dan Xu, Â Yan Yan, Â Nicu Sebe</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
          
            In <em> ACM MM</em> 2018, Seoul, South Korea
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://dl.acm.org/doi/pdf/10.1145/3240508.3240704" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GestureGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

</ol>
          </div>    
    <!-- <a href="https://clustrmaps.com/site/19ncr" title="Visit tracker" rel="external nofollow noopener" target="_blank"><img src="//www.clustrmaps.com/map_v2.png?d=jIdUd0dDYkE8CiqptfhnfiWcZHCc5p62dIsontyW-FQ&amp;cl=ffffff" style="width: 0px;"></a> -->
  </article>

</div>

    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        Â© Copyright 2024 Hao Tang. Last updated: Oct 16, 2024.

        <!-- Default Statcounter code for Hao Tang' homepage
        https://ha0tang.github.io/ -->
        <script type="text/javascript">
        var sc_project=12830645; 
        var sc_invisible=1; 
        var sc_security="11b4016e"; 
        </script>
        <script type="text/javascript"
        src="https://www.statcounter.com/counter/counter.js"
        async></script>
        <noscript><div class="statcounter"><a title="Web Analytics
        Made Easy - Statcounter" href="https://statcounter.com/"
        target="_blank"><img class="statcounter"
        src="https://c.statcounter.com/12830645/0/11b4016e/1/"
        alt="Web Analytics Made Easy - Statcounter"
        referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
        <!-- End of Statcounter Code -->

      </div>
    </footer>


    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
