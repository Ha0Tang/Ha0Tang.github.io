<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Hao Tang</title>
    <meta name="author" content="Hao Tang">
    <meta name="description" content="&lt;p&gt; Computer Vision Lab, ETH ZÃ¼rich, Switzerland&lt;br&gt; Office: ETF C 108, Sternwartstrasse 7, 8092 ZÃ¼rich, Switzerland&lt;br&gt; Email: bjdxtanghao@gmail.com&lt;/p&gt;
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text y=%22.9em%22 font-size=%2290%22&gt;%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;">

    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              
              <!--  -->
              <!-- Blog -->
              <!-- <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li> -->
              
              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/services/">Services</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/others/">Others</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <div class="post">


  <article>
    <div class="row">
        <div class="col-sm-3">
            
            <img style="width: 200px;border-radius: 10px" src="/assets/img/th.jpg">
            
        </div>
    
        <div class="col-sm-9">
            <h1 class="post-title">
            Hao Tang
            </h1>
            <p class="desc"></p>
          <p><span style="color: gray;">åŒ—äº¬å¤§å­¦ åŠ©ç†æ•™æˆ åšå£«ç”Ÿå¯¼å¸ˆ åŒ—å¤§åšé›…é’å¹´å­¦è€… å›½å®¶çº§æµ·å¤–é«˜æ°´å¹³äººæ‰è®¡åˆ’å…¥é€‰è€…</span><br>Office: 5 Yiheyuan Road, Haidian District, Beijing, 100871, ChinağŸ‡¨ğŸ‡³<br> Email: bjdxtanghao@gmail.com</p>

            <div class="social">
                <div class="contact-icons">
                <a href="https://scholar.google.com/citations?user=9zJkeEMAAAAJ&hl=en" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/Ha0Tang" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/hao-tang-887475138/" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://dblp.org/pid/07/5751-5.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a>
            <a href="https://twitter.com/HaoTang_ai" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            <a href="https://orcid.org/0000-0002-2077-1246" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="fab fa-orcid"></i></a>
            <a href="https://arxiv.org/a/tang_h_1.html" title="Arxiv" target="_blank" rel="noopener noreferrer"><i class="ai ai-arxiv"></i></a>
            <a href="https://zhuanlan.zhihu.com/p/710908497" title="Zhihu" target="_blank" rel="noopener noreferrer"><i class="fab fa-zhihu"></i></a>
                </div>
            </div>
   
        </div>
    </div>

    <div class="clearfix">
      <p>Hey, thanks for stopping by! ğŸ‘‹</p>

      <p>
        I am a tenure-track Assistant Professor at the School of Computer Science, Peking University, ChinağŸ‡¨ğŸ‡³, studying computer vision, machine learning, and AI. Previously, I held postdoctoral positions at both CMU (Robotics Institute), USAğŸ‡ºğŸ‡¸, and ETH ZÃ¼rich (Computer Vision Lab), SwitzerlandğŸ‡¨ğŸ‡­. My academic journey includes earning a master's degree from Peking University, ChinağŸ‡¨ğŸ‡³, and completing my Ph.D. (cum laude) at University of Trento, ItalyğŸ‡®ğŸ‡¹. Additionally, I had the privilege of being a visiting Ph.D. student at the University of Oxford, UKğŸ‡¬ğŸ‡§. Furthermore, I undertook a visiting internship at IIAI, UAEğŸ‡¦ğŸ‡ª. 
      </p>
      
      <p>
        Beyond academia, I have also had the honor of serving as a senior technical advisor for numerous AI startups, including those in USAğŸ‡ºğŸ‡¸, UKğŸ‡¬ğŸ‡§, RomaniağŸ‡·ğŸ‡´, and ChinağŸ‡¨ğŸ‡³, with technologies ranging from Efficient AI to 3D to AIGC to AI4Blockchain, etc.
      </p>

    </div>

          <div class="news">
            <h2>News & Events</h2>
            <div class="table-responsive" style="max-height: 20vw">
              <table class="table table-sm table-borderless">

                  <th scope="row"><strong style="color: red;">Hiring!</strong></th>
                  <td>
                    <strong style="color: red;">We're hiring Postdoc/Ph.D./Master/Intern researchers on Embodied AI, and AIGC (including LLM) for our PKU lab, feel free to reach out to me direclty.</strong>
                  </td>
                </tr> 

                <th scope="row">2025-04</th>
                  <td>
                    We have 1 paper (Continual Gesture Learning) accepted to <strong>IJCNN 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-03</th>
                  <td>
                    ğŸ‰I was invited to serve as an Area Chair (AC) at <strong>ACM MM 2025</strong>, and we have 1 paper (Accident Warning Agent) accepted to <strong>IV 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-02</th>
                  <td>
                    ğŸ‰I was invited to serve as an Area Chair (AC) for the Large Language Models (LLM) track at <strong>ACL 2025</strong> and a SPC at <strong>IJCAI 2025</strong>, and we have 3 papers including 1 oral (Mamba for Image Compression + 4D Reconstruction + Diffusion Fourier Neural Operator) accepted to <strong>CVPR 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2025-01</th>
                  <td>
                    We have 1 paper (SAR Automatic Target Recognition) accepted to <strong>TAES 2025</strong>, 1 paper (Person Image Generation) accepted to <strong>TPAMI 2025</strong>, 1 paper (Explainability in MLLMs) accepted to <strong>NAACL 2025 Main Conference</strong>, and 1 paper (Urological Surgical Robots) accepted to <strong>ICRA 2025</strong>. 
                  </td>
                </tr> 

                <th scope="row">2024-12</th>
                  <td>
                    We have 3 papers (Structured Pruning for LLM + FG-SBIR + Hair Transfer via Diffusion Model) accepted to <strong>AAAI 2025</strong> and 1 paper (Efficient Fine-Tuning of LLM) accepted to <strong>ICASSP 2025</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-11</th>
                  <td>
                    We have 1 paper (Virtual Try-On) accepted to <strong>TMM 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-10</th>
                  <td>
                    We have 1 paper (Quantization on Bird's-Eye View Representation) accepted to <strong>WACV 2025</strong> and 1 paper (Semantic Segmentation on Autonomous Vehicles Platform) accepted to <strong>TCAD 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-09</th>
                  <td>
                    ğŸ‰I was elected as one of the <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/7" target="_blank">World's Top 2% Scientists in 2024 by Stanford University</a>,
                    and we have 1 paper (Camera-Agnostic Attack) accepted to <strong>NeurIPS 2024</strong> and 1 paper (Medical Image Segmentation) accepted to <strong>ACCV 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-08</th>
                  <td>
                    ğŸ‰I was invited as a speaker at <a href="https://cv-ac.github.io/MiGA2/" target="_blank">the 2nd Workshop & Challenge on Micro-gesture Analysis for Hidden Emotion Understanding (MiGA)</a> at IJCAI 2024 and we have 2 papers (Guided Image Translation + 3D Human Pose Estimation) accepted to <strong>PR 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-07</th>
                  <td>
                    We have 6 papers (Motion Mamba + Dataset Growth + Story Visualization and Completion + Diffusion Model for Semantic Image Synthesis + Generalizable Image Editing + 3D Semantic Segmentation) accepted to <strong>ECCV 2024</strong>, 1 paper (Survey about Physical Adversarial Attack) accepted to <strong>TPAMI 2024</strong>, and 2 papers (Talking Head Avatar + Story Visualization and Continuation) accepted to <strong>ACM MM 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-06</th>
                  <td>
                    ğŸ‰I joined <strong>Peking University</strong> as an Assistant Professor.
                  </td>
                </tr> 

                <th scope="row">2024-04</th>
                  <td>
                    ğŸ‰I received offers from <strong>MIT</strong> and <strong>Harvard University</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-02</th>
                  <td>
                    We have 7 papers (Explanation for ViT + Faithfulness of ViT + Diffusion Policy for Versatile Navigation + Subject-Driven Generation [Final rating: 455] + Diffusion Model for 3D Hand Pose Estimation + Adversarial Learning for 3D Pose Transfer + Efficient Diffusion Distillation [224->235]) accepted to <strong>CVPR 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2024-01</th>
                  <td>
                    We have 1 paper (Architectural Layout Generation) accepted to <strong>TPAMI 2024</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-12</th>
                  <td>
                    We have 1 paper (Sign Pose Sequence Generation) accepted to <strong>AAAI 2024</strong>.
                  </td>
                </tr> 


                <th scope="row">2023-10</th>
                  <td>
                    ğŸ‰I was elected as one of the <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/6" target="_blank">World's Top 2% Scientists in 2023 by Stanford University</a>
                    and we have 4 papers (BEV Perception + Efficient ViT + 3D Motion Transfer + Graph Distillation) accepted to <strong>NeurIPS 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-09</th>
                  <td>
                    We have 1 paper (Practical Blind Image Denoising) accepted to <strong>MIR 2023</strong> and 1 paper (Diffusion Model for HDR Deghosting) accepted to <strong>TCSVT 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-08</th>
                  <td>
                    ğŸ‰I received an offer from <strong>CMU</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-07</th>
                  <td>
                    We have 1 paper (Semantic Image Synthesis) accepted to <strong>TPAMI 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-06</th>
                  <td>
                    We have 1 paper (Visible-Infrared Person Re-ID) accepted to <strong>ICCV 2023</strong>.
                  </td>
                </tr> 

                <th scope="row">2023-05</th>
                  <td>
                    We have 2 papers (Image Restoration Dataset + 3D-Aware Video Generation) accepted to <strong>CVPRW 2023</strong> and 1 paper (3D Face Generation) accepted to <strong>JSTSP 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-04</th>
                  <td>
                    We have 1 paper (Speed-Aware Object Detection) accepted to <strong>ICML 2023</strong>, 2 papers (Lottery Ticket Hypothesis for ViT + Zero-shot Character Recognition) accepted to <strong>IJCAI 2023</strong>, 1 paper (3D Human Pose Estimation) accepted to <strong>PR 2023</strong>, and 1 paper (SAR Target Recognition) accepted to <strong>TGRS 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-03</th>
                  <td>
                    We have 6 papers (HDR Deghosting + Point Cloud Registration + Graph-Constrained House Generation + Mathematical Architecture Design + Text-to-Image Synthesis + Efficient Semantic Segmentation) accepted to <strong>CVPR 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2023-02</th>
                  <td>
                    We have 3 papers (Camouflaged Object Detection + Brain Vessel Image Segmentation + Cross-View Image Translation) accepted to <strong>ICASSP 2023</strong> and 1 paper (Camouflaged Object Detection) accepted to <strong>TCSVT 2023</strong>.
                  </td>
                </tr> 
               
                <tr>
                  <th scope="row">2023-01</th>
                  <td>
                    We have 1 paper (Semantic Image Synthesis) accepted to <strong>ICLR 2023</strong> and 1 paper (Human Reaction Generation) accepted to <strong>TMM 2023</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-11</th>
                  <td>
                    We have 4 papers (Real-Time Segmentation + Wearable Design + Efficient ViT Training + Text-Guided Image Editing) accepted to <strong>AAAI 2023</strong>, 1 paper accepted (Person Pose and Facial Image Synthesis) to <strong>IJCV 2022</strong>, 1 paper (Salient Object Detection) accepted to <strong>TIP 2022</strong>, and 1 paper (Object Detection Transformer) accepted to <strong>TCSVT 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2022-10</th>
                  <td>
                    We have 1 paper (Sinusoidal Neural Radiance Fields) accepted to <strong>BMVC 2022</strong> and 1 paper (Guided Image-to-Image Translation) accepted to <strong>TPAMI 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-09</th>
                  <td>
                    We have 1 paper (Facial Expression Translation) accepted to <strong>TAFFC 2022</strong> and 1 paper (Ship Detection) accepted to <strong>TGRS 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-07</th>
                  <td>
                     We have 5 papers (Real-Time SR + Video SR + Soft Token Pruning for ViT + 3D-Aware Human Synthesis + Video Semantic Segmentation) accepted to <strong>ECCV 2022</strong>, 1 paper (Gaze Correction and Animation) accepted to <strong>TIP 2022</strong>, and 1 paper (Cross-view Panorama Image Synthesis) accepted to <strong>PR 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-06</th>
                  <td>
                     We have 2 papers (Character Image Restoration + Character Image Denoising) accepted to <strong>ACM MM 2022</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2022-04</th>
                  <td>
                    We have 1 paper (Real-Time Portrait Stylization) accepted to <strong>IJCAI 2022</strong>, 1 paper (Wide-Context Transformer for Semantic Segmentation) accepted to <strong>TGRS 2022</strong>, and 1 paper (Incremental Learning for Semantic Segmentation) accepted to <strong>TMM 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2022-03</th>
                  <td>
                    We have 5 papers including 1 oral (Text-to-Image Synthesis + 3D Human Pose Estimation + Text-Driven Image Manipulation + 3D Face Modeling + 3D Face Restoration) accepted to <strong>CVPR 2022</strong>, 1 paper (Image Generation) accepted to <strong>TPAMI 2022</strong>, and 1 paper (Cross-View Panorama Image Synthesis) accepted to <strong>TMM 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-12</th>
                  <td>
                    We have 2 papers (Generalized 3D Pose Transfer + Audio-Visual Speaker Tracking) accepted to <strong>AAAI 2022</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-11</th>
                  <td>
                    We have 1 paper (Building Extraction in VHR Remote Sensing Images) accepted to <strong>TIP 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-10</th>
                  <td>
                    We have 3 papers (Cross-View Image Translation + Data-driven 3D Animation + Natural Image Matting) accepted to <strong>BMVC 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-08</th>
                  <td>
                    We have 1 paper (Layout-to-Image Translation) accepted to <strong>TIP 2021</strong> and 1 paper (Unpaired Image-to-Image Translation) accepted to <strong>TNNLS 2021</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2021-07</th>
                  <td>
                    We have 2 papers (Continuous Pixel-Wise Prediction + Unsupervised 3D Pose Transfer) accepted to <strong>ICCV 2021</strong>.
                  </td>
                </tr> 
                <tr>
                  <th scope="row">2021-06</th>
                  <td>
                    We have 1 paper (Cross-View Exocentric to Egocentric Video Synthesis) accepted to <strong>ACM MM 2021</strong> and 1 paper (Total Generate) accepted to <strong>TMM 2021</strong>.
                  </td>
                </tr> 

                <th scope="row">2021-05</th>
                  <td>
                    ğŸ‰I received an offer from <strong>ETH Zurich</strong>.
                  </td>
                </tr> 

                <th scope="row">2020-09</th>
                  <td>
                    ğŸ‰I received an offer from <strong>IIAI</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2020-08</th>
                  <td>
                    We have 1 paper (Person Image Generation) accepted to <strong>BMVC 2020</strong>, 2 papers (Semantic Image Synthesis + Unsupervised Gaze Correction and Animation) accepted to <strong>ACM MM 2020</strong>, and 1 paper (Controllable Image-to-Image Translation) accepted to <strong>TIP 2020</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2020-07</th>
                  <td>
                    We have 1 paper (Person Image Generation) accepted to <strong>ECCV 2020</strong>.
                  </td>
                </tr>

                <tr>
                  <th scope="row">2020-05</th>
                  <td>
                    We have 1 paper (Deep Dictionary Learning and Coding) accepted to <strong>TNNLS 2020</strong> and 1 paper (Semantic Segmentation of Remote Sensing Images) accepted to <strong>TGRS 2020</strong>.
                  </td>
                </tr>

                <tr>
                  <th scope="row">2020-02</th>
                  <td>
                    We have 1 paper (Semantic-Guided Scene Generation) accepted to <strong>CVPR 2020</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2019-07</th>
                  <td>
                    We have 1 paper (Keypoint-Guided Image Generation) accepted to <strong>ACM MM 2019</strong>.
                  </td>
                </tr>

                <th scope="row">2019-05</th>
                  <td>
                    ğŸ‰I received an offer from <strong>University of Oxford</strong>.
                  </td>
                </tr> 

                <tr>
                  <th scope="row">2019-02</th>
                  <td>
                    We have 1 paper (Cross-View Image Translation) accepted to <strong>CVPR 2019</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2018-06</th>
                  <td>
                    We have 1 paper (Hand Gesture-to-Gesture Translation) accepted to <strong>ACM MM 2018</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2018-02</th>
                  <td>
                    We have 1 paper (Monocular Depth Estimation) accepted to <strong>CVPR 2018</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2016-07</th>
                  <td>
                    We have 1 paper (Large Scale Image Retrieval) accepted to <strong>IJCAI 2016</strong>.
                  </td>
                </tr>
                <tr>
                  <th scope="row">2015-08</th>
                  <td>
                    We have 1 paper (Gender Classification) accepted to <strong>ACM MM 2015</strong>.
                  </td>
                </tr>
                
              </table>
            </div> 
          </div>


<div class="lab" style="margin-top: 40px;">
      <h2>Position Openings</h2>
      <div class="box">
            <!-- <p><strong style="color: red;">We are recruiting two postdocs to join our team, focusing on cutting-edge Embodied AI topics (e.g., robotics, perception, and adaptive intelligence).</strong></p> -->

            <p>For prospective collaborators interested in Embodied AI, and AIGC (including LLM), we are offering multiple positions for Postdoc/Ph.D./Master/Intern researchers. We welcome applicants from diverse disciplines, including Computer Scienceã€Roboticsã€Medicineã€Physicsã€Chemistryã€Biologyã€and more. If you are interested in using AI to solve problems in your field, you are encouraged to apply. Please email me with your self-introduction, the project of interest (including the problem you are trying to solve and how you plan to solve it, being as specific as possible), your transcript, and CV. Send your application to haotang@pku.edu.cn. I'm sorry that I may not be able to respond to every email, but I assure you that your message will stand out if you have a strong research background.</p>

            <p>For Ph.D./Postdoc/Master applicants, we have several openings for domestic students each year. Please reach out at least one year prior to the application deadline.

            For international students, PKU CS offers a variety of programs in English, including <a href="https://twitter.com/HaoTang_ai/status/1847175500558750055/photo/1" target="_blank">Master's</a>, <a href="http://www.studyatpku.com" target="_blank">Ph.D. programs</a>, <a href="https://cs.pku.edu.cn/English/Internationalization/Summer_Winter_Camp1.htm" target="_blank">Summer/Winter Schools</a>, and <a href="https://cs.pku.edu.cn/English/Internationalization/Internship_Program.htm" target="_blank">various other options</a>. Feel free to reach out if you are interested or have any questions.

            For visiting students/undergraduates/research interns/research assistants, we welcome undergraduate and graduate students from all over the world to apply for >6 months research internship. Our visitors/interns have published many top-tier conference/journal papers (e.g., TPAMIã€CVPRã€NeurIPS) and have been admitted to Postdoc/Ph.D./Master programs in prestigious institutions such as MIT, Harvard, Google, University of Toronto, Caltech, ETH ZÃ¼rich, NTU, NUS, and TUM.</p>
      </div>
  
      <h2>Research Lab</h2>
      <div class="box">

            <p>The mission of our research lab is to harness AI to address real-world challenges. Our research priorities include Embodied AI and AIGC.</p>

        <ul>
            <li>Xiaoyuan Wang (Visiting from CMU, USAğŸ‡ºğŸ‡¸)</li>
            <li>Zhenyu Lu (Visiting from CMU, USAğŸ‡ºğŸ‡¸)</li>
            <li>Wenbo Gou (Visiting from CMU, USAğŸ‡ºğŸ‡¸)</li>
            <li>Jun Liu (Visiting from Northeastern University, USAğŸ‡ºğŸ‡¸)</li>
            <li>Zihao Wang (Visiting from UPenn, USAğŸ‡ºğŸ‡¸)</li>
            <li>Yao Gong (Visiting from UPenn, USAğŸ‡ºğŸ‡¸)</li>
            <li>Junjie Zeng (Visiting from UMich, USAğŸ‡ºğŸ‡¸)</li>
            <li>Xiaoyi Liu (Visiting from Washington University in St. Louis, USAğŸ‡ºğŸ‡¸)</li>
            <li>Jingyi Wan (Visiting from University of Cambridge, UKğŸ‡¬ğŸ‡§)</li>
            <li>Xuanyu Lai (Visiting from Imperial College London, UKğŸ‡¬ğŸ‡§)</li>
            <li>Baohua Yin (Visiting from University of Sussex, UKğŸ‡¬ğŸ‡§)</li>
            <li>Zhiguang Han (Visiting from NTU, SingaporeğŸ‡¸ğŸ‡¬)</li>
            <li>Pirzada Suhail (Visiting from IIT Bombay, IndiağŸ‡®ğŸ‡³)</li>
            <li>Zeyu Zhang (Visiting from Australian National University, AustraliağŸ‡¦ğŸ‡º)</li>
            <li>Hongpeng Wang (Visiting from University of Sydney, AustraliağŸ‡¦ğŸ‡º)</li>
            <li>Zeyu Ren (Visiting from University of Melbourne, AustraliağŸ‡¦ğŸ‡º)</li>
            <li>Zhixing Wang (Visiting from University of Malaya, MalaysiağŸ‡²ğŸ‡¾)</li>
            <li>Yuxuan Fan (Visiting from HKUST (Guangzhou), ChinağŸ‡¨ğŸ‡³)</li>
            <li>Nonghai Zhang (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Chenyang Gu (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Keyu Chen (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Di Yu (Visiting from Tsinghua University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Yuxuan Zhang (Visiting from Shanghai Jiao Tong University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Renkai Wu (Visiting from Shanghai Jiao Tong University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Yihua Shao (Visiting from University of Science and Technology Beijing, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Aoming Liang (Visiting from Westlake University, ChinağŸ‡¨ğŸ‡³)</li>
            <li>Sifan Li (Visiting from Liaoning University, ChinağŸ‡¨ğŸ‡³)</li>
        </ul>   

        <p><strong>Former members and visitors:</strong> 

        <ul>
            <li>Guillaume Thiry (Intern, now Software Engineer at Google, SwitzerlandğŸ‡¨ğŸ‡­)</li>
            <li>Sherwin Bahmani (Intern, now Ph.D. student at University of Toronto, CanadağŸ‡¨ğŸ‡¦)</li>
            <li>Sanghwan Kim (Intern, now Ph.D. student at TUM, GermanyğŸ‡©ğŸ‡ª)</li>
            <li>Alexandros Delitzas (Intern, now Ph.D. student at ETH ZÃ¼rich and Max Planck Institute for Informatics, SwitzerlandğŸ‡¨ğŸ‡­ and GermanyğŸ‡©ğŸ‡ª)</li>
            <li>Jingfeng Rong (Intern, now Ph.D. student at Swiss Finance Institute, SwitzerlandğŸ‡¨ğŸ‡­)</li>
            <li>Yitong Xia (Intern from ETH ZÃ¼rich, SwitzerlandğŸ‡¨ğŸ‡­)</li>
            <li>Boyan Duan (Intern, now Master at ETH ZÃ¼rich, SwitzerlandğŸ‡¨ğŸ‡­)</li>
            <li>Baptiste Chopin (now Postdoc at INRIA, FranceğŸ‡«ğŸ‡·)</li>
            <li>Xiaoyu Yi (Intern from Peking University, ChinağŸ‡¨ğŸ‡³)</li>
        </ul>   
   
      </div>

      <h2>Teaching</h2>
      <div class="box">   
            <ul>
              <li><a href="https://ha0tang.github.io/DGM_PKU" target="_blank">PKU: Deep Generative Models</a></li>   
              <li>PKU: Video Encoding and Understanding</li>   
            </ul>   

      </div>

      <h2>International Collaborations</h2>
      <div class="box">

            <p>
            Our lab maintains strong collaborative relationships with several leading international research institutions, including 
              <ul>
                  <li>USAğŸ‡ºğŸ‡¸: MIT, Harvard, Stanford University, CMU, Princeton University, UIUC, UMich, Northeastern University, University of Maryland, University of Texas at Austin, UC Irvine, University of Illinois at Chicago, Illinois Institute of Technology, University of Connecticut, Texas State University, University of Georgia, Clemson University, University of Oregon, College of William & Mary</li>
                  <li>CanadağŸ‡¨ğŸ‡¦: University of Toronto, Simon Fraser University</li>
                  <li>SwitzerlandğŸ‡¨ğŸ‡­: ETH ZÃ¼rich, EPFL</li>
                  <li>UKğŸ‡¬ğŸ‡§: University of Oxford, University of Cambridge, University of Leicester, University of Warwick</li>
                  <li>ItalyğŸ‡®ğŸ‡¹: University of Trento, FBK, Politecnico di Milano, University of Modena e Reggio Emilia</li>
                  <li>GermanyğŸ‡©ğŸ‡ª: TUM, University of WÃ¼rzburg</li>
                  <li>FranceğŸ‡«ğŸ‡·: INRIA, University of Lille</li>
                  <li>FinlandğŸ‡«ğŸ‡®: University of Oulu</li>
                  <li>NetherlandsğŸ‡³ğŸ‡±: TU Delft</li>
                  <li>BelgiumğŸ‡§ğŸ‡ª: KU Leuven</li>
                  <li>BulgariağŸ‡§ğŸ‡¬: INSAIT</li>
                  <li>SingaporeğŸ‡¸ğŸ‡¬: NUS, NTU</li>
                  <li>JapanğŸ‡¯ğŸ‡µ: University of Tokyo, National Institute of Informatics</li>
                  <li>South KoreağŸ‡°ğŸ‡·: Sungkyunkwan University</li>
                  <li>AustraliağŸ‡¦ğŸ‡º: University of Adelaide, ANU, Monash University, University of Technology Sydney</li>
                  <li>UAEğŸ‡¦ğŸ‡ª: IIAI, MBZUAI</li>  
                  <li>HongKongğŸ‡­ğŸ‡°: University of Hong Kong, Hong Kong University of Science and Technology</li>  
              </ul>   
            I am deeply grateful for the opportunities to collaborate with such esteemed institutions and for the valuable contributions they have made to our joint research efforts.

            Additionally, we maintain long-term collaborations with industry, including Google, Meta, Amazon, Cisco, Western Digital, Mercedes-Benz, Xiaohongshu, Alibaba, Tencent, etc, aiming to translate cutting-edge research into practical applications and drive technological advancement.
      </div>

    </div>
      
          <div class="publications">
            <h2>Featured Publications</h2>
            <h5>(Including CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, AAAI, IJCAI, ACM MM, ICRA, NAACL, TPAMI, IJCV)</h5>
            <p class="post-description"><sup>â€ </sup>My Students or Interns, <sup>*</sup>Corresponding Author(s)</p>
            <ol class="bibliography">

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance</div>
          <!-- Author -->
          <div class="author">

          Â Haijie Yang, Â Zhenyu Zhang, Â <em>Hao Tang</em>, Â Jianjun Qian, Â Jian Yang</div>
          <div class="periodical">
          
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2503.22225" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">FE-UNet: Frequency Domain Enhanced U-Net with Segment Anything Capability for Versatile Image Segmentation</div>
          <!-- Author -->
          <div class="author">

        Â Guohao Huo, Â Ruiting Dai, Â Ling Shao, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2502.03829" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">RFMedSAM 2: Automatic Prompt Refinement for Enhanced Volumetric Medical Image Segmentation with SAM 2</div>
          <!-- Author -->
          <div class="author">

        Â Bin Xie, Â <em>Hao Tang</em>, Â Yan Yan, Â Gady Agam</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2502.02741" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Self-Prompt SAM: Medical Image Segmentation via Automatic Prompt SAM Adaptation</div>
          <!-- Author -->
          <div class="author">

        Â Bin Xie, Â <em>Hao Tang</em>, Â Dawen Cai, Â Yan Yan, Â Gady Agam</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2502.00630" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">In-Context Meta LoRA Generation</div>
          <!-- Author -->
          <div class="author">

          Â Yihua Shao, Â Minxi Yan, Â Yang Liu, Â Siyu Chen, Â Wenjie Chen, Â Xinwei Long, Â Ziyang Yan, Â Lei Li, Â Chenyu Zhang, Â Nicu Sebe, Â <em>Hao Tang*</em>, Â Yan Wang, Â Hao Zhao, Â Mengzhu Wang, Â Jingcai Guo*</div>
          <div class="periodical">
          
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2501.17635" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">UDiTQC: U-Net-Style Diffusion Transformer for Quantum Circuit Synthesis</div>
          <!-- Author -->
          <div class="author">

        Â Zhiwei Chen, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2501.16380" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Boosting Adversarial Transferability with Spatial Adversarial Alignment</div>
          <!-- Author -->
          <div class="author">

        Â Zhaoyu Chen, Â Haijing Guo, Â Kaixun Jiang, Â Jiyuan Fu, Â Xinyu Zhou, Â Dingkang Yang, Â <em>Hao Tang</em>, Â Bo Li, Â Wenqiang Zhang</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2025
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2501.01015" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Artificial Intelligence for Quantum Error Correction: A Comprehensive Review</div>
          <!-- Author -->
          <div class="author">
          
          Â Zihao Wang, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2412.20380" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="https://thequantuminsider.com/2025/01/06/ai-for-quantum-error-correction-a-comprehensive-guide-to-using-artificial-intelligence-to-improve-quantum-error-correction/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Media</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">PolSAM: Polarimetric Scattering Mechanism Informed Segment Anything Model</div>
          <!-- Author -->
          <div class="author">
          
          Â Yuqing Wang, Â Zhongling Huang, Â Shuxin Yang, Â <em>Hao Tang</em>, Â Xiaolan Qiu, Â Junwei Han, Â Dingwen Zhang</div>
          
          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2412.12737" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/XAI4SAR/PolSAM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Artificial Intelligence for Central Dogma-Centric Multi-Omics: Challenges and Breakthroughs</div>
          <!-- Author -->
          <div class="author">
          
          Â Lei Xin, Â Caiyun Huang, Â Hao Li, Â Shihong Huang, Â Yuling Feng, Â Zhenglun Kong, Â Zicheng Liu, Â Siyuan Li, Â Chang Yu, Â Fei Shen, Â <em>Hao Tang*</em></div>
          
          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2412.12668" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Text-to-Image Synthesis: A Decade Survey</div>
          <!-- Author -->
          <div class="author">
          
          Â Nonghai Zhang, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2411.16164" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Multimodal Alignment and Fusion: A Survey</div>
          <!-- Author -->
          <div class="author">
          
          Â Songtao Li, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2411.17040" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">AllRestorer: All-in-One Transformer for Image Restoration under Composite Degradations</div>
          <!-- Author -->
          <div class="author">
          
          Â Jiawei Mao, Â Yu Yang, Â Xuesong Yin, Â Ling Shao, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2411.10708" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">KMM: Key Frame Mask Mamba for Extended Motion Generation</div>
          <!-- Author -->
          <div class="author">
          
          Â Zeyu Zhang, Â Hang Gao, Â Akide Liu, Â Qi Chen, Â Feng Chen, Â Yiran Wang, Â Danning Li, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2411.06481" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/steve-zeyu-zhang/KMM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">GWQ: Gradient-Aware Weight Quantization for Large Language Models</div>
          <!-- Author -->
          <div class="author">
          
          Â Yihua Shao, Â Siyu Liang, Â Xiaolin Lin, Â Zijian Ling, Â Zixian Zhu, Â Minxi Yan, Â Haiyang Liu, Â Siyu Chen, Â Ziyang Yan, Â Yilan Meng, Â Chenyu Zhang, Â Haotong Qin*, Â Michele Magno, Â Yang Yang, Â Zhen Lei, Â Yan Wang, Â Jingcai Guo, Â Ling Shao, Â <em>Hao Tang*</em></div>


          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2411.00850" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">M<sup>2</sup>M: Learning Controllable Multi of Experts and Multi-Scale Operators Are the Partial Differential Equations Need</div>
          <!-- Author -->
          <div class="author">
          
          Â Aoming Liang, Â Zhaoyang Mu, Â Pengxiao Lin, Â Cong Wang,  Â Mingming Ge, Â Ling Shao, Â Dixia Fan*, Â <em>Hao Tang*</em></div>


          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2410.11617" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Barbie: Text to Barbie-Style 3D Avatars</div>
          <!-- Author -->
          <div class="author">
          
          Â Xiaokun Sun, Â Zhenyu Zhang, Â Ying Tai, Â Qian Wang, Â <em>Hao Tang</em>, Â Zili Yi, Â Jian Yang</em></div>


          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2408.09126" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/2017211801/Barbie" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">InfiniMotion: Mamba Boosts Memory in Transformer for Arbitrary Long Motion Generation</div>
          <!-- Author -->
          <div class="author">
          
          Â Zeyu Zhang, Â Akide Liu, Â Qi Chen, Â Feng Chen, Â Ian Reid, Â Richard Hartley, Â Bohan Zhuang, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2407.10061" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://steve-zeyu-zhang.github.io/InfiniMotion/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">A Survey on Multimodal Wearable Sensor-based Human Action Recognition</div>
          <!-- Author -->
          <div class="author">
          
          Â Jianyuan Ni, Â <em>Hao Tang</em>, Â Syed Tousiful Haque, Â Yan Yan, Â Anne HH Ngu</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.15349" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation</div>
          <!-- Author -->
          <div class="author">
          
          Â Bin Xie, Â <em>Hao Tang</em>, Â Bin Duan, Â Dawen Cai, Â Yan Yan</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.14103" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#FF0000"><a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">Arxiv</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">StableGarment: Garment-Centric Generation via Stable Diffusion</div>
          <!-- Author -->
          <div class="author">
          
          Â Rui Wang, Â Hailong Guo, Â Jiaming Liu, Â Huaxia Li, Â Haibo Zhao, Â Xu Tang, Â Yao Hu, Â <em>Hao Tang</em>, Â Peipei Li</div>

          <div class="periodical">
          
            In <em>Arxiv</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.10783" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/logn-2024/StableGarment" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR Oral</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">DiffFNO: Diffusion Fourier Neural Operator</div>
          <!-- Author -->
          <div class="author">
          
          Â Xiaoyi Liu, Â <em>Hao Tang*</em></div>
          
          <div class="periodical">
          
            In <em>CVPR</em> 2025, Nashville, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2411.09911" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">MambaIC: State Space Models for High-Performance Learned Image Compression</div>
          <!-- Author -->
          <div class="author">
          
          Â Fanhu Zeng, Â <em>Hao Tang</em>, Â Yihua Shao, Â Siyu Chen, Â Ling Shao, Â Yan Wang</div>

          <div class="periodical">
          
            In <em>CVPR</em> 2025, Nashville, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2503.12461" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/AuroraZengfh/MambaIC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model</div>
          <!-- Author -->
          <div class="author">
          
          Â Mingju Gao, Â Yike Pan, Â Huan-ang Gao, Â Zongzheng Zhang, Â Wenyi Li, Â Hao Dong, Â <em>Hao Tang</em>, Â Li Yi, Â Hao Zhao</div>
          
          <div class="periodical">
          
            In <em>CVPR</em> 2025, Nashville, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2503.19913" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/GasaiYU/PartRM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Robotics_and_Automation" rel="external nofollow noopener" target="_blank">ICRA</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Toward Zero-Shot Learning for Visual Dehazing of Urological Surgical Robots</div>
          <!-- Author -->
          <div class="author">

          Â Renkai Wu, Â Xianjin Wang, Â Pengchen Liang, Â Zhenyu Zhang, Â Qing Chang*, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>ICRA</em> 2025, Atlanta, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2410.01395" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/wurenkai/RSF-Dehaze" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/North_American_Chapter_of_the_Association_for_Computational_Linguistics" target="_blank">NAACL</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks</div>
          <!-- Author -->
          <div class="author">

          Â Xiaofeng Zhang, Â Yihao Quan, Â Chen Shen, Â Xiaosong Yuan, Â Shaotian Yan, Â Liang Xie, Â Wenxiao Wang, Â Chaochen Gu, Â <em>Hao Tang</em>, Jieping Ye</div>

          <div class="periodical">
          
            In <em>NAACL</em> 2025, Albuquerque, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2406.06579" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/zhangbaijin/From-Redundancy-to-Relevance" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Toward Adaptive Large Language Models Structured Pruning via Hybrid-grained Weight Importance Assessment</div>
          <!-- Author -->
          <div class="author">

          Â Jun Liu, Â Zhenglun Kong, Â Pu Zhao, Â Changdi Yang, Â <em>Hao Tang*</em>, Â Xuan Shen, Â Geng Yuan, Â Wei Niu, Â Wenbin Zhang, Â Xue Lin, Â Dong Huang*, Â Yanzhi Wang*</div>

          <div class="periodical">
          
            In <em>AAAI</em> 2025, Philadelphia, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.10799" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div>

        <!-- Entry bib key -->
        <div id="chen2022cross" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Stable-Hair: Real-World Hair Transfer via Diffusion Model</div>
          <!-- Author -->
          <div class="author">
          
          Â Yuxuan Zhang, Â Qing Zhang, Â Yiren Song, Â Jichao Zhang, Â <em>Hao Tang</em>, Â Jiaming Liu</div>
          
          <!-- <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>AAAI</em> 2025, Philadelphia, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2407.14078" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://xiaojiu-z.github.io/Stable-Hair.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
     
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Towards Self-Supervised FG-SBIR with Unified Sample Feature Alignment and Multi-Scale Token Recycling</div>
          <!-- Author -->
          <div class="author">
          
          Â Jianan Jiang, Â <em>Hao Tang*</em>, Â Zhilin Jiang, Â Weiren Yu, Â Di Wu*</div>

          <div class="periodical">
          
            In <em>AAAI</em> 2025, Philadelphia, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2406.11551" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/ExponentiAI/ARNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Revisiting Adversarial Patches for Designing Camera-Agnostic Attacks against Person Detection</div>
          <!-- Author -->
          <div class="author">

          Â Hui Wei, Â Zhixiang Wang, Â Kewei Zhang, Â Jiaqi Hou, Â Yuanwei Liu, Â <em>Hao Tang</em>, Â Zheng Wang</div>

          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2024, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/forum?id=2Inwtjvyx8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/weihui1308/CAP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/ACM_Multimedia" rel="external nofollow noopener" target="_blank">ACM MM Oral</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">ConsistentAvatar: Learning to Diffuse Fully Consistent Talking Head Avatar with Temporal Guidance</div>
          <!-- Author -->
          <div class="author">
          
          Â Haijie Yang, Â Zhenyu Zhang, Â <em>Hao Tang</em>,  Â Jianjun Qian, Â Jian Yang</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
          
            In <em> ACM MM</em> 2024, Melbourne, Australia
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://dl.acm.org/doi/10.1145/3664647.3680619" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/ACM_Multimedia" rel="external nofollow noopener" target="_blank">ACM MM</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">CoIn: A Lightweight and Effective Framework for Story Visualization and Continuation</div>
          <!-- Author -->
          <div class="author">
          
          Â Ming Tao, Â Bao Bingkun, Â <em>Hao Tang</em>,  Â Yaowei Wang, Â Changsheng Xu</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
          
            In <em>ACM MM</em> 2024, Melbourne, Australia
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://dl.acm.org/doi/10.1145/3664647.3680873" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/NJUPT-MCC/CoIn" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" rel="external nofollow noopener" target="_blank">TPAMI</a></abbr></div>

        <!-- Entry bib key -->
        <div id="xia2022gan" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Physical Adversarial Attack Meets Computer Vision: A Decade Survey</div>
          <!-- Author -->
          <div class="author">
          

          Â Hui Wei, Â <em>Hao Tang</em>, Â Xuemei Jia, Â Zhixiang Wang, Â Hanxun Yu, Â Zhubo Li, Â Shin'ichi Satoh, Â Luc Van Gool, Â Zheng Wang</div>

          <div class="periodical">
          
            <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2024
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2209.15179" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/weihui1308/PAA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Motion Mamba: Efficient and Long Sequence Motion Generation with Hierarchical and Bidirectional Selective SSM</div>
          <!-- Author -->
          <div class="author">
          
          Â Zeyu Zhang, Â Akide Liu, Â Ian Reid, Â Richard Hartley, Â Bohan Zhuang, Â <em>Hao Tang*</em></div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.07487" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/steve-zeyu-zhang/MotionMamba/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">3D Weakly Supervised Semantic Segmentation with 2D Vision-Language Guidance</div>
          <!-- Author -->
          <div class="author">
          
          Â Xiaoxu Xu, Â Yitian Yuan, Â Jinlong Li, Â Qiudan Zhang, Â Zequn Jie, Â Lin Ma, Â <em>Hao Tang</em>, Â Nicu Sebe, Â Xu Wang</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2407.09826v1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/xuxiaoxxxx/3DSS-VLG/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">StoryImager: A Unified and Efficient Framework for Coherent Story Visualization and Completion</div>
          <!-- Author -->
          <div class="author">
          
          Â Ming Tao, Â Bingkun Bao, Â <em>Hao Tang</em>, Â Yaowei Wang, Â Changsheng Xu</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.05979" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/tobran/StoryImager" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">InstructGIE: Towards Generalizable Image Editing</div>
          <!-- Author -->
          <div class="author">
        
          Â Zichong Meng, Â Changdi Yang, Â Jun Liu, Â <em>Hao Tang*</em>, Â Pu Zhao*, Â Yanzhi Wang*</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.05018" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://cr8br0ze.github.io/InstructGIE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">SCP-Diff: Photo-Realistic Semantic Image Synthesis with Spatial-Categorical Joint Prior</div>
          <!-- Author -->
          <div class="author">
          
          Â Huan-ang Gao, Â Mingju Gao, Â Jiaju Li, Â Wenyi Li, Â Rong Zhi, Â <em>Hao Tang</em>, Â Hao Zhao</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.09638" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://air-discover.github.io/SCP-Diff/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>


        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Dataset Growth</div>
          <!-- Author -->
          <div class="author">
          
          Â Ziheng Qin, Â Zhaopan Xu, Â Yukun Zhou, Â Zangwei Zheng, Â Zebang Cheng, Â <em>Hao Tang</em>, Â Lei Shang, Â Baigui Sun, Â Xiaojiang Peng, Â Radu Timofte, Â Hongxun Yao, Â Kai Wang, Â Yang You</div>

          <div class="periodical">
          
            In <em>ECCV 2024</em>, Milan, Italy
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2405.18347" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/NUS-HPC-AI-Lab/InfoGrowth" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR <br> Highlight</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud</div>
          <!-- Author -->
          <div class="author">
          
          Â Wencan Cheng, Â <em>Hao Tang</em>, Â Luc Van Gool, Â Jong Hwan Ko </div>
          
          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.03159" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/cwc1260/HandDiff" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Versatile Navigation under Partial Observability via Value-guided Diffusion Policy</div>
          <!-- Author -->
          <div class="author">
          
          Â Gengyu Zhang, Â <em>Hao Tang</em>, Â Yan Yan </div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.02176" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Towards Robust 3D Pose Transfer with Adversarial Learning</div>
          <!-- Author -->
          <div class="author">
          
          Â Haoyu Chen, Â <em>Hao Tang</em>, Â Ehsan Adeli, Â Guoying Zhao </div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.02242" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">On the Faithfulness of Vision Transformer Explanations</div>
          <!-- Author -->
          <div class="author">
          
          Â Junyi Wu, Â Weitai Kang, Â <em>Hao Tang</em>, Â Yuan Hong, Â Yan Yan </div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2404.01415" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer</div>
          <!-- Author -->
          <div class="author">
          
          Â Junyi Wu, Â Bin Duan, Â Weitai Kang, Â <em>Hao Tang</em>, Â Yan Yan </div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.14552" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation</div>
          <!-- Author -->
          <div class="author">
          
          Â Yuxuan Zhang, Â Jiaming Liu, Â Yiren Song, Â Rui Wang, Â <em>Hao Tang</em>, Â Jinpeng Yu, Â Huaxia Li, Â Xu Tang, Â Yao Hu, Â Han Pan, Â Zhongliang Jing</div>

          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2312.16272" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Xiaojiu-z/SSR_Encoder" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR <br> Workshop</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Towards Online Real-Time Memory-based Video Inpainting Transformers</div>
          <!-- Author -->
          <div class="author">
          
          Â Guillaume Thiry, Â <em>Hao Tang*</em>, Â Radu Timofte, Â Luc Van Gool</div>
          
          <div class="periodical">
          
            In <em>CVPR</em> 2024, Seattle, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2403.16161" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div>

        <!-- Entry bib key -->
        <div id="chen2022cross" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">G2P-DDM: Generating Sign Pose Sequence from Gloss Sequence with Discrete Diffusion Model</div>
          <!-- Author -->
          <div class="author">
          
          Â Pan Xie, Â Qipeng Zhang, Â Peng Taiying, Â <em>Hao Tang*</em>, Â Yao Du, Â Zexian Li</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In Advances in Neural Information Processing Systems</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>AAAI</em> 2024, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2208.09141.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://slpdiffusier.github.io/g2p-ddm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">HotBEV: Hardware-oriented Transformer-based Multi-View 3D Detector for BEV Perception</div>
          <!-- Author -->
          <div class="author">

          Â Peiyan Dong, Â Zhenglun Kong, Â Xin Meng, Â Pinrui Yu, Â Yifan Gong, Â Geng Yuan, Â <em>Hao Tang*</em>, Yanzhi Wang</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=3Cj67k38st" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/PeiyanFlying/HotBEV" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">PackQViT: Faster Sub-8-bit Vision Transformers via Full and Packed Quantization on the Mobile</div>
          <!-- Author -->
          <div class="author">

        Â Peiyan Dong, Â Lei Lu, Â Chao Wu, Â Cheng Lyu, Â Geng Yuan, Â <em>Hao Tang*</em>, Yanzhi Wang</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=N56hAiQvot" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/PeiyanFlying/PackQViT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">LART: Neural Correspondence Learning with Latent Regularization Transformer for 3D Motion Transfer</div>
          <!-- Author -->
          <div class="author">

        Â Haoyu Chen, Â <em>Hao Tang</em>, Â Radu Timofte, Â Luc Van Gool, Â Guoying Zhao</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=g27BggUT3L" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/mikecheninoulu/LART" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Does Graph Distillation See Like Vision Dataset Counterpart?</div>
          <!-- Author -->
          <div class="author">

        Â Beining Yang, Â Kai Wang, Â Qingyun Sun, Â Cheng Ji, Â Xingcheng Fu, Â <em>Hao Tang</em>, Â Yang You, Â Jianxin Li</div>

          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>NeurIPS</em> 2023, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2310.09192.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/RingBDStack/SGDD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge" style="background-color:#00369f"><a href="https://link.springer.com/journal/11633" rel="external nofollow noopener" target="_blank">MIR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Practical Blind Image Denoising via Swin-Conv-UNet and Data Synthesis</div>
          <!-- Author -->
          <div class="author">

        Â Kai Zhang, Â Yawei Li, Â Jingyun Liang, Â Jiezhang Cao, Â Yulun Zhang, Â <em>Hao Tang</em>, Â Dengping Fan, Â Radu Timofte, Â Luc Van Gool</div>

          <div class="periodical">
          
          <em>Springer Machine Intelligence Research (MIR)</em>, 2023
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://link.springer.com/article/10.1007/s11633-023-1466-0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/cszn/SCUNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>  
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ICCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Learning Concordant Attention via Target-aware Alignment for Visible-Infrared Person Re-identification</div>
          <!-- Author -->
          <div class="author">
          
          Â Jianbing Wu, Â Hong Liu, Â Yuxin Su, Â Wei Shi, Â <em>Hao Tang</em></div>

          <div class="periodical">
          
            In <em>ICCV</em> 2023, Paris, France
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Learning_Concordant_Attention_via_Target-aware_Alignment_for_Visible-Infrared_Person_Re-identification_ICCV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning" rel="external nofollow noopener" target="_blank">ICML</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">SpeedDETR: Speed-aware Transformers for End-to-end Object Detection</div>
          <!-- Author -->
          <div class="author">
          
          Â Peiyan Dong, Â Zhenglun Kong, Â Xin Meng, Â Peng Zhang, Â <em>Hao Tang*</em>, Â Yanzhi Wang, Â Chih-Hsien Chou</div>

          <div class="periodical">
          
            In <em>ICML</em> 2023, Hawaii, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openreview.net/pdf?id=5VdcSxrlTK" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/PeiyanFlying/SpeedDETR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Joint_Conference_on_Artificial_Intelligence" rel="external nofollow noopener" target="_blank">IJCAI</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <div class="title">Data Level Lottery Ticket Hypothesis for Vision Transformers</div>
          <!-- Author -->
          <div class="author">
          
          Â Xuan Shen, Â Zhenglun Kong, Â Minghai Qin, Â Peiyan Dong, Â Geng Yuan, Â Xin Meng, Â <em>Hao Tang</em>, Â Xiaolong Ma, Â Yanzhi Wang</div>

          <div class="periodical">
          
            In <em>IJCAI</em> 2023, Macao, China
          
          </div>
        
          <div class="links">
            <a href="https://arxiv.org/pdf/2211.01484.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/shawnricecake/vit-lottery-ticket-input" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Graph Transformer GANs for Graph-Constrained House Generation</div>
          <!-- Author -->
          <div class="author">
          
          Â <em>Hao Tang</em>, Â Zhenyu Zhang, Â Humphrey Shi, Â Bo Li, Â Ling Shao, Â Nicu Sebe, Â Radu Timofte, Â Luc Van Gool</div>

          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2303.08225.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GTGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Unsupervised Deep Probabilistic Approach for Partial Point Cloud Registration</div>
          <!-- Author -->
          <div class="author">
          
          Â Guofeng Mei, Â <em>Hao Tang</em>, Â Xiaoshui Huang, Â Weijie Wang, Â Juan Liu, Â Jian Zhang, Â Luc Van Gool, Â Qiang Wu  </div>
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2303.13290" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/gfmei/UDPReg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network</div>
          <!-- Author -->
          <div class="author">
          
          Â Xuan Shen, Â Yaohua Wang, Â Ming Lin, Â Yilun Huang, Â <em>Hao Tang</em>, Â Xiuyu Sun, Â Yanzhi Wang </div>
          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2303.02165.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/alibaba/lightweight-neural-architecture-search" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â Ming Tao, Â Bingkun Bao, Â <em>Hao Tang</em>, Â Changsheng Xu</div>
          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>CVPR</em> 2023, Vancouver, Canada
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2301.12959.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/tobran/GALIP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations" rel="external nofollow noopener" target="_blank">ICLR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Edge Guided GANs with Contrastive Learning for Semantic Image Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â <em>Hao Tang</em>, Â Xiaojuan Qi, Â Guolei Sun, Â Dan Xu, Â Nicu Sebe, Â Radu Timofte, Â Luc Van Gool</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>ICLR</em> 2023, Kigali, Rwanda
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/abs/2003.13898" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/ECGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">3D-Aware Semantic-Guided Generative Model for Human Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â Jichao Zhang, Â Enver Sangineto, Â <em>Hao Tang</em>, Â Aliaksandr Siarohin, Â Zhun Zhong, Â Nicu Sebe, Â Wei Wang</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>ECCV</em> 2022, Tel Aviv, Israel
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2112.01422.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/zhangqianhui/3DSGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/European_Conference_on_Computer_Vision" rel="external nofollow noopener" target="_blank">ECCV</a></abbr></div>

        <!-- Entry bib key -->
        <div id="wang2022modeling" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Towards Interpretable Video Super-Resolution via Alternative Optimization</div>
          <!-- Author -->
          <div class="author">
          
          Â Jiezhang Cao, Â Jingyun Liang, Â Kai Zhang, Â Wenguan Wang, Â Qin Wang, Â Yulun Zhang, Â <em>Hao Tang</em>, Â Luc Van Gool</div>

          <!-- Journal/Book title and date -->
          
          <!-- <div class="periodical">
            <em>In European Conference on Computer Vision</em> 2022
          </div> -->
          <div class="periodical">
          
            In <em>ECCV</em> 2022, Tel Aviv, Israel
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://arxiv.org/pdf/2207.10765.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/caojiezhang/DAVSR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR</a></abbr></div>

        <!-- Entry bib key -->
        <div id="cai2022mask" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation</div>
          <!-- Author -->
          <div class="author">
          
          Â Wenhao Li, Â Hong Liu, Â <em>Hao Tang</em>, Â Pichao Wang, Â Luc Van Gool</div>

          <div class="periodical">
          
            In <em>CVPR</em> 2022, New Orleans, USA
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_MHFormer_Multi-Hypothesis_Transformer_for_3D_Human_Pose_Estimation_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Vegetebird/MHFormer" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition" rel="external nofollow noopener" target="_blank">CVPR Oral</a></abbr></div>

        <!-- Entry bib key -->
        <div id="cai2022mask" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis</div>
          <!-- Author -->
          <div class="author">
          
          Â Ming Tao, Â <em>Hao Tang</em>, Â Fei Wu, Â Xiaoyuan Jing, Â Bingkun Bao, Â Changsheng Xu</div>

          <div class="periodical">
          
              In <em>CVPR</em> 2022, New Orleans, USA
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_DF-GAN_A_Simple_and_Effective_Baseline_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/tobran/DF-GAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>


<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://en.wikipedia.org/wiki/ACM_Multimedia" rel="external nofollow noopener" target="_blank">ACM MM Best<br>Paper Candidate</a></abbr></div>

        <!-- Entry bib key -->
        <div id="luo2022adjustable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">GestureGAN for Hand Gesture-to-Gesture Translation in the Wild</div>
          <!-- Author -->
          <div class="author">
          
          Â <em>Hao Tang</em>, Â Wei Wang, Â Dan Xu, Â Yan Yan, Â Nicu Sebe</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
          
            In <em> ACM MM</em> 2018, Seoul, South Korea
          
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="https://dl.acm.org/doi/pdf/10.1145/3240508.3240704" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Ha0Tang/GestureGAN" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>

          
        </div>
      </div>
</li>

</ol>
          </div>    
    <!-- <a href="https://clustrmaps.com/site/19ncr" title="Visit tracker" rel="external nofollow noopener" target="_blank"><img src="//www.clustrmaps.com/map_v2.png?d=jIdUd0dDYkE8CiqptfhnfiWcZHCc5p62dIsontyW-FQ&amp;cl=ffffff" style="width: 0px;"></a> -->
  </article>

</div>

    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        Â© Copyright 2025 Hao Tang. Last updated: Apr 4, 2025.

        <!-- Default Statcounter code for Hao Tang' homepage
        https://ha0tang.github.io/ -->
        <script type="text/javascript">
        var sc_project=12830645; 
        var sc_invisible=1; 
        var sc_security="11b4016e"; 
        </script>
        <script type="text/javascript"
        src="https://www.statcounter.com/counter/counter.js"
        async></script>
        <noscript><div class="statcounter"><a title="Web Analytics
        Made Easy - Statcounter" href="https://statcounter.com/"
        target="_blank"><img class="statcounter"
        src="https://c.statcounter.com/12830645/0/11b4016e/1/"
        alt="Web Analytics Made Easy - Statcounter"
        referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
        <!-- End of Statcounter Code -->

      </div>
    </footer>


    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
