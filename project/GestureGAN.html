
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en" class="gr__disi_unitn_it"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>SelectionGAN Project Page</title>


<meta property="og:image" content="images/teaser_fb.jpg">
<meta property="og:title" content="GestureGAN for Hand Gesture-to-Gesture Translation in the Wild">

<script type="text/javascript" async="" src="./GestureGAN Project Page_files/analytics.js"></script><script src="./GestureGAN Project Page_files/lib.js" type="text/javascript"></script>
<script src="./GestureGAN Project Page_files/popup.js" type="text/javascript"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async="" src="./GestureGAN Project Page_files/js"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-136330885-1');
</script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="./GestureGAN Project Page_files/glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 0px;
	FLOAT: right;
	PADDING-BOTTOM: 0px;
	PADDING-TOP: 0px
}
#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1000px }
BODY {
	TEXT-ALIGN: center
}
</style>

<meta content="MSHTML 6.00.2800.1400" name="GENERATOR"><script src="./GestureGAN Project Page_files/b5m.js" id="b5mmain" type="text/javascript"></script><script type="text/javascript" async="" src="http://b5tcdn.bang5mai.com/js/flag.js?v=155518528"></script></head>

<body data-gr-c-s-loaded="true">

<div id="primarycontent"; class="justify">
<center> <h1> <b> GestureGAN for Hand Gesture-to-Gesture Translation in the Wild </b></h1> </center>

<center><h2>
	<a href="http://disi.unitn.it/~hao.tang/"        target="_blank" >Hao Tang<sup>1</sup>*</a>&nbsp;&nbsp;&nbsp;
	<a href="https://weiwangtrento.github.io/"     target="_blank" >Wei Wang<sup>1,2</sup>*</a>&nbsp;&nbsp;&nbsp;
	<a href="http://www.robots.ox.ac.uk/~danxu/"     target="_blank" >Dan Xu<sup>1,3</sup>*</a>&nbsp;&nbsp;&nbsp;
	<a href="https://userweb.cs.txstate.edu/~y_y34/" target="_blank" >Yan Yan<sup>4</sup></a>&nbsp;&nbsp;&nbsp;
	<a href="https://scholar.google.com/citations?user=stFCYOAAAAAJ&hl=en" target="_blank" >Nicu Sebe<sup>1</sup></a>&nbsp;&nbsp;&nbsp;
	</h2>

	<center><h3>
		<a href="https://www.disi.unitn.it/"    style="color:black;" target="_blank"><sup>1</sup>University of Trento</a>&nbsp;&nbsp;&nbsp;
		<a href="https://www.epfl.ch/en/"    style="color:black;" target="_blank"><sup>2</sup>EPFL</a>&nbsp;&nbsp;&nbsp;
		<a href="http://www.ox.ac.uk/"          style="color:black;" target="_blank"><sup>3</sup>University of Oxford</a>&nbsp;&nbsp;&nbsp;
		<a href="https://www.txstate.edu/"      style="color:black;" target="_blank"><sup>4</sup>Texas State University</a>&nbsp;&nbsp;&nbsp;
	</h3></center>
<center><h2>in <strong><a href="https://dl.acm.org/citation.cfm?id=3240508&picked=prox" target="_blank">ACM MM 2018 </a></strong> (Oral)</h2></center>
<center><h2><strong>
	<a href="https://arxiv.org/pdf/1808.04859.pdf" target="_blank">Paper</a> | 
	<a href="https://github.com/Ha0Tang/GestureGAN" target="_blank" target="_blank">Code</a> | 
	<a href="http://disi.unitn.it/~hao.tang/uploads/slides/SelectionGAN_CVPR19.pptx" target="_blank">Slides</a> | 
	<a href="http://disi.unitn.it/~hao.tang/uploads/posters/SelectionGAN_CVPR19.pdf" target="_blank">Poster</a> </strong> </h2></center>

<center><a href="./GestureGAN Project Page_files/framework.jpg">
<img src="./GestureGAN Project Page_files/framework.jpg" width="100%"> </a></center>
<p align="justify"> Overview of the proposed SelectionGAN. Stage I presents a cycled semantic-guided generation sub-network which accepts images from one view and conditional semantic maps and simultaneously synthesizes images and semantic maps in another view. Stage II takes the coarse predictions and the learned deep semantic features from stage I, and performs a fine-grained generation using the proposed multi-channel attention selection module.
</p>

<p></p>
<p>

</p>
<h2 align="center">Abstract</h2>

<div style="font-size:14px"><p align="justify">Cross-view image translation is challenging because it involves  images with drastically different views and severe deformation.
In this paper, we propose a novel approach named Multi-Channel Attention SelectionGAN (SelectionGAN) that makes it possible to generate images of natural scenes in arbitrary viewpoints, based on an image of the scene and a novel semantic map. The proposed SelectionGAN explicitly utilizes the semantic information and consists of two stages. In the first stage, the condition image and the target semantic map are fed into a cycled semantic-guided generation network to produce initial coarse results. In the second stage, we refine the initial results by using a multi-channel attention selection mechanism. Moreover, uncertainty maps automatically learned from attentions are used to guide the pixel loss for better network optimization. Extensive experiments on Dayton, CVUSA and Ego2Top datasets show that our model is able to generate significantly better results than the state-of-the-art methods.</p></div>

<a href="https://arxiv.org/pdf/1808.04859.pdf" target="_blank"><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="./GestureGAN Project Page_files/paper_thumbnail.jpg" width="150"></a>



<h2>Paper</h2>
<p><a href="https://arxiv.org/pdf/1808.04859.pdf" target="_blank">arxiv</a>,  2019. </p>



<h2>Citation</h2>
<p>Hao Tang, Wei Wang, Dan Xu, Yan Yan, Nicu Sebe.<br>"GestureGAN for Hand Gesture-to-Gesture Translation
in the Wild". In ACM MM, 2018.
<a href="./GestureGAN Project Page_files/SelectionGAN.txt" target="_blank">Bibtex</a>
</p>

<p></p>

<h1 align="center">  Multi-Channel Attention Selection Module </h1>
<center><img src="./GestureGAN Project Page_files/method.jpg" width="100%"></center>
<br>
<p align="justify"> The multi-scale spatial pooling pools features in different receptive fields in order to have better generation of scene details; the multi-channel attention selection aims at automatically select from a set of intermediate diverse generations in a larger generation space to improve the generation quality.
</p>


<br>
<h1 align="center"> Arbitrary Cross-View Image Translation </h1>
<center><img src="./SelectionGAN Project Page_files/supp_ego2top_results.jpg" width="1000"></center>
<p align="justify"> 
Arbitrary cross-view image translation on <a href="https://arxiv.org/abs/1607.06986" target="_blank"><span style="font-weight:normal">Ego2Top dataset</span></a>. Given an image and some novel semantic maps, SelectionGAN is able to generate the same scene but with different viewpoints.
</p>
<br>

<br>
<h1 align="center"> State-of-the-art Comparisons on Ego2Top Dataset </h1>
<center><img src="./SelectionGAN Project Page_files/supp_ego2top_comp.jpg" width="1000"></center>
<p align="justify"> 
Results generated by different methods in 256×256 resolution on <a href="https://arxiv.org/abs/1607.06986" target="_blank"><span style="font-weight:normal">Ego2Top dataset</span></a>. These samples were randomly selected for visualization purposes.
</p>
<br>


<br>
<h1 align="center"> State-of-the-art Comparisons on CVUSA Dataset </h1>
<center><img src="./SelectionGAN Project Page_files/supp_cvusa_comp.jpg" width="1000"></center>
<p align="justify"> 
Results generated by different methods in 256×256 resolution on <a href="http://cs.uky.edu/~jacobs/datasets/cvusa/" target="_blank"><span style="font-weight:normal">CVUSA dataset</span></a>. These samples were randomly selected for visualization purposes.
</p>
<br>

<br>
<h1 align="center"> State-of-the-art Comparisons on Dayton Dataset in g2a Direction </h1>
<center><img src="./SelectionGAN Project Page_files/supp_dayton_256_g2a_comp.jpg" width="1000"></center>
<p align="justify"> 
Results generated by different methods in 256×256 resolution in g2a direction on <a href="https://github.com/lugiavn/gt-crossview" target="_blank"><span style="font-weight:normal">Dayton dataset</span></a>. These samples were randomly selected for visualization purposes.
</p>
<br>

<h1 align="center"> Code, Data and Trained Models</h1>
	<p align="justify"> Please visit our <a href="https://github.com/Ha0Tang/GestureGAN" target="_blank">github repo</a>.  </p>

<br>
<h1>Acknowledgement</h1>
<p align="justify">We want to thank the Nvidia Corporation for the donation of the TITAN Xp GPUs used in this work.</p>

<br>
<h1>Related Work</h1>

<ul id="relatedwork">
<div align="left">
<li font-size:="" 15px=""> Phillip  Isola,  Jun-Yan  Zhu,  Tinghui  Zhou,  and  Alexei  AEfros <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" target="_blank"><strong>"Image-to-Image Translation with Conditional Adversarial Networks"</strong></a>. In CVPR 2017. (Pix2pix)
<li font-size:="" 15px=""> Krishna Regmi and Ali Borji. <a href="https://github.com/kregmi/cross-view-image-synthesis/" target="_blank"><strong>"Cross-View Image Synthesis Using Conditional GANs"</strong></a>. In CVPR 2018. (X-Fork & X-Seq)
</li>
</div>
</ul>


<div style="display:none">
<script type="text/javascript" src="./SelectionGAN Project Page_files/counter.js"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script>
<noscript><a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"><img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" /></a></noscript>
</div>

</center></div></body><div><div class="gr_-editor gr-iframe-first-load" style="display: none;"><div class="gr_-editor_back"></div><iframe class="gr_-ifr gr-_dialog-content" src="./SelectionGAN Project Page_files/saved_resource.html"></iframe></div></div><grammarly-card><div></div></grammarly-card><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>